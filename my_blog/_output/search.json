[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jesus LM",
    "section": "",
    "text": "For over a decade, I‚Äôve leveraged data analysis and visualization to deliver strategic value across diverse sectors like public security, e-commerce, and healthcare. I use Python1 and SQL to collect, transform, and analyze raw data, creating actionable data products such as webpages, slideshows, and dashboards, ocasionally including regression and classification analyses to support informed decision-making.\nBackground in Economics (BA), Information Technology (MA) and Data Science and Machine Learning (Certification).\nCommitted to fostering data literacy, I share some data posts on Learning Data and T3CH via Medium.\nCurrently serving as Deputy Director of Public Policy at the Executive Secretariat of the National Public Security System, responsible for applying analytical skills to assess public security policy initiatives.\nVegan."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Jesus LM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIncluding libraries such as numpy, pandas, polars, duckdb, seaborn, plotly, folium, dash and sckitlearn‚Ü©Ô∏é"
  },
  {
    "objectID": "farmacias_similares.html",
    "href": "farmacias_similares.html",
    "title": "Location of Farmacias Similares in Mexico",
    "section": "",
    "text": "# import libraries\nimport pandas as pd\nimport requests\nimport folium\nimport folium.plugins\n\n\nurl = 'https://www.farmaciasdesimilares.com/getpickuppoints'\n\n\n# request data\nresponse = requests.get(url)\nstores = response.json()\n\n\nstores[0]\n\n{'id': '47',\n 'name': 'ACAPULCO 1',\n 'description': 'Farmacias de Similares',\n 'instructions': 'Farmacias de Similares',\n 'formatted_address': 'undefined',\n 'address': {'postalCode': '39350',\n  'country': {'acronym': 'MEX', 'name': 'M√©xico'},\n  'city': 'ACAPULCO DE JUAREZ',\n  'state': 'Guerrero',\n  'neighborhood': 'PROGRESO',\n  'street': 'GUERRERO',\n  'number': 'ED A',\n  'complement': 'LC 1,2,3',\n  'reference': 'Farmacias de similares',\n  'location': {'latitude': 16.86016, 'longitude': -99.90353}},\n 'isActive': False,\n 'distance': 1693858004992,\n 'seller': 'farmaciassimilaresmx',\n '_sort': [1693858000000],\n 'businessHours': [{'dayOfWeek': 1,\n   'openingTime': '01:00:00',\n   'closingTime': '22:00:00'},\n  {'dayOfWeek': 2, 'openingTime': '02:00:00', 'closingTime': '22:00:00'},\n  {'dayOfWeek': 3, 'openingTime': '03:00:00', 'closingTime': '22:00:00'},\n  {'dayOfWeek': 4, 'openingTime': '03:00:00', 'closingTime': '22:00:00'},\n  {'dayOfWeek': 5, 'openingTime': '04:00:00', 'closingTime': '22:00:00'},\n  {'dayOfWeek': 6, 'openingTime': '05:00:00', 'closingTime': '22:00:00'},\n  {'dayOfWeek': 0, 'openingTime': '06:00:00', 'closingTime': '22:00:00'}],\n 'tagsLabel': ['47'],\n 'pickupHolidays': [],\n 'isThirdPartyPickup': False,\n 'accountOwnerName': 'farmaciassimilaresmx',\n 'accountOwnerId': '010e8ff0-804a-4f40-bb32-20e5c7b50254',\n 'parentAccountName': None,\n 'originalId': None}\n\n\n\ndrugstore=[]\ncity=[]\nstate=[]\nneighborhood=[]\nstreet=[]\nzip_code=[]\nnumber=[]\nlatitude=[]\nlongitude=[]\n\nfor store in range(len(stores)):\n    drugstore.append(stores[store]['name'])\n    city.append(stores[store]['address']['city'])\n    state.append(stores[store]['address']['state'])\n    neighborhood.append(stores[store]['address']['neighborhood'])\n    street.append(stores[store]['address']['street'])\n    number.append(stores[store]['address']['number'])\n    zip_code.append(stores[store]['address']['postalCode'])\n    latitude.append(stores[store]['address']['location']['latitude'])\n    longitude.append(stores[store]['address']['location']['longitude'])\n\n\nsimi_df = pd.DataFrame(\n    {'drugstore':drugstore,\n    'street':street,\n    'number':number,\n    'neighborhood':neighborhood,\n    'city': city,\n    'zip_code':zip_code,\n    'latitude': latitude,\n    'longitude': longitude,}\n)\n\n\nsimi_df.head()\n\n\n\n\n\n\n\n\ndrugstore\nstreet\nnumber\nneighborhood\ncity\nzip_code\nlatitude\nlongitude\n\n\n\n\n0\nACAPULCO 1\nGUERRERO\nED A\nPROGRESO\nACAPULCO DE JUAREZ\n39350\n16.86016\n-99.90353\n\n\n1\nACAPULCO 10\nRUIZ CORTINEZ\n8\nALTA PROGRESO\nACAPULCO DE JUAREZ\n39610\n16.87317\n-99.89115\n\n\n2\nACAPULCO 11\nGRAN VIA EL COLOSO\nLT 10\nLA ESPERANZA\nACAPULCO DE JUAREZ\n39610\n16.84682\n-99.81231\n\n\n3\nACAPULCO 13\nLAZARO CARDENAS\n36\nLAS CRUCES\nACAPULCO DE JUAREZ\n39902\n16.88586\n-99.83698\n\n\n4\nACAPULCO 14\nCUAUHTEMOC\n129\nPROGRESO\nACAPULCO DE JUAREZ\n39350\n16.85863\n-99.89725\n\n\n\n\n\n\n\n\nsimi_df.to_csv('similares.csv')\n\n\nsimi_face = 'https://tinyurl.com/3mvfbu4t'\n\n\n# map\nsimi_map = folium.Map(location=[23, -101], zoom_start=5, attr='Google')\n\nfor i in range(0,len(simi_df)):\n    # logo marker\n    folium.Marker(location=[simi_df['latitude'][i], simi_df['longitude'][i]],\n                  popup=folium.Popup(\n                      f\"&lt;b&gt;Drugstore:&lt;/b&gt; {simi_df['drugstore'][i]} \\\n                      &lt;br&gt;&lt;b&gt;Street:&lt;/b&gt; {simi_df['street'][i]} \\\n                      &lt;br&gt;&lt;b&gt;Neighborhood:&lt;/b&gt; {simi_df['neighborhood'][i]}\"\n                      ),\n                  icon=folium.features.CustomIcon(\n                      simi_face,\n                      icon_size=(40,40)\n                      )\n                 ).add_to(simi_map)\n\nfolium.plugins.Fullscreen(\n    position=\"topleft\",\n    title=\"Expand me\",\n    title_cancel=\"Exit me\",\n    force_separate_button=True,\n    ).add_to(simi_map)\n\n&lt;folium.plugins.fullscreen.Fullscreen at 0x7658502ff830&gt;\n\n\n\nsimi_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nReferences\n\nPacheco, C. (2025) simi_ubicas. Github.\n\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Farmacias Similares Locations"
    ]
  },
  {
    "objectID": "public_security.html",
    "href": "public_security.html",
    "title": "Analyzing Data, from Crime Trends to Resource Allocation",
    "section": "",
    "text": "Analyzing Data, from Crime Trends to Resource Allocation\nThe Economist and Data Scientist Role in Public Security\nMay, 2025\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\nImage by NBER\n\n\nThe combination of economic and data science expertise can be highly valuable in enhancing public security. Here‚Äôs a breakdown of how these professionals can contribute.\n\nEconomist Contributions\n\n\n\nAnalyzing the Economic Roots of Crime\nEconomists can study the correlation between economic factors (poverty, unemployment, inequality) and crime rates. They can analyze how economic policies impact crime rates, helping policymakers understand the potential consequences of their decisions.\nCost-benefit analysis of crime prevention programs\nEconomists can evaluate the economic efficiency of various public security initiatives.\nResource Allocation\nThey can help optimize the allocation of public security resources, ensuring that funds are directed towards the most effective programs and areas. Analyzing the economic impact of crime on communities and businesses.\nUnderstanding illicit economies\nAnalyzing the economic structures of illegal markets, such as those involving drugs, human trafficking, or illegal arms sales. Evaluating the effectiveness of policies aimed at disrupting these markets.\n\n\n\nData Scientist Contributions\n\n\n\nPredictive Policing\nData scientists can develop models to predict crime hotspots and identify individuals at high risk of involvement in criminal activity.\nAnalyzing crime patterns and trends to inform resource deployment.\nData-Driven Crime Analysis\nThey can analyze large datasets from various sources (police reports, social media, etc.) to identify patterns and insights that would be difficult to detect manually. Using geospatial analysis to map crime hotspots and understand spatial patterns.\nImproving Law Enforcement Efficiency\nDeveloping tools to automate tasks, improve data management, and enhance communication between law enforcement agencies. Analyzing the effectiveness of law enforcement strategies and identifying areas for improvement.\nFraud Detection\nData Scientists can build models that detect fraudulent activities, which are often connected to organized crime.\nAnalyzing the effects of social programs\nThey can analyze the effects of social programs on crime rates, and help determine which programs are the most effective.\n\n\n\nSynergistic Collaboration\nWhen economists and data scientists collaborate, they can provide a more comprehensive understanding of public security challenges. Economists can provide the context and theoretical framework, while data scientists can provide the tools and techniques for analyzing the data. This interdisciplinary approach can lead to more effective and evidence-based public security policies. In essence, by combining economic theory with data-driven analysis, these professionals can play a vital role in creating safer and more secure communities.\n\n\n\nKey functions that an economist and data scientist might perform in public security.\n\nCrime Pattern Analysis and Prediction\n\nEconomist Focus\nAnalyze how economic conditions (e.g., unemployment, poverty, income inequality) correlate with crime rates in specific areas. Develop economic models to explain these relationships.\nData Scientist Focus\nUse machine learning algorithms to identify patterns in crime data (time, location, type of crime, offender demographics). Build predictive models to forecast future crime hotspots and times.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef predict_crime_hotspots(crime_data, features, target, test_size=0.2):\n    \"\"\"\n    Predicts crime hotspots using a Random Forest Regressor model.\n\n    Args:\n        crime_data (pd.DataFrame): DataFrame containing crime data.\n        features (list): List of feature columns to use for prediction.\n        target (str): The target variable (e.g., 'crime_rate').\n        test_size (float): Proportion of the data to use for testing.\n\n    Returns:\n        tuple: (model, X_test, y_test) - Trained model, test features, \n        and test target.\n    \"\"\"\n    X = crime_data[features]\n    y = crime_data[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n    test_size=test_size, random_state=42)\n\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n\n    return model, X_test, y_test\nExample usage:\n''' \nAssuming 'crime_data' is a DataFrame with columns like:\n    'location_x', 'location_y', 'time', 'crime_type', \n    'unemployment_rate', 'poverty_rate', 'crime_rate'\n'''\n\nfeatures = ['location_x', 'location_y', 'time', 'crime_type', \n            'unemployment_rate', 'poverty_rate']\ntarget = 'crime_rate'\nmodel, X_test, y_test = predict_crime_hotspots(crime_data, features,\n                                                target)\npredictions = model.predict(X_test)\nprint(predictions)\n\n\nConclusions\nIn summary, the combined expertise of economists and data scientists offers a powerful toolkit for enhancing public security. Economists illuminate the underlying causes of crime, while data scientists provide the means to predict, prevent, and effectively respond to threats. Embracing this interdisciplinary approach is crucial for developing smarter, more targeted, and ultimately more successful strategies for creating safer communities.\n\n\nReferences\n\nNBER (2025). Economics of Crime. Retrieved from website.\n\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Articles",
      "The Economist & Data Scientist Role in Public Security"
    ]
  },
  {
    "objectID": "plotnine.html",
    "href": "plotnine.html",
    "title": "ggplot2 in Python",
    "section": "",
    "text": "ggplot2 is based on Hadley Wickham‚Äôs Layered Grammar of Graphics1, which provides a systematic way to describe the components of a statistical graphic.\nggplot2 is a declarative language, which implies to specify the ‚Äúwhat you want‚Äù to plot, rather than ‚Äúthe how‚Äù.\nggplot2 has a wide range of features, including:\n\nSupport for a variety of data types (data frames, matrices, and lists)\nA wide variety of geometric objects (points, lines, bars, and histograms)\nA variety of statistical transformations (aggregation, smoothing, and binning)\nA powerful system for customizing plots",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Plotting with ggplot2 in Python"
    ]
  },
  {
    "objectID": "plotnine.html#overview",
    "href": "plotnine.html#overview",
    "title": "ggplot2 in Python",
    "section": "",
    "text": "ggplot2 is based on Hadley Wickham‚Äôs Layered Grammar of Graphics1, which provides a systematic way to describe the components of a statistical graphic.\nggplot2 is a declarative language, which implies to specify the ‚Äúwhat you want‚Äù to plot, rather than ‚Äúthe how‚Äù.\nggplot2 has a wide range of features, including:\n\nSupport for a variety of data types (data frames, matrices, and lists)\nA wide variety of geometric objects (points, lines, bars, and histograms)\nA variety of statistical transformations (aggregation, smoothing, and binning)\nA powerful system for customizing plots",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Plotting with ggplot2 in Python"
    ]
  },
  {
    "objectID": "plotnine.html#a-layered-grammar-of-graphics",
    "href": "plotnine.html#a-layered-grammar-of-graphics",
    "title": "ggplot2 in Python",
    "section": "A Layered Grammar of Graphics",
    "text": "A Layered Grammar of Graphics\nThere are 03 compulsory components for building visualizations with plotnine:\n\nData\nAesthetics\nGeometric objects",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Plotting with ggplot2 in Python"
    ]
  },
  {
    "objectID": "plotnine.html#environment-settings",
    "href": "plotnine.html#environment-settings",
    "title": "ggplot2 in Python",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nShow code\nfrom plotnine import *\nfrom plotnine.data import *\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Plotting with ggplot2 in Python"
    ]
  },
  {
    "objectID": "plotnine.html#dataset",
    "href": "plotnine.html#dataset",
    "title": "ggplot2 in Python",
    "section": "Dataset",
    "text": "Dataset\n\n\nShow code\neconomics = pl.from_pandas(economics)\n\n\n\n\nShow code\n(\n    ggplot(data=economics)\n    + aes(x='date', y='uempmed')\n    + geom_line(color='#670000', size=1)\n    + labs(title='US unemployed duration (weeks)')\n    + theme(figure_size=(16, 8),\n            axis_text_x = element_text(color=\"black\", size=18, angle=0, hjust=.3),\n            axis_text_y = element_text(color=\"grey\", size=16), \n            plot_title = element_text(size = 25, face = \"bold\"), \n            axis_title = element_text(size = 18)\n            )\n    + xlab('')\n    + ylab('Unemployed duration (median)')\n).draw()",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Plotting with ggplot2 in Python"
    ]
  },
  {
    "objectID": "plotnine.html#contact",
    "href": "plotnine.html#contact",
    "title": "ggplot2 in Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Plotting with ggplot2 in Python"
    ]
  },
  {
    "objectID": "plotnine.html#footnotes",
    "href": "plotnine.html#footnotes",
    "title": "ggplot2 in Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHadley Wickham (2010) A layer Grammar of Graphics.‚Ü©Ô∏é",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Plotting with ggplot2 in Python"
    ]
  },
  {
    "objectID": "duckdb_gsheets.html",
    "href": "duckdb_gsheets.html",
    "title": "Level Up Your Data Analysis from Your Command Line",
    "section": "",
    "text": "Level Up Your Data Analysis from Your Command Line\nConnecting DuckDB to Google Sheets!\nApr, 2025\n\nJesus L. Monroy\nEconomist & Data Scientist\n\nSource: https://www.arecadata.com/sql-for-google-sheets-with-duckdb\n\n\nIntroduction\nTired of downloading CSVs and juggling spreadsheets for your data analysis? Want the power of a lightning-fast, in-process analytical database without the complex setup? Then it‚Äôs time to unlock the magic of connecting DuckDB directly to your Google Sheets!\nDuckDB is a phenomenal open-source analytical data management system. It‚Äôs incredibly fast, lightweight, and can run directly within your Python environment (or other languages). Combining its power with the accessibility and collaborative nature of Google Sheets opens up a world of possibilities for streamlined and efficient data analysis.\n\n\nDuckDB & Google Sheets: A Game Changer\nReal-time Data Access\nNo more manual exporting and importing! DuckDB can directly query the data in your Google Sheet, ensuring you‚Äôre always working with the latest information.\nLeverage DuckDB‚Äôs Analytical Power\nPerform complex SQL queries, joins, aggregations, and window functions on your Google Sheets data with DuckDB‚Äôs blazing-fast engine. This goes far beyond the basic formulas available in spreadsheets.\nSimplified Data Pipelines\nAutomate your analysis workflows by directly pulling data from Google Sheets into your DuckDB scripts. This reduces manual steps and potential errors.\nCollaboration & Analysis in One Place\nMaintain the collaborative benefits of Google Sheets for data collection and sharing, while empowering analysts with DuckDB‚Äôs robust analytical capabilities.\nScalability Beyond Spreadsheets\nHandle larger datasets more efficiently than you could within Google Sheets alone. DuckDB can process significantly more data with greater speed.\nReproducible Analysis\nYour analysis code in DuckDB becomes a clear and reproducible record of how you processed the data from your Google Sheet.\n\n\nHow to Connect DuckDB to Google Sheets\nUsing CLI\n\nInstall duckdb to your terminal\n\ncurl https://install.duckdb.org | sh\n\nOpen duckdb in your terminal\n\nduckdb\n\n\nInstall gsheets\n\ninstall gsheets from community;\nload gsheets;\n\nObtain gsheets url\n\nhttps://docs.google.com/spreadsheets/d/YOUR_SHEET_ID/edit#gid=YOUR_TAB_ID\nYou will just need: YOUR_SHEET_ID\n\nConnect to gsheets\n\ncreate secret (type gsheet, provider oauth);\n\n\n\nRun your query\n\nSELECT current_job_title,\n    round(avg(try_cast(base_salary as int))) as avg_salary\nFROM read_gsheet('YOUR_SHEET_ID', sheet='titles')\nWHERE currency = 'USD'\nAND state = 'Texas'\nGROUP BY current_job_title\nORDER BY avg_salary DESC;\n Source: https://www.arecadata.com/sql-for-google-sheets-with-duckdb\n\n\nConclusions\nConnecting DuckDB to Google Sheets is a powerful and convenient way to enhance data analysis workflows. It brings the speed and analytical capabilities of an in-process OLAP database to the familiar environment of Google Sheets. While there are some setup and authentication considerations, the benefits of using SQL for spreadsheet data and the ability to read and write data directly make this integration a valuable tool for data professionals and anyone working with data in Google Sheets.\n\n\nReferences\n\nDuckdb (n.d.) DuckDB GSheets Tutorial\nPalma, D (2024) SQL for Google Sheets with DuckDB Analyze data in Google Sheets using SQL with DuckDB. Areca Data. Retrieved from website.\n\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Duckdb & Google Sheets"
    ]
  },
  {
    "objectID": "evidence.html",
    "href": "evidence.html",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "",
    "text": "Ditch the drag-and-drop: Code is the Future of Data Analysis\nMar, 2025\nJesus L. Monroy\nEconomist & Data Scientist",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#lack-of-version-control",
    "href": "evidence.html#lack-of-version-control",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Lack of Version Control",
    "text": "Lack of Version Control\nChanges to dashboards are often made directly in the interface, making it difficult to track modifications, revert to previous versions, or collaborate effectively. This leads to ‚Äúdashboard sprawl‚Äù and inconsistencies.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#limited-reproducibility",
    "href": "evidence.html#limited-reproducibility",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Limited Reproducibility",
    "text": "Limited Reproducibility\nReplicating complex analyses or deploying them across different environments can be challenging. The point and click nature of those tools create a lot of hidden dependencies, and no clear way to move the work between systems.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#scalability-issues",
    "href": "evidence.html#scalability-issues",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Scalability Issues",
    "text": "Scalability Issues\nAs your data volume increases, drag-and-drop tools can become slow and inefficient. Complex transformations and calculations may not be optimized for performance.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#vendor-lock-in",
    "href": "evidence.html#vendor-lock-in",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Vendor Lock-in",
    "text": "Vendor Lock-in\nRelying on proprietary interfaces and data models can make it difficult to migrate to other platforms or integrate with other tools.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#version-control",
    "href": "evidence.html#version-control",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Version Control",
    "text": "Version Control\nUsing Git or other version control systems allows you to track changes, collaborate effectively, and revert to previous versions. This ensures consistency and reduces the risk of errors.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#reproducibility",
    "href": "evidence.html#reproducibility",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Reproducibility",
    "text": "Reproducibility\nCode-based analyses are easily reproducible and deployable. You can define your data transformations, calculations, and visualizations in code, ensuring consistent results across different environments.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#scalability-and-performance",
    "href": "evidence.html#scalability-and-performance",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Scalability and Performance",
    "text": "Scalability and Performance\nCode can be optimized for performance and scalability. You can leverage powerful programming languages and libraries to handle large datasets and complex calculations efficiently.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#flexibility-and-customization",
    "href": "evidence.html#flexibility-and-customization",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Flexibility and Customization",
    "text": "Flexibility and Customization\nCode provides greater flexibility and customization. You can tailor your analyses and visualizations to meet specific business needs, without being limited by the constraints of a drag-and-drop interface.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "evidence.html#collaboration",
    "href": "evidence.html#collaboration",
    "title": "BI with SQL and Markdown: Evidence.dev",
    "section": "Collaboration",
    "text": "Collaboration\nCode is easier to share and collaborate on than a drag and drop dashboard. Automation Code can be automated, allowing for scheduled updates, refreshes, and deployments.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI with SQL & Markdown"
    ]
  },
  {
    "objectID": "national_guard.html",
    "href": "national_guard.html",
    "title": "The Mexican National Guard",
    "section": "",
    "text": "The Mexican National Guard\nProgress, Concerns, and the Future of Public Security\nApr, 2024\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\nIntroduction\nThe National Guard (Guardia Nacional - GN) is Mexico‚Äôs national gendarmerie, officially established on June 30, 2019. Created with the aim of tackling the country‚Äôs high levels of crime and restoring public safety, it absorbed personnel and units from the former Federal Police, as well as the military police and naval police.\n\n\nHistory and Formation\nThe National Guard was initially campaigned on taking the military off the streets. However, facing a persistent security crisis, it was proposed as a new force under military command to prevent and combat crime.\nIn February 2019, the Mexican Congress approved constitutional reforms to create the National Guard. The law regulating the force came into effect in May 2019.\nThe National Guard effectively replaced the Federal Police, which had been plagued by issues of corruption and human rights violations throughout its 20-year existence. While some experts suggested reforming the Federal Police, the decision was made to create a new institution.\nInitially, the National Guard was intended to be a civilian institution under the Secretariat of Security and Civilian Protection. However, in practice, it has been largely composed of military personnel, and in September 2024, its command was officially transferred to the Secretariat of National Defense.\nThe force launched operations with tens of thousands of members and aimed to reach an estimated 120,000 members.\n\n\nFunctions and Responsibilities\nThe National Guard has a broad mandate that includes:\n\nPreventing and combating crime throughout the national territory.\nPreserving public security.\nIntensifying the enforcement of immigration policy, including deployment to the southern and northern borders to control the flow of migrants. This was notably increased as part of agreements with the United States.\nFederal law enforcement duties.\nGendarmerie functions, implying a force with military discipline focused on general law enforcement.\nCivilian police duties, with the power to detain suspects and collaborate with public prosecutors in investigations. They can investigate both federal and common crimes under certain agreements.\nSupporting public security tasks in coordination with state and municipal authorities.\nCivil protection in disasters and crises.\nKidnapping resolution, criminal surveillance, and intelligence gathering.\nBorder protection.\n\nMore recently, the armed forces, including the National Guard, have been assigned additional tasks such as protecting oil pipelines, helping build infrastructure projects, and distributing vaccines.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can hover the mouse over the map to get additional information.\n\n\n\n\n\nConcerns and Criticisms\nThe National Guard has faced significant scrutiny and criticism:\nMilitarization of public security\nA primary concern is the increasing role of the military in traditionally civilian law enforcement tasks. Critics argue this blurs the lines between military and police functions and could lead to issues related to training, accountability, and human rights.\nHuman rights\nThere have been concerns about the training of National Guard members in human rights and allegations of excessive use of force, arbitrary detentions, and other abuses, particularly in the context of immigration enforcement.\nEffectiveness in reducing crime\nDespite its large deployment, some analyses suggest that the National Guard has not yet demonstrably reduced crime or violence in the country.\nAccountability Questions remain about the mechanisms for holding National Guard members accountable for misconduct, with some advocating for strong, independent civilian oversight.\nMission creep\nThe broadening scope of the National Guard‚Äôs responsibilities, extending beyond traditional law enforcement to areas like infrastructure and migration control, has raised concerns about its intended purpose and potential overreach.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can hover the mouse over the map to get additional information.\n\n\n\n\n\nConclusions\nAs of today, the National Guard continues to be a significant security force in Mexico, with a strong military component. It is actively involved in law enforcement across the country, border security operations, and various other tasks assigned by the government. The debate surrounding its role, effectiveness, and adherence to human rights standards remains ongoing. Recent deployments, such as the reinforcement of the northern border with Army troops, highlight the continued reliance on federal forces for security challenges.\n\n\nReferences\n\nStorr, S. (2023) What is Guardia Nacional?. Seguridad Ciudadana: La via Civil.\nGobierno de Mexico (2019) National Guard Personnel Deployed\n\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Articles",
      "National Guard in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web.html",
    "href": "gasoline_web.html",
    "title": "Gasoline Prices in Mexico",
    "section": "",
    "text": "Show code\nimport polars as pl\nimport duckdb as db\nimport plotly.express as px\nfrom plotnine import *"
  },
  {
    "objectID": "gasoline_web.html#top-05-states-with-highest-gasoline-prices",
    "href": "gasoline_web.html#top-05-states-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with highest gasoline prices",
    "text": "Top 05 States with highest gasoline prices\n\n\nShow code\n(\n    states\n        .top_k(5, by='sale_price')\n        .to_pandas()\n        .style\n        .hide()    \n        .format({'gasoline_cost': '${:,.2f}',\n                 'sale_price': '${:.2f}',\n                 'profit': '${:.2f}',\n                 'profit%': '{:.2%}',})\n)\n\n\n\n\n\nTop 05 States with highest prices\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nQUINTANA ROO\n$21.46\n$24.80\n$3.34\n15.55%\n\n\nYUCATAN\n$21.46\n$24.79\n$3.33\n15.52%\n\n\nNAYARIT\n$22.54\n$24.76\n$2.23\n9.90%\n\n\nGUERRERO\n$22.37\n$24.76\n$2.39\n10.67%\n\n\nSINALOA\n$22.36\n$24.72\n$2.36\n10.59%"
  },
  {
    "objectID": "gasoline_web.html#top-05-states-with-lowest-gasoline-prices",
    "href": "gasoline_web.html#top-05-states-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with lowest gasoline prices",
    "text": "Top 05 States with lowest gasoline prices\n\n\nShow code\n(\n    states\n        .bottom_k(5, by='sale_price')\n        .to_pandas()\n        .style\n        .hide()    \n        .format({'gasoline_cost': '${:,.2f}',\n                 'sale_price': '${:.2f}',\n                 'profit': '${:.2f}',\n                 'profit%': '{:.2%}',})\n)\n\n\n\n\n\nTop 05 States with lowest prices\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nTAMAULIPAS\n$21.22\n$23.10\n$1.87\n8.79%\n\n\nCOAHUILA DE ZARAGOZA\n$22.25\n$23.12\n$0.87\n4.01%\n\n\nCHIHUAHUA\n$21.47\n$23.36\n$1.89\n8.90%\n\n\nNUEVO LEON\n$21.41\n$23.40\n$1.99\n9.32%\n\n\nVERACRUZ DE IGNACIO DE LA LLAVE\n$21.59\n$23.44\n$1.86\n8.62%"
  },
  {
    "objectID": "gasoline_web.html#top-05-municipalities-with-highest-gasoline-prices",
    "href": "gasoline_web.html#top-05-municipalities-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with highest gasoline prices",
    "text": "Top 05 Municipalities with highest gasoline prices\n\n\nShow code\n(\n    municipalities\n        .top_k(5, by='sale_price')\n        .to_pandas()\n        .style\n        .hide()    \n        .format({'gasoline_cost': '${:,.2f}',\n                 'sale_price': '${:.2f}',\n                 'profit': '${:.2f}',\n                 'profit%': '{:.2%}',})\n)\n\n\n\n\n\nTop 05 Municipalities with highest prices\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nNUEVA ITALIA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nZIRACUARETIRO, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nGABRIEL ZAMORA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nHUIRAMBA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nNAHUATZEN, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%"
  },
  {
    "objectID": "gasoline_web.html#top-05-municipalities-with-lowest-gasoline-prices",
    "href": "gasoline_web.html#top-05-municipalities-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with lowest gasoline prices",
    "text": "Top 05 Municipalities with lowest gasoline prices\n\n\nShow code\n(\n    municipalities\n        .bottom_k(5, by='sale_price')\n        .to_pandas()\n        .style\n        .hide()    \n        .format({'gasoline_cost': '${:,.2f}',\n                 'sale_price': '${:.2f}',\n                 'profit': '${:.2f}',\n                 'profit%': '{:.2%}',})\n)\n\n\n\n\n\nTop 05 Municipalities with lowest prices\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nANAHUAC, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nCD. GUERRERO, TAMAULIPAS\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nLAMPAZOS DE NARANJO, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nALLENDE, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\nMORELOS, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%"
  },
  {
    "objectID": "slides_as_code.html",
    "href": "slides_as_code.html",
    "title": "Level Up Your Presentations: Embrace Slides as Code",
    "section": "",
    "text": "Level Up Your Presentations: Embrace Slides as Code\nCreate your slideshows with Marp, Python, and Quarto! üöÄ\nApr, 2025\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\nIntroduction\nTired of wrestling with drag-and-drop interfaces and endless formatting tweaks for your presentations? Ready to embrace the power of version control and reproducible results for your slides. Then it‚Äôs time to dive into the exciting world of Slides as Code! This approach treats your presentation content and styling as code, offering a more efficient, collaborative, and maintainable workflow, by using Markdown, Python, and Quarto.\n\n\nWhat‚Äôs the Magic Behind Slides as Code?\nInstead of directly manipulating visual elements, you define your slides using a lightweight markup language (like Markdown) and potentially sprinkle in code for dynamic content or visualizations.\nThis offers several key advantages:\n\nVersion Control\nTrack changes, collaborate seamlessly with Git, and easily revert to previous versions. Say goodbye to accidental deletions and confusing ‚Äúfinal_final_v3‚Äù files!\nReproducibility\nEnsure your presentations are consistent and can be easily recreated. No more wondering how you achieved that specific layout or chart.\nEfficiency\nFocus on your content rather than tedious formatting. Write in a clean, text-based format and let the tools handle the presentation rendering.\nAutomation\nGenerate slides programmatically using Python, pulling data from various sources or creating dynamic visualizations.\nPlatform Agnostic\nOften, the output formats are widely compatible (HTML, PDF, etc.), making sharing and viewing easy.\n\n\n\nMeet the Dream Team\nMarkdown\nA lightweight markup language with plain text formatting syntax. Think of it as a simplified way to add basic formatting (like bolding, italics, headings, lists, etc.) to your text using simple symbols. Instead of using complex word processors with menus and buttons, you use characters you already know on your keyboard.\nPython\nThe workhorse for data manipulation, analysis, and generating dynamic content. You can use Python scripts to fetch data, create charts with libraries like Matplotlib or Seaborn, and then seamlessly integrate these outputs into your Marp slides.\nQuarto\nThis powerful open-source scientific and technical publishing system ties everything together. Quarto can render Marp Markdown into beautiful presentations, and it also provides excellent support for embedding Python code chunks directly within your Markdown.\n\n\nHow They Work Together?\n\nContent Creation (Markdown)\nYou write your presentation content in a .md file using Marp‚Äôs Markdown syntax. You define slide breaks using ---, apply themes, and add basic formatting.\nDynamic Content (Python)\nIf you need to include data visualizations or dynamic information, you write Python code snippets within your Quarto document (which can include your Marp Markdown). These code chunks can:\n\nRead data from files or APIs.\nPerform calculations and analysis.\nGenerate plots and charts using Python libraries.\n\nRendering (Quarto)\nQuarto takes your .md file (with embedded Python if any) and renders it into a polished presentation in your desired format (e.g., HTML reveal.js slides, PDF beamer slides). Quarto handles the execution of your Python code and seamlessly integrates the output into your web presentation.\n\n\n\nConclusions\nSlides as code is the future! The Benefits are Clear. Embracing Slides as Code with Markdown, Python, and Quarto can significantly improve your presentation workflow. You‚Äôll gain better control, enhance collaboration, and create more dynamic and reproducible presentations. Ready to ditch the traditional slide editors and unlock the power of code for your presentations?\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Slides as Code"
    ]
  },
  {
    "objectID": "gasoline_web_nocode.html",
    "href": "gasoline_web_nocode.html",
    "title": "Gasoline Prices in Mexico",
    "section": "",
    "text": "Figure¬†1: Gas Prices in Mexico Report",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_nocode.html#top-05-states-with-highest-gasoline-prices",
    "href": "gasoline_web_nocode.html#top-05-states-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with highest gasoline prices",
    "text": "Top 05 States with highest gasoline prices\nThe following table presents a snapshot of the States with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nQUINTANA ROO\n$21.46\n$24.80\n$3.34\n15.55%\n\n\nYUCATAN\n$21.46\n$24.79\n$3.33\n15.52%\n\n\nNAYARIT\n$22.54\n$24.76\n$2.23\n9.90%\n\n\nGUERRERO\n$22.37\n$24.76\n$2.39\n10.67%\n\n\nSINALOA\n$22.36\n$24.72\n$2.36\n10.59%\n\n\n\n\n\n\nTable¬†2: Top 05 States with highest prices",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_nocode.html#top-05-states-with-lowest-gasoline-prices",
    "href": "gasoline_web_nocode.html#top-05-states-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with lowest gasoline prices",
    "text": "Top 05 States with lowest gasoline prices\nThe following table presents a snapshot of the States with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nTAMAULIPAS\n$21.22\n$23.10\n$1.87\n8.79%\n\n\nCOAHUILA DE ZARAGOZA\n$22.25\n$23.12\n$0.87\n4.01%\n\n\nCHIHUAHUA\n$21.47\n$23.36\n$1.89\n8.90%\n\n\nNUEVO LEON\n$21.41\n$23.40\n$1.99\n9.32%\n\n\nVERACRUZ DE IGNACIO DE LA LLAVE\n$21.59\n$23.44\n$1.86\n8.62%\n\n\n\n\n\n\nTable¬†3: Top 05 States with lowest prices",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "href": "gasoline_web_nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with highest gasoline prices",
    "text": "Top 05 Municipalities with highest gasoline prices\nThe following table presents a snapshot of the Municipalities with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nNUEVA ITALIA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nZIRACUARETIRO, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nGABRIEL ZAMORA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nHUIRAMBA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nNAHUATZEN, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\n\n\n\n\nTable¬†4: Top 05 Municipalities with highest prices",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "href": "gasoline_web_nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with lowest gasoline prices",
    "text": "Top 05 Municipalities with lowest gasoline prices\nThe following table presents a snapshot of the Municipalities with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nANAHUAC, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nCD. GUERRERO, TAMAULIPAS\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nLAMPAZOS DE NARANJO, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nALLENDE, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\nMORELOS, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\n\n\n\n\nTable¬†5: Top 05 Municipalities with lowest prices",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#current-situation",
    "href": "gasoline_web_slides.html#current-situation",
    "title": "Gasoline Prices in Mexico",
    "section": "Current Situation",
    "text": "Current Situation\n\nRising Prices\nDespite government efforts to stabilize fuel costs, gasoline prices in Mexico have been on the rise. Some estimates suggest prices could reach MX$30 per liter during 2025.\nRegional Variations\nGasoline prices vary significantly across different states in Mexico. Quintana Roo has the highest average price for regular gasoline, while Veracruz has the lowest.\nGovernment Intervention\nThe government has implemented measures like a price cap of MX$24 per liter for regular gasoline and has been working to stabilize fuel prices. However, these efforts have faced challenges.",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#factors-affecting-prices",
    "href": "gasoline_web_slides.html#factors-affecting-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Factors Affecting Prices",
    "text": "Factors Affecting Prices\n\nGlobal Oil Prices\nInternational oil prices play a significant role in determining gasoline prices in Mexico.\nGovernment Policies\nTaxes, subsidies, and regulations implemented by the Mexican government influence fuel prices.\nEconomic Conditions\nInflation, exchange rates, and other economic factors can impact gasoline prices.",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#challenges-and-concerns",
    "href": "gasoline_web_slides.html#challenges-and-concerns",
    "title": "Gasoline Prices in Mexico",
    "section": "Challenges and Concerns",
    "text": "Challenges and Concerns\n\nImpact on Low-Income Families\nRising gasoline prices put a strain on low-income households, affecting their daily expenses and overall economic well-being.\nEconomic Impact\nIncreased fuel costs can have a ripple effect across various sectors of the economy, potentially leading to inflation and affecting businesses.\nGovernment Promises\nThe government‚Äôs promises to keep gasoline prices low have been met with skepticism, as prices continue to climb.",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#top-05-states-with-highest-gasoline-prices",
    "href": "gasoline_web_slides.html#top-05-states-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with highest gasoline prices",
    "text": "Top 05 States with highest gasoline prices\nThe following table presents a snapshot of the States with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nQUINTANA ROO\n$21.46\n$24.80\n$3.34\n15.55%\n\n\nYUCATAN\n$21.46\n$24.79\n$3.33\n15.52%\n\n\nNAYARIT\n$22.54\n$24.76\n$2.23\n9.90%\n\n\nGUERRERO\n$22.37\n$24.76\n$2.39\n10.67%\n\n\nSINALOA\n$22.36\n$24.72\n$2.36\n10.59%\n\n\n\n\n\n\nTable¬†1: Top 05 States with highest prices",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#top-05-states-with-lowest-gasoline-prices",
    "href": "gasoline_web_slides.html#top-05-states-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with lowest gasoline prices",
    "text": "Top 05 States with lowest gasoline prices\nThe following table presents a snapshot of the States with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nTAMAULIPAS\n$21.22\n$23.10\n$1.87\n8.79%\n\n\nCOAHUILA DE ZARAGOZA\n$22.25\n$23.12\n$0.87\n4.01%\n\n\nCHIHUAHUA\n$21.47\n$23.36\n$1.89\n8.90%\n\n\nNUEVO LEON\n$21.41\n$23.40\n$1.99\n9.32%\n\n\nVERACRUZ DE IGNACIO DE LA LLAVE\n$21.59\n$23.44\n$1.86\n8.62%\n\n\n\n\n\n\nTable¬†2: Top 05 States with lowest prices",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#top-05-municipalities-with-highest-gasoline-prices",
    "href": "gasoline_web_slides.html#top-05-municipalities-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with highest gasoline prices",
    "text": "Top 05 Municipalities with highest gasoline prices\nThe following table presents a snapshot of the Municipalities with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nNUEVA ITALIA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nZIRACUARETIRO, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nGABRIEL ZAMORA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nHUIRAMBA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nNAHUATZEN, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\n\n\n\n\nTable¬†3: Top 05 Municipalities with highest prices",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#top-05-municipalities-with-lowest-gasoline-prices",
    "href": "gasoline_web_slides.html#top-05-municipalities-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with lowest gasoline prices",
    "text": "Top 05 Municipalities with lowest gasoline prices\nThe following table presents a snapshot of the Municipalities with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nANAHUAC, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nCD. GUERRERO, TAMAULIPAS\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nLAMPAZOS DE NARANJO, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nALLENDE, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\nMORELOS, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\n\n\n\n\nTable¬†4: Top 05 Municipalities with lowest prices",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#average-gasoline-price-by-state",
    "href": "gasoline_web_slides.html#average-gasoline-price-by-state",
    "title": "Gasoline Prices in Mexico",
    "section": "Average Gasoline Price by State",
    "text": "Average Gasoline Price by State\nThe following chart shows average gasoline prices by State in Mexico.\n\n\n\n\n\n\n\n\nFigure¬†1: Average Gasoline Prices by State",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#profit-by-state",
    "href": "gasoline_web_slides.html#profit-by-state",
    "title": "Gasoline Prices in Mexico",
    "section": "Profit by State",
    "text": "Profit by State\nThe following chart shows average profit by selling gasoline by State in Mexico.\n\n\n\n\n\n\n\n\nFigure¬†2: Profit by State",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#dispersion-of-gasoline-prices",
    "href": "gasoline_web_slides.html#dispersion-of-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Dispersion of Gasoline Prices",
    "text": "Dispersion of Gasoline Prices\nThe following chart shows the dispersion of gasoline prices by State in Mexico.\n\n\n\n\n\n\n\n\nFigure¬†3: Dispersion of Gasoline Prices by State",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#dispersion-of-profits",
    "href": "gasoline_web_slides.html#dispersion-of-profits",
    "title": "Gasoline Prices in Mexico",
    "section": "Dispersion of Profits",
    "text": "Dispersion of Profits\nThe following chart shows the dispersion of profit by selling gasoline by State in Mexico.\n\n\n\n\n\n\n\n\nFigure¬†4: Dispersion of Profits by State",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "gasoline_web_slides.html#geographical-distribution",
    "href": "gasoline_web_slides.html#geographical-distribution",
    "title": "Gasoline Prices in Mexico",
    "section": "Geographical Distribution",
    "text": "Geographical Distribution\n\n\n\n\n\n\nFigure¬†5: Gasoline Sale Prices by State in Mexico",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Gasoline Prices in Mexico"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright ¬© 2025 Jesus LM\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "bi_as_code.html",
    "href": "bi_as_code.html",
    "title": "Unleashing Business Intelligence with Python",
    "section": "",
    "text": "Shifting from Drag-and-Drop BI Tools to Code-based Applications\nMay, 2025\nJesus L. Monroy\nEconomist & Data Scientist",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#why-sql-as-a-paintbrush-for-your-tableau-dashboards",
    "href": "bi_as_code.html#why-sql-as-a-paintbrush-for-your-tableau-dashboards",
    "title": "Unleashing Business Intelligence with Python",
    "section": "Why SQL as a Paintbrush for Your Tableau Dashboards?",
    "text": "Why SQL as a Paintbrush for Your Tableau Dashboards?\n\nImagine Tableau is your canvas and the data is the paint. SQL acts as your expert color mixer, allowing you to prepare exactly the shades and proportions you need before applying them to the canvas. Here are some of the key benefits:\n\nOptimized Performance\nInstead of loading large amounts of raw data into Tableau and relying on its data engine to perform complex transformations, SQL lets you filter, aggregate, and transform data directly in the database. This means Tableau works with much smaller, optimized data sets, resulting in faster and more responsive dashboards.\nGreater Flexibility and Control\nSQL gives you granular control over your data transformation logic. You can perform complex joins, apply conditional logic, create calculated fields, and shape the data exactly as you need it for your specific visualizations. This is much more flexible than relying solely on Tableau‚Äôs capabilities for these tasks.\nReuse and Consistency\nSQL queries can be saved, versioned, and reused across multiple dashboards or even different tools. This promotes consistency in business logic and reduces work redundancy.\nScalability\nAs your data volumes grow, performing heavy transformations within Tableau can become prohibitively expensive in terms of performance. Delegating these tasks to the database via SQL allows your solution to scale more efficiently.\nData Warehouse Integration\nIf your organization already uses a data warehouse, SQL is the native language for interacting with it. Using SQL to prepare data for Tableau integrates naturally with your existing data infrastructure.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#the-ideal-workflow-sql-first-tableau-later",
    "href": "bi_as_code.html#the-ideal-workflow-sql-first-tableau-later",
    "title": "Unleashing Business Intelligence with Python",
    "section": "The Ideal Workflow: SQL First, Tableau Later",
    "text": "The Ideal Workflow: SQL First, Tableau Later\nAn efficient workflow involves:\n\nConnecting Tableau to the database\nUsing custom SQL queries or materialized views to prepare and shape the data. This includes filtering, aggregations, joins, and any other necessary transformations.\nBuilding visualizations\nUsing Tableau drag and drop from the pre-processed data.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#replacement-or-integration",
    "href": "bi_as_code.html#replacement-or-integration",
    "title": "Unleashing Business Intelligence with Python",
    "section": "Replacement or Integration?",
    "text": "Replacement or Integration?\nWhile Evidence.dev offers an attractive alternative, it doesn‚Äôt necessarily have to be a complete replacement for the SQL + Tableau combination in all cases.\nPotential Replacement\nFor teams with a strong focus on data engineering and who value full control through code, Evidence.dev could be an excellent option to replace the traditional SQL + Tableau workflow.\nStrategic Integration\nIt is also possible to integrate Evidence.dev into an existing Tableau workflow. For example, data models defined and transformed in Evidence.dev could be used as data sources for Tableau dashboards, leveraging the strengths of both tools.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#why-choose-plotly-dash-for-your-dashboards",
    "href": "bi_as_code.html#why-choose-plotly-dash-for-your-dashboards",
    "title": "Unleashing Business Intelligence with Python",
    "section": "Why Choose Plotly Dash for Your Dashboards?",
    "text": "Why Choose Plotly Dash for Your Dashboards?\nDash offers a compelling set of advantages that make it a go-to choice for building data dashboards:\nPython Native\nBuilt entirely on Python, Dash seamlessly integrates with your existing data science stack, including libraries like Pandas, NumPy, and Scikit-learn. This means you can leverage your familiar Python workflows to build sophisticated dashboards.\nInteractive and Dynamic\nDash dashboards are inherently interactive. Users can filter data, zoom into charts, hover over data points for details, and trigger updates in real-time. This level of engagement fosters deeper data exploration and understanding.\nDeclarative Syntax\nDash utilizes a declarative syntax, making it incredibly intuitive to define the layout and interactivity of your dashboard. You describe what you want to see, and Dash handles the underlying complexities of building the web application.\nRich Component Library\nPlotly provides a comprehensive library of interactive and customizable components, including charts, graphs, tables, dropdown menus, sliders, and more. These components are built on top of the robust Plotly.js library, ensuring high-quality visualizations.\nCustomizable Layouts\nDash offers flexible layout options, allowing you to arrange components precisely as needed. You can create multi-page dashboards, organize elements into rows and columns, and design visually appealing interfaces.\nDeployment Flexibility\nDash applications can be deployed on various platforms, from local machines to enterprise-level servers and cloud platforms like Heroku, AWS, and Google Cloud. This makes it easy to share your dashboards with a wider audience.\nActive Community and Extensive Documentation\nDash boasts a vibrant and supportive community, along with comprehensive and well-maintained documentation. This makes it easier to learn, troubleshoot, and find solutions to your dashboarding challenges.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#recent-improvements-and-exciting-features",
    "href": "bi_as_code.html#recent-improvements-and-exciting-features",
    "title": "Unleashing Business Intelligence with Python",
    "section": "Recent Improvements and Exciting Features",
    "text": "Recent Improvements and Exciting Features\nThe Plotly Dash ecosystem is continuously evolving, with recent updates bringing even more power and flexibility:\nPattern-Matching Callbacks\nThis feature simplifies the creation of dashboards with dynamically generated components, making it easier to handle scenarios with a variable number of inputs or outputs.\nClientside Callbacks (with JavaScript)\nWhile Dash is primarily Python-based, the introduction of clientside callbacks allows developers to write JavaScript code for specific interactive elements, leading to faster and more responsive user experiences for certain interactions.\nImproved Performance\nOngoing efforts focus on optimizing the performance of Dash applications, ensuring smoother interactions and faster loading times, especially for large datasets.\nEnhanced Theming and Styling\nRecent updates have brought more options for customizing the visual appearance of Dash applications, allowing for greater control over branding and aesthetics.\nIntegration with More Tools\nDash continues to improve its integration with other data science and web development tools, expanding its versatility.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#getting-started-with-plotly-dash",
    "href": "bi_as_code.html#getting-started-with-plotly-dash",
    "title": "Unleashing Business Intelligence with Python",
    "section": "Getting Started with Plotly Dash",
    "text": "Getting Started with Plotly Dash\nDiving into Plotly Dash is easier than you might think!\n\nFrom there, you can explore the official Dash documentation and numerous online tutorials to start building your own interactive dashboards.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#why-choose-shiny-for-python-lets-dive-into-the-advantages",
    "href": "bi_as_code.html#why-choose-shiny-for-python-lets-dive-into-the-advantages",
    "title": "Unleashing Business Intelligence with Python",
    "section": "Why Choose Shiny for Python? Let‚Äôs Dive into the Advantages",
    "text": "Why Choose Shiny for Python? Let‚Äôs Dive into the Advantages\nPython Powerhouse\nYou can seamlessly integrate your favorite Python data science libraries like Pandas, NumPy, Matplotlib, Seaborn, and Plotly directly into your dashboards. This means you can perform complex data manipulation and visualization within the same familiar environment.\nReactive Magic\nShiny‚Äôs core strength lies in its reactivity. When a user interacts with an input (like a slider or dropdown), the linked outputs (charts, tables, text) automatically update in real-time. This creates a fluid and engaging exploratory experience for your audience.\nSimplified Web Development\nSay goodbye to wrestling with HTML, CSS, and JavaScript intricacies. Shiny for Python abstracts away much of this complexity, allowing you to focus on the logic and presentation of your data. Its declarative syntax makes defining UI elements and their behavior remarkably straightforward.\nCustomizable Layouts\nTailor your dashboards to your exact needs with flexible layout options. Arrange elements in columns, rows, or use more advanced grid systems to create visually appealing and informative interfaces.\nShareability\nOnce your dashboard is built, deploying it is relatively simple. You can share it with colleagues, stakeholders, or even the wider world, enabling data-driven decision-making and communication.\nGrowing Ecosystem\nWhile still relatively new compared to its R counterpart, the Shiny for Python ecosystem is rapidly expanding. Expect to see more components, extensions, and community support emerge over time.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "bi_as_code.html#whats-improved-building-on-the-shiny-legacy",
    "href": "bi_as_code.html#whats-improved-building-on-the-shiny-legacy",
    "title": "Unleashing Business Intelligence with Python",
    "section": "What‚Äôs Improved? Building on the Shiny Legacy",
    "text": "What‚Äôs Improved? Building on the Shiny Legacy\nShiny for Python benefits from the years of experience and refinement of its R predecessor. This translates to:\nModern Pythonic Syntax\nThe Python API is designed to feel natural and intuitive for Python developers.\nLeveraging Python‚Äôs Strengths\nIt seamlessly integrates with Python‚Äôs rich ecosystem for data manipulation, scientific computing, and visualization.\nCross-Platform Compatibility\nDeploy your dashboards on various operating systems and environments.\nIf you‚Äôre eager to jump in, here‚Äôs a taste of how simple it can be:\n\nThis minimal example demonstrates how to create a simple dashboard with a dropdown to filter a bar chart.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "BI as Code"
    ]
  },
  {
    "objectID": "judiciary_elections.html",
    "href": "judiciary_elections.html",
    "title": "The Scales of Popularity: Economic Implications of Judicial Elections in Mexico",
    "section": "",
    "text": "Mar, 2025\nJesus L. Monroy\nEconomist & Data Scientist",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Judiciary Elections in Mexico"
    ]
  },
  {
    "objectID": "judiciary_elections.html#principal-agent-problem",
    "href": "judiciary_elections.html#principal-agent-problem",
    "title": "The Scales of Popularity: Economic Implications of Judicial Elections in Mexico",
    "section": "Principal-Agent Problem",
    "text": "Principal-Agent Problem\nEconomic theory highlights the principal-agent problem, where elected officials (agents) may not always act in the best interests of the voters (principals). In the context of judicial elections, this raises concerns about judges prioritizing political expediency over impartial legal interpretation.\nJudges, seeking re-election, might be incentivized to make rulings that appeal to the electorate, potentially compromising their independence and adherence to the rule of law.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Judiciary Elections in Mexico"
    ]
  },
  {
    "objectID": "judiciary_elections.html#information-asymmetry",
    "href": "judiciary_elections.html#information-asymmetry",
    "title": "The Scales of Popularity: Economic Implications of Judicial Elections in Mexico",
    "section": "Information Asymmetry",
    "text": "Information Asymmetry\nVoters often have limited information about judicial candidates‚Äô qualifications and legal philosophies. This information asymmetry can lead to decisions based on superficial factors like name recognition or campaign rhetoric, rather than substantive legal expertise.\nThis can result in the election of judges who lack the necessary competence, undermining the quality of the judiciary.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Judiciary Elections in Mexico"
    ]
  },
  {
    "objectID": "judiciary_elections.html#interest-group-influence",
    "href": "judiciary_elections.html#interest-group-influence",
    "title": "The Scales of Popularity: Economic Implications of Judicial Elections in Mexico",
    "section": "Interest Group Influence",
    "text": "Interest Group Influence\nEconomic theory recognizes the influence of interest groups in elections. Well-funded groups can exert disproportionate influence on judicial campaigns, potentially leading to judges who are beholden to specific interests rather than the public good.\nThis raises concerns about the potential for ‚Äújudicial capture,‚Äù where powerful groups manipulate the legal system to their advantage.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Judiciary Elections in Mexico"
    ]
  },
  {
    "objectID": "judiciary_elections.html#political-cycles",
    "href": "judiciary_elections.html#political-cycles",
    "title": "The Scales of Popularity: Economic Implications of Judicial Elections in Mexico",
    "section": "Political Cycles",
    "text": "Political Cycles\nElected judges may be more susceptible to political cycles, with their rulings influenced by upcoming elections. This can create instability and uncertainty in the legal system, which can deter investment and economic growth.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Judiciary Elections in Mexico"
    ]
  },
  {
    "objectID": "judiciary_elections.html#impact-on-economic-stability",
    "href": "judiciary_elections.html#impact-on-economic-stability",
    "title": "The Scales of Popularity: Economic Implications of Judicial Elections in Mexico",
    "section": "Impact on Economic Stability",
    "text": "Impact on Economic Stability\nA compromised judiciary can erode investor confidence and create an unpredictable legal environment. This can negatively impact economic stability, foreign investment, and overall economic development.\nA strong, independent judiciary is essential for enforcing contracts, protecting property rights, and ensuring fair competition, all of which are crucial for a healthy economy.",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Judiciary Elections in Mexico"
    ]
  },
  {
    "objectID": "social_security_mexico.html",
    "href": "social_security_mexico.html",
    "title": "Crowding in Social Security Services in Mexico",
    "section": "",
    "text": "Crowding in Social Security Services in Mexico\nThe Case of FOVISSSTE\nApril, 2025\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\nAbstract\nThe strain on Mexico‚Äôs public healthcare systems, particularly the Instituto de Seguridad y Servicios Sociales de los Trabajadores del Estado (ISSSTE) and the Instituto Mexicano del Seguro Social (IMSS), is a persistent and deeply concerning issue. Overcrowded waiting rooms, lengthy delays, and limited resources paint a stark picture of the challenges faced by both patients and healthcare providers.\n\nISSTE serves approximately 13.7 million Mexican government workers, retirees, their spouses, and underage children.\nTogether with the Mexican Social Security Institute (IMSS), ISSSTE provides health coverage for 55% to 60% of the total Mexican population.\nISSTE primarily focuses on providing healthcare and social services to federal, state, and municipal government employees.\n\n\n\nThe Overwhelming Crowd: A Daily Reality\nReports and personal accounts consistently describe scenes of overwhelming congestion in ISSSTE and IMSS facilities. Patients often endure hours, sometimes even days, of waiting for consultations, treatments, or surgeries. This situation leads to:\n\nExtended wait times\nDelays in receiving necessary care can exacerbate existing health conditions and lead to complications.\nReduced quality of care\nOverburdened staff may struggle to provide adequate attention to each patient.\nPatient frustration and anxiety\nThe stress of long waits and uncertain outcomes can significantly impact patients‚Äô well-being.\nIncreased risk of infections\nCrowded waiting areas can become breeding grounds for contagious diseases.\n\n\n\nRoot Causes of Overcrowding\nSeveral factors contribute to the overcrowding crisis:\n\nInsufficient infrastructure\nThe number of healthcare facilities and medical personnel has not kept pace with the growing population and its healthcare needs.\nUnderfunding\nLimited financial resources restrict the ability of ISSSTE and IMSS to expand capacity, invest in new equipment, and hire additional staff.\nIncreased prevalence of chronic diseases\nThe rising rates of diabetes, hypertension, and other chronic conditions place a significant burden on the healthcare system.\nInefficient administrative processes\nBureaucratic hurdles and delays in scheduling appointments and processing paperwork contribute to longer wait times.\nAging population\nAs the population ages, the demand for healthcare services, particularly for geriatric care, increases.\nLack of preventative care\nA focus on treating illnesses rather than preventing them leads to more people requiring intensive medical intervention.\n\n\n\nConsequences of Overcrowding\nThe consequences of overcrowding extend beyond individual patients, impacting the entire healthcare system and society:\n\nIncreased mortality and morbidity\nDelays in treatment can lead to preventable deaths and worsened health outcomes.\nEconomic burden\nLost productivity due to illness and disability places a strain on the economy.\nErosion of public trust\nFrustration with the healthcare system can erode public confidence in government institutions.\nStrain on healthcare professionals\nOverwork and burnout among doctors, nurses, and other healthcare workers can lead to staff shortages and reduced morale.\nIncreased emergency room usage\nWhen regular appointments take too long, people go to emergency rooms, further over crowding those areas.\n\n\n\nConclusions\nAddressing the overcrowding crisis requires a multifaceted approach involving increased investment in infrastructure, improved administrative efficiency, a greater focus on preventative care, and strategies to manage the growing burden of chronic diseases. It is a critical challenge that demands urgent attention to ensure the well-being of the Mexican population.\n\n\nReferences\n\nVazquez, C (2024) Understanding IMSS: Mexico‚Äôs Social Security System. Coderslink. Retrieved from website.\nVillarreal, V. (2022) Infographic | How Do Mexicans Get Healthcare?. Wilson Center. Retrieved from website.\n\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Social Security in Mexico"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Jesus LM",
    "section": "",
    "text": "For over a decade, I‚Äôve leveraged data analysis and visualization to deliver strategic value across diverse sectors like public security, e-commerce, and healthcare. I use Python and SQL and Tableau to collect, transform, and analyze raw data, creating actionable data products such as webpages, slideshows, and dashboards, ocasionally including regression and classification analyses to support informed decision-making.\nBackground in Economics (BA), Information Technology (MA) and Data Science & Machine Learning (Certification).\nCommitted to fostering data literacy, I share some data posts on Learning Data and T3CH via Medium."
  },
  {
    "objectID": "duckdb_bi.html",
    "href": "duckdb_bi.html",
    "title": "Building Efficient Dashboards for The Modern Data Scientist",
    "section": "",
    "text": "Building Efficient Dashboards for The Modern Data Scientist\nEfficient Data Exploration and Reporting with DuckDB and Evidence.dev\nMar, 2025\nJesus L. Monroy\nEconomist & Data Scientist \n\n\n\nOverview\nIn the modern data landscape, speed and agility are paramount. We‚Äôre constantly seeking tools that empower us to analyze data quickly and efficiently, without the overhead of complex cloud infrastructure. Two such tools, DuckDB and Evidence.dev, are making waves in this space, offering a powerful combination for local data analysis and reporting.\n\n\nDuckDB: The Analytical Swiss Army Knife\nDuckDB excels in handling large datasets efficiently due to its columnar storage and vectorized execution engine. It is well-suited for scenarios where you need a fast and embedded database solution within your Python application. Here are some key things you can do with it:\nExecute SQL Queries\n\nDirectly run SQL queries using duckdb.sql within your Python code.\nEfficiently work with in-memory databases, making DuckDB ideal for quick data exploration and analysis.\nDuckDB provides a full-fledged SQL dialect, enabling you to perform various data manipulation and analysis tasks using familiar SQL syntax.\nYou can use duckdb.sql to execute queries against in-memory or disk-based databases created with DuckDB.\n\nLeverage In-memory and Persistent Databases\n\nCreate in-memory databases through duckdb.connect() for immediate querying without external storage needs.\nConnect to persistent databases on disk using the same duckdb.connect() function, specifying the database path.\n\nWork with Various Data Formats\n\nRead and write data from different file formats, including CSV, Parquet, and JSON, both locally and remotely (e.g., S3 buckets).\n\nBuild Complex Queries Incrementally\n\nConstruct SQL queries step-by-step by storing the results (relations) of previous queries into variables.\nUse these stored relations in subsequent queries, allowing for modular and reusable code.\n\nUtilize DuckDB‚Äôs SQL Dialect\n\nTake advantage of DuckDB‚Äôs comprehensive SQL dialect, supporting various operations like joins, aggregations, window functions, and more.\n\nLoad and manipulate data\n\nDuckDB supports reading and writing data from various file formats like CSV, Parquet, and JSON.\nYou can load data from these files into DuckDB tables for further processing and analysis.\nDuckDB offers various functions for data manipulation, including filtering, sorting, aggregation, and joining tables.\n\nExplore and analyze data\n\nDuckDB allows you to perform exploratory data analysis (EDA) using its SQL capabilities.\nYou can write queries to calculate descriptive statistics, identify patterns, and gain insights from your data.\n\nPrototype and experiment\n\nDuckDB‚Äôs lightweight and in-process nature makes it ideal for rapid prototyping and experimentation.\nYou can quickly test and iterate on data analysis tasks without the overhead of setting up and managing a separate database server.\n\n\n\nEvidence.dev: Transforming Data into Actionable Insights\nEvidence.dev is an open-source framework that allows you to build data reports and dashboards using SQL and Markdown. It simplifies the process of turning raw data into clear, shareable insights. Key features include:\n\nSQL-First Approach\nLeverage your existing SQL skills to query data and generate visualizations.\nMarkdown-Based Reporting\nCreate interactive reports using Markdown, with embedded SQL queries and visualizations.\nLocal Development & Deployment\nDevelop reports locally and deploy them as static websites.\nData Source Flexibility\nConnect to various data sources, including databases and files.\n\n\n\nThe Power of Integration: DuckDB + Evidence.dev\nCombining DuckDB and Evidence.dev creates a powerful local data analysis and reporting pipeline. Here‚Äôs how they work together:\n\nData Ingestion\nUse DuckDB to efficiently load and transform your data from various sources.\nLocal Analysis\nPerform complex analytical queries directly on your local machine using DuckDB‚Äôs high-performance engine.\nReport Building\nConnect Evidence.dev to your DuckDB database and create interactive reports using SQL and Markdown.\nSharing Insights\nDeploy your Evidence.dev reports as static websites to share your findings with your team or stakeholders.\n\n\n\nExample Workflow\nImagine you have a large CSV file containing sales data. You can use DuckDB to:\n\nLoad the CSV into a DuckDB table\nPerform aggregations and calculate key metrics (e.g., total sales per region).\nJoin the sales data with other relevant data sources.\n\nThen, you can use Evidence.dev to:\n\nConnect to the DuckDB database\nWrite SQL queries to retrieve the calculated metrics.\nCreate visualizations (e.g., bar charts, line graphs) to represent the data.\nGenerate a report that summarizes your findings.\n\n\n\nConclusions\nDuckDB and Evidence.dev are democratizing data analysis, empowering individuals and teams to gain valuable insights without the complexity and expense of traditional data infrastructure. If you‚Äôre looking for a powerful and efficient way to analyze and report on your data, this combination is worth exploring.\n\nFaster Iteration\nLocal development with DuckDB and Evidence.dev allows for rapid iteration and experimentation.\nData Privacy\nAnalyzing sensitive data locally eliminates the need to upload it to cloud servers.\nCost-Effectiveness\nAvoid expensive cloud data warehousing and processing costs.\nSimplified Workflow\nStreamline your data analysis and reporting process with these user-friendly tools.\n\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Duckdb & Evidence"
    ]
  },
  {
    "objectID": "docs_as_code.html",
    "href": "docs_as_code.html",
    "title": "Documentation as Code: Revolutionizing Your Documentation Workflow",
    "section": "",
    "text": "Documentation as Code: Revolutionizing Your Documentation Workflow\nThe Future of Documentation is Code\nMar, 2025\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\n\n\nImage by Onose, Ejiro\n\n\n\n\nIntroduction\nLet‚Äôs be honest. Documentation often feels like the neglected stepchild of software development. It‚Äôs the thing we promise to get to ‚Äúlater,‚Äù the first thing to get cut when deadlines loom, and often ends up being outdated and unreliable. But what if we flipped the script? What if we treated documentation with the same care and rigor as our codebase?\nThat‚Äôs the core idea behind Documentation as Code.\nIt‚Äôs a paradigm shift that advocates for managing and creating documentation using the same tools, workflows, and principles we apply to writing and maintaining software. Instead of siloed documents living in shared drives or outdated wikis, your documentation lives alongside your code in your version control system (e.g., Github).You use familiar tools to write it (Markdown, AsciiDoc, or reStructuredText), build it automatically, test it, and deploy it just like any other part of your project.\n\n\nWhy should you embrace Docs as Code?\nThere are diverse benefits, including the following:\n\nVersion Control & Collaboration\nJust like your code, your documentation benefits from the power of version control. Track changes, collaborate effectively through pull requests, and easily revert to previous versions. No more wondering who made what change or struggling to merge conflicting edits.\nAutomation & Consistency\nAutomate the build process to generate consistent and well-formatted documentation. This eliminates manual steps prone to errors and ensures your documentation always reflects the latest code.\nLiving Documentation\nBy integrating documentation with your development workflow, you ensure it stays up-to-date. Changes in the code can trigger updates in the documentation, preventing the dreaded scenario of outdated or misleading information.\nImproved Discoverability\nWhen documentation lives alongside the code, it‚Äôs easier for developers (and anyone else) to find the information they need, right where they expect it.\nEnhanced Quality\nThe review process inherent in version control and pull requests applies to documentation as well. This leads to more accurate, clearer, and more comprehensive documentation.\nDeveloper Involvement\nBy using familiar tools and workflows, developers are more likely to contribute to and maintain the documentation. It becomes less of a separate chore and more of an integrated part of their development process.\nReduced Friction\nSay goodbye to clunky, separate documentation platforms. Everything is in one place, making it easier to create, update, and consume documentation.\n\n\n\nKey Principles of Docs as Code\nPlain Text Formats\nUse lightweight markup languages like Markdown or AsciiDoc for easy writing, version control, and tooling.\nVersion Control\nStore documentation alongside code in a version control system (e.g., Git).\nAutomation\nAutomate the build, testing, and deployment of documentation.\nCode Reviews\nSubject documentation changes to the same review process as code changes.\nTreat Documentation as a First-Class Citizen\nGive documentation the same level of importance and attention as your codebase.\n\n\nGetting Started with Docs as Code\nThe transition doesn‚Äôt have to be drastic. Here are a few steps you can take:\n\nChoose a Lightweight Markup Language\nExplore Markdown, AsciiDoc, or reStructuredText and pick one that suits your team‚Äôs needs.\nMove Your Documentation to Version Control\nStart by migrating your existing documentation into your project‚Äôs repository.\nImplement a Static Site Generator\nTools like Sphinx, MkDocs, Docusaurus, and Jekyll can automatically generate beautiful and searchable documentation from your plain text files.\nEncourage Collaboration\nFoster a culture where everyone feels responsible for contributing to and maintaining the documentation.\n\n\n\nConclusions\nDocumentation as Code isn‚Äôt just about the tools; it‚Äôs about a mindset shift. It‚Äôs about recognizing the crucial role documentation plays in the success of a project and treating it with the respect and rigor it deserves. By embracing this approach, you can create documentation that is accurate, up-to-date, discoverable, and a true asset to your team and users.\n\n\nReferences\n\nOnose E. (2024) Understanding Docs-as-Code. Medium. Retrieved from website.\n\nHolscher, E. (n.d.) Docs as Code. Write The Docs. Retrieved from website.\n\n\n\n\nContact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Articles",
      "Docs as Code"
    ]
  },
  {
    "objectID": "spotify.html",
    "href": "spotify.html",
    "title": "Understanding Spotify User Behavior",
    "section": "",
    "text": "Figure¬†1: Spotify, a leading digital music, podcast, and video streaming service, has revolutionized the way individuals consume audio content. By offering a vast library of on-demand content through a freemium model, Spotify has significantly shifted music consumption from ownership to access. Spotify‚Äôs expansion into podcasting and other audio content reflects its ambition to become a comprehensive audio platform.",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Understanding Spotify User Behavior"
    ]
  },
  {
    "objectID": "spotify.html#database-connection",
    "href": "spotify.html#database-connection",
    "title": "Understanding Spotify User Behavior",
    "section": "Database connection",
    "text": "Database connection\nDuckdb is a powerful tool for data analysts and developers who need to perform fast and efficient analytical queries on large datasets, especially in environments where simplicity and portability are crucial.\n\n\nShow code\n# database connection\ntoken = os.getenv('MD_TOKEN')\nconn = db.connect(f\"md:?motherduck_token={token}\")\n# create dataframe\ndf = conn.sql('select * from projects.spotify_history').pl()\nconn.close()\n\n\nPolars is a modern DataFrame library designed for speed and efficiency, offering a compelling alternative to traditional data manipulation tools.",
    "crumbs": [
      "Portfolio",
      "Reports",
      "Understanding Spotify User Behavior"
    ]
  },
  {
    "objectID": "resume.html#economist-data-scientist",
    "href": "resume.html#economist-data-scientist",
    "title": "Jesus LM",
    "section": "Economist & Data Scientist",
    "text": "Economist & Data Scientist\nFor over a decade, I‚Äôve leveraged data analysis and visualization to deliver strategic value across diverse sectors like public security, e-commerce, and healthcare. I use Python and SQL and Tableau to collect, transform, and analyze raw data, creating actionable data products such as webpages, slideshows, and dashboards, ocasionally including regression and classification analyses to support informed decision-making.\nBackground in Economics (BA), Information Technology (MA) and Data Science & Machine Learning (Certification).\nCommitted to fostering data literacy, I share some data posts on Learning Data and T3CH via Medium."
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Jesus LM",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nSecretariado Ejecutivo del Sistema Nacional de Seguridad Publica\nDeputy Director of Public Policy\nJun 2025 -\n\nDesign and implement SQL queries and Python scripts for data collection, cleaning, transformation, and analysis of statistical and geospatial analysis from diverse sources.\nAutomate the generation of reports and presentations to effectively communicate public policy performance in public security.\nDeveloped detailed documentation using Markdown for the development and usage of digital solutions.\nEngage with cross-functional stakeholders to facilitate continuous evaluation of public policy initiatives.\n\n\n\nAtos Consulting\nInformation Analyst\nMay 2024 - Mar 2025\n\nUsed Snowpark and Python for data exploration, transformation, and analysis, enabling scalable data workflows.\nDesigned intuitive Tableau dashboards to empower end-users with data-driven insights.\nCreated compelling interactive visualizations and web reports using Python to drive informed decision-making.\nCollaborated with cross-functional teams to understand business needs and translate them into data solutions.\nDeveloped engaging web slideshows with interactive tables and charts using Python to effectively communicate data-driven stories to diverse audiences.\n\n\n\nGrupo PM Soluciones\nTableau Analyst\nJun 2023 - Feb 2024\n\nUsed SQL for data manipulation and analysis, enabling data-driven decision-making.\nCreated visually appealing and informative dashboards using Tableau.\nCreated clear and concise documentation for digital products using Markdown.\nAutomated reports using Python, transitioning from manual spreadsheets and slideshows to webpages.\n\n\n\nPepsi (Digital Transformation)\nSales Digitalization Specialist\nMar 2023 - May 2023\n\nEnhanced data-driven decision-making by building interactive dashboards with Looker Studio and Google Analytics.\nUtilized SQL queries to extract, clean, and analyze relevant data from various sources.\nDeveloped a repository with Python-based web reports for stakeholders.\n\n\n\nGuardia Nacional\nInformation Analyst\nJan 2010 - Dec 2022\n\nUsed SQL for data collection and transformation.\nCreated data pipelines with Python and Google apps for management reporting.\nDesigned interactive dashboards using Looker Studio for monitoring of operational performance.\nParticipated in meetings with diverse managers to understand business problems and propose data solutions.\nCreated a digital map of operational facilities using Python, providing an interactive visual of key locations.\nCollaborated in the implementation of Google Forms and Google Sheets to streamline data collection.\nDeveloped a digital newspaper for communication among personnel, fostering an engaged workforce.\nI was promoted as deputy director of the analytical area, improving data governance and literacy."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Jesus LM",
    "section": "Education",
    "text": "Education\n\n\n\nUniversity\nCareer\nPeriod\n\n\n\n\nMIT-IDSS\nData Science & Machine Learning Certification\nmar - jun 2024\n\n\nUMOV Academy\nMaster of Information Technology\n2022 - 2023\n\n\nUniversity of Puebla\nBachelor of Economics\n2001 - 2006"
  },
  {
    "objectID": "resume.html#contact",
    "href": "resume.html#contact",
    "title": "Jesus LM",
    "section": "Contact",
    "text": "Contact\nLinkedin | Medium | Twitter |"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Jesus LM",
    "section": "Skills",
    "text": "Skills\n\n\n\nSQL\nPython\nTableau\nMarkdown\nGit & Github\n\n\n\nProblem-solving\nCollaboration\nTime management\nBusiness acumen\nStorytelling"
  },
  {
    "objectID": "resume.html#tools",
    "href": "resume.html#tools",
    "title": "Jesus LM",
    "section": "Tools",
    "text": "Tools\n\nSQL | Python | Tableau | Markdown | Git & Github"
  },
  {
    "objectID": "resume.html#section",
    "href": "resume.html#section",
    "title": "Jesus LM",
    "section": "",
    "text": "SQL | Python | Tableau | Markdown | Git & Github"
  },
  {
    "objectID": "resume.html#skills-1",
    "href": "resume.html#skills-1",
    "title": "Jesus LM",
    "section": "Skills",
    "text": "Skills\n\nProblem-solving | Collaboration | Time management | Business acumen | Storytelling"
  },
  {
    "objectID": "water_collection_slides.html#summary",
    "href": "water_collection_slides.html#summary",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Summary",
    "text": "Summary\n\n\nRainwater harvesting is a sustainable and effective way to manage water resources in Mexico.\nBy promoting water conservation and increasing water security, these systems offer a path towards a more water-resilient future.",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#what-is-scall",
    "href": "water_collection_slides.html#what-is-scall",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "What is SCALL?",
    "text": "What is SCALL?\n\nRainwater harvesting systems are a prominent water collection method in Mexico, especially in areas facing water scarcity. These systems, called ‚ÄúSistemas de Captaci√≥n de Agua de Lluvia‚Äù (SCALL) offer several benefits:\n\n\nReduced strain on aquifers: By collecting rainwater, these systems decrease reliance on groundwater sources, which are often overexploited.\nImproved water security: Especially in peri-urban areas, rainwater harvesting provides a more dependable water source, particularly during dry spells.\nCost-effective solution: Rainwater harvesting can significantly reduce water bills for households.\nEmpowering communities: Programs like ‚ÄúAgua a tu Casa‚Äù in Mexico City train women to install and maintain these systems, fostering economic opportunities.",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#environment-setting",
    "href": "water_collection_slides.html#environment-setting",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Environment setting",
    "text": "Environment setting\nImport libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport duckdb as db\nimport os\nfrom dotenv import load_dotenv\nload_dotenv('.env')\nimport folium\nfrom folium.plugins import Fullscreen\nplt.style.use('ggplot')\n\nLoad dataset\n\n# database connection\ntoken = os.getenv('MD_TOKEN')\nconn = db.connect(f\"md:?motherduck_token={token}\")\n# create dataframe\ndf = conn.sql('select * from projects.water_collection').df()\nconn.close()",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#dataset-preview",
    "href": "water_collection_slides.html#dataset-preview",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Dataset preview",
    "text": "Dataset preview\nDataset extract\n\ndf.columns = ['file', 'village', 'scall', 'territory', 'facility_date', 'capacity', 'alcaldia',\n              'suburb', 'latitude','longitude']\ndf.head(3)\n\n\n\n\n\n\n\n\nfile\nvillage\nscall\nterritory\nfacility_date\ncapacity\nalcaldia\nsuburb\nlatitude\nlongitude\n\n\n\n\n0\n1-MEMEMA-MIA-22\nSAN ANTONIO TECOMITL\nBARRIO XALTIPAC\nNO APLICA\n2022-05-02\n2500\nMILPA ALTA\nSAN ANTONIO TECOMITL (PBLO)\n19.219028\n-98.991283\n\n\n1\n2-RUCOAL-MIA-22\nSAN ANTONIO TECOMITL\nBARRIO TENANTITLA\nNO APLICA\n2022-05-07\n2500\nMILPA ALTA\nSAN ANTONIO TECOMITL (PBLO)\n19.219880\n-99.002227\n\n\n2\n3-GAALRA-MIA-22\nSAN ANTONIO TECOMITL\nBARRIO XALTIPAC\nNO APLICA\n2022-07-21\n2500\nMILPA ALTA\nSAN ANTONIO TECOMITL (PBLO)\n19.218343\n-98.997507",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#descriptive-statitistics",
    "href": "water_collection_slides.html#descriptive-statitistics",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Descriptive Statitistics",
    "text": "Descriptive Statitistics\nSummary of basic stats\n\ndf.describe(include='number')\n\n\n\n\n\n\n\n\ncapacity\nlatitude\nlongitude\n\n\n\n\ncount\n16971.000000\n16971.000000\n16971.000000\n\n\nmean\n2200.218019\n19.244556\n-99.057861\n\n\nstd\n574.321315\n0.169027\n0.069845\n\n\nmin\n1100.000000\n0.000000\n-99.245452\n\n\n25%\n2500.000000\n19.192449\n-99.089082\n\n\n50%\n2500.000000\n19.218752\n-99.029098\n\n\n75%\n2500.000000\n19.274890\n-99.003864\n\n\nmax\n2500.000000\n19.578064\n-98.961553",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#installed-capacity-by-alcaldia",
    "href": "water_collection_slides.html#installed-capacity-by-alcaldia",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Installed Capacity by Alcaldia",
    "text": "Installed Capacity by Alcaldia\nWe will group capacity by alcaldia\n\nalcaldias = (\n    df.groupby('alcaldia')['capacity']\n        .agg('sum')\n        .to_frame()\n        .reset_index()\n        .sort_values('capacity')\n)",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#installed-capacity-by-village",
    "href": "water_collection_slides.html#installed-capacity-by-village",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Installed Capacity by Village",
    "text": "Installed Capacity by Village\n\nvillages = (\n    df.groupby('village')['capacity']\n        .agg('sum')\n        .to_frame()\n        .reset_index()\n        .sort_values('capacity')\n)",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#installed-capacity-by-month",
    "href": "water_collection_slides.html#installed-capacity-by-month",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Installed Capacity by Month",
    "text": "Installed Capacity by Month\n\nmonths = (\n    df.groupby(pd.Grouper(key='facility_date', freq=\"ME\"))\n    ['capacity'].sum()\n                .to_frame()\n                .reset_index()\n)",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#villages-with-installations",
    "href": "water_collection_slides.html#villages-with-installations",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Villages with Installations",
    "text": "Villages with Installations\n\nzones = (\n    df.groupby('village')[['capacity','latitude','longitude']]\n        .agg({'capacity':'sum','latitude':'mean','longitude':'mean'})\n        .reset_index(drop=False)\n)\n\n\n\n&lt;folium.plugins.fullscreen.Fullscreen at 0x78c585ddec90&gt;",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#references",
    "href": "water_collection_slides.html#references",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "References",
    "text": "References\n\nSedema\nDataset",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#resources",
    "href": "water_collection_slides.html#resources",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Resources",
    "text": "Resources",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "water_collection_slides.html#contact",
    "href": "water_collection_slides.html#contact",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nLinkedin | Medium | Twitter",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "Water Collection"
    ]
  },
  {
    "objectID": "zumpango_slides.html#overview",
    "href": "zumpango_slides.html#overview",
    "title": "Zumpango Lake",
    "section": "Overview",
    "text": "Overview\n\nThe Zumpango lake is technically a regulatory vessel, with a capacity of 100 million cubic meters of water and an approximate surface area of 1,853 hectares.\nThe main reason for the construction of this regulatory vessel was at the time to be able to provide water to the agricultural centers of the Zumpango-Nextlalpan-Teoloyucan region.",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "RIP Lake Zumpango, Mexico"
    ]
  },
  {
    "objectID": "zumpango_slides.html#important-events",
    "href": "zumpango_slides.html#important-events",
    "title": "Zumpango Lake",
    "section": "Important Events",
    "text": "Important Events\n\n\n\nIn 2023, the Zumpango Lake stopped having water and is now almost entirely covered by crassipes water lily.\nThis site was a tourist attraction that provided employment to dozens of families in the region, who served restaurants, shops and boat services to tourists who came every weekend, but today everything is abandoned.\n\n\n\nThe more than 2 thousand hectares in which the lake extended are now easily traversable on foot, since the water level dropped so much in 2023 that it ended up drying up.\nBetween January and February 2024, large fires consumed dry grasslands and lilies, leaving poor air quality in the surrounding areas.\n\n\n\ndw.display_chart('yeXJQ')",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "RIP Lake Zumpango, Mexico"
    ]
  },
  {
    "objectID": "zumpango_slides.html#important-events-output",
    "href": "zumpango_slides.html#important-events-output",
    "title": "Zumpango Lake",
    "section": "Important Events",
    "text": "Important Events",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "RIP Lake Zumpango, Mexico"
    ]
  },
  {
    "objectID": "zumpango_slides.html#resources",
    "href": "zumpango_slides.html#resources",
    "title": "Zumpango Lake",
    "section": "Resources",
    "text": "Resources",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "RIP Lake Zumpango, Mexico"
    ]
  },
  {
    "objectID": "zumpango_slides.html#references",
    "href": "zumpango_slides.html#references",
    "title": "Zumpango Lake",
    "section": "References",
    "text": "References\n\n‚ÄúAdios a la Laguna de Zumpango; esta seca desde hace un a√±o‚Äù in La Jornada, (2024)\nLake Zumpango in Wikipedia, (2023)",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "RIP Lake Zumpango, Mexico"
    ]
  },
  {
    "objectID": "zumpango_slides.html#contact",
    "href": "zumpango_slides.html#contact",
    "title": "Zumpango Lake",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter",
    "crumbs": [
      "Portfolio",
      "Slideshows",
      "RIP Lake Zumpango, Mexico"
    ]
  },
  {
    "objectID": "portfolio2/index.html",
    "href": "portfolio2/index.html",
    "title": "Projects",
    "section": "",
    "text": "Mexico City Crime According to the Statistics\n\n\nIssues and Figures, 2019-2024\n\n\n\npython\n\n\n\nCrime in Mexico City presents a complex and evolving challenge. The city, a sprawling metropolis, grapples with a range of criminal activities, from petty theft and street-level drug offenses to organized crime and violent acts. Factors contributing to this multifaceted issue include socioeconomic disparities, corruption, and the influence of transnational criminal organizations.\n\n\n\n\n\nApr 11, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City Crime Incidence 2019-2024\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nStop Building Dashboards\n\n\nStart Understanding the White Collar Workflow\n\n\n\npython\n\nsql\n\n\n\nIn today‚Äôs data-driven world, Business Intelligence (BI) tools promise powerful insights and streamlined reporting. Yet, the humble spreadsheet and slideshow persist in the white-collar world. While BI tools manages complex analysis and visualization, spreadsheets and slideshows offer unique advantages that keep them firmly entrenched in our workflows.\n\n\n\n\n\nFeb 24, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGasoline Prices in Mexico\n\n\nFuel Price Fluctuations, A Constant Concern\n\n\n\npython\n\n\n\nGasoline prices in Mexico are a complex issue influenced by various factors. In this article we examine the gasoline prices in the 32 States that constitute Mexico and also their differences in cost, sale price and profits. Challenges remain in stabilizing prices and mitigating the impact on consumers and the economy.\n\n\n\n\n\nFeb 3, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nThis project explores how to leverage the strengths of DuckDB and MotherDuck to build a robust data processing and storage solution. DuckDB excels at fast in-memory analytics, while MotherDuck provides a scalable and cost-effective cloud data warehouse. By combining these technologies, you can achieve optimal performance for both local and cloud-based data operations.\n\n\n\n\n\nJan 13, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nData Alchemy, SQL Analysis within Python\n\n\nA Case Study using Duckdb, Polars and Plotly\n\n\n\npython\n\nsql\n\n\n\nThis project focuses on leveraging the strengths of DuckDB, Polars, and Plotly for efficient data analysis and visualization. DuckDB is used for fast in-memory data processing, Polars provides a user-friendly and high-performance DataFrame library, and Plotly offers interactive and customizable visualizations.\n\n\n\n\n\nJan 6, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Analysis in Python\n\n\nA Case Study using Duckdb, Polars and Folium\n\n\n\npython\n\n\n\nGeospatial analysis involves the application of spatial concepts and techniques to data that has geographic coordinates. With the rise of big data and the increasing availability of geospatial information, the demand for effective geospatial analysis tools has grown significantly. Python, with its rich ecosystem of libraries, has emerged as a powerful and popular choice for geospatial data scientists.\n\n\n\n\n\nJan 2, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy BI Tools Fall Short, A Failure to Capture the Office Workflow\n\n\nSlideshows and Spreadsheets Still Rule the Business World\n\n\n\npython\n\n\n\nDespite BI solutions, spreadsheets & slideshows persist in data-driven decision making.\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSports Commerce\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge Your SQL Analysis with Python and DuckDB\n\n\nRethinking Data Analysis: A New Paradigm\n\n\n\npython\n\nsql\n\n\n\nThis article explores the synergy between Python and DuckDB, a powerful in-memory database, to revolutionize SQL-based data analysis. By leveraging Python‚Äôs extensive data science ecosystem and DuckDB‚Äôs lightning-fast query execution, data professionals can significantly accelerate their workflows.\n\n\n\n\n\nNov 23, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid Cases in Americas\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSpotipy, A Python Library for Spotify Music Lovers\n\n\nDiscover your Music Habits with Python\n\n\n\npython\n\n\n\nEver wondered about the intricacies of your listening habits on Spotify? Or perhaps you‚Äôre a developer looking to build a music-related application? Enter Spotipy, a fantastic Python library that acts as a bridge between your code and the Spotify Web API.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFlying into Mexico\n\n\nA Bird‚Äôs-Eye View of Major Airports\n\n\n\npython\n\n\n\nAs a popular tourist destination, Mexico offers visitors a diverse range of experiences, from ancient ruins to pristine beaches. The country‚Äôs well-connected airport system ensures seamless travel to these captivating destinations.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Stunning Charts with Datawrapper and Python\n\n\nAn Overview\n\n\n\npython\n\n\n\nDatawrapper is a powerful online tool designed to help you create engaging and informative data visualizations. Whether you‚Äôre a journalist, researcher, or simply someone who wants to present data in a more visually appealing way, Datawrapper can make the process quick and easy.\n\n\n\n\n\nSep 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City‚Äôs Underground\n\n\nA History of Challenges and Triumphs\n\n\n\npython\n\n\n\nMexico City‚Äôs metro is the largest and busiest in Latin America, serving more than 5.5 million passengers daily in its 195 stations and 12 lines\n\n\n\n\n\nSep 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico‚Äôs Peace index 2024\n\n\n\n\n\n\npython\n\n\n\nThe Mexico Peace Index (MPI), created by the Institute for Economics and Peace (IEP), is a valuable tool for understanding peacefulness in Mexico. According to IEP, Mexico‚Äôs peacefulness has improved for 04 years in a row. However, the situation isn‚Äôt uniform. While 15 states showed improvement, 17 got worse. Drug cartel activity and political violence remain challenges.\n\n\n\n\n\nMay 17, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Data Pipelines with BigQuery and Python\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn the realm of data analytics, Extract, Transform, Load (ETL) processes play a pivotal role. They streamline the integration of data from various sources, enabling its cleaning, manipulation, and loading into target systems like BigQuery, Google‚Äôs cloud-based data warehouse. By leveraging Python‚Äôs versatility and BigQuery‚Äôs scalability, you can construct powerful ETL pipelines to prepare your data for insightful analysis.\n\n\n\n\n\nMay 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nNational Guard‚Äôs Deployed Personnel in Mexico\n\n\n\n\n\n\npython\n\n\n\nMexico‚Äôs National Guard is a relatively new security force established in 2019. It was created to address the country‚Äôs high crime rates and complement traditional law enforcement.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Charts with HoloViews\n\n\nUnveiling Insights with Less Effort by Using HoloViews\n\n\n\npython\n\n\n\nHoloViews offers a compelling alternative for data visualization in Python. With its emphasis on simplicity, flexibility, and interactivity, HoloViews empowers you to create insightful visualizations that effectively communicate your data‚Äôs story.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Dashboards with Python\n\n\nUnleash the Power of Python\n\n\n\npython\n\n\n\nPython, a versatile programming language, empowers you to create interactive dashboards that unlock the hidden potential of your data.\n\n\n\n\n\nApr 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Use Mermaid to Create Diagrams as a Data Scientist\n\n\nUse Python code as your Secret Weapon\n\n\n\npython\n\n\n\nIn this article, we will show how you can create diagrams with code within Jupyter and stop using external diagramming graphical user interfaces (GUIs) like draw.io or Lucidchart.\n\n\n\n\n\nMar 29, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico‚Äôs Political Landscape\n\n\n\n\n\n\npython\n\n\n\nIn this article, we will show an overview of Mexico‚Äôs Political Scenario and its correlation with Mexico City‚Äôs.\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Slides with Python and Quarto\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDuckDB: A Compelling Solution for Data Analysis within Python Environment\n\n\nA valuable tool for data scientists working with Python due to its speed, ease of use, and tight integration.\n\n\n\npython\n\nsql\n\n\n\nDuckDB is a fast, embedded analytical database that outstands in in-memory operations. It provides a SQL interface, making it easy for users with database querying experience.\n\n\n\n\n\nMar 18, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWater Collection System in Mexico City - 2022\n\n\nRainwater harvesting systems are a prominent water collection method in Mexico\n\n\n\npython\n\n\n\nRainwater harvesting is a sustainable and effective way to manage water resources in Mexico. By promoting water conservation and increasing water security, these systems offer a path towards a more water-resilient future.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico: A Populous Nation with a Dynamic Demographic Landscape\n\n\nAn Overview of Mexico‚Äôs Population Dynamics since 1950\n\n\n\npython\n\n\n\nMexico boasts a population of nearly 130 million, ranking it as the 10th most populous country globally. Mexico is a highly urbanized nation, with 88% of the population residing in cities. This trend is expected to continue, driven by economic opportunities and infrastructure development.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating JSON Files with Python\n\n\nAn Overview Using Pandas\n\n\n\npython\n\n\n\nThis article presents a straightforward and efficient method for reading JSON files using Python Pandas library. We will show how to import JSON data into DataFrames, enabling comprehensive analysis and exploration. This approach significantly simplifies the process of working with JSON data, making it accessible for data analysis.\n\n\n\n\n\nMar 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Lego Toys with Polars\n\n\nA quick summary about Lego bricks\n\n\n\npython\n\n\n\nThe Lego brick was invented in 1949 by Ole Kirk Christiansen, and the company has since grown to become one of the world‚Äôs leading toy manufacturers. Lego products are sold in over 140 countries, and the company has over 40,000 employees worldwide.\n\n\n\n\n\nMar 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFederal Prisons in Mexico\n\n\nA Geographical Overview of Mexico‚Äôs Federal Penal System\n\n\n\npython\n\n\n\nMexico‚Äôs federal prisons, known as Ceferesos (Centros Federales de Readaptaci√≥n Social), are a complex system facing challenges. While intended for rehabilitation, reports often highlight overcrowding, violence, and inadequate living conditions. In this article we will show the location of federal prisons in Mexico, including the famous Islas Mar√≠as, which closed in 2019. We wil use Folium, a powerful Python library to enhance data analysis and geographic visualization.\n\n\n\n\n\nMar 6, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression\n\n\nAn Extension for Linear Regression Models\n\n\n\npython\n\n\n\nIn this article, we shall show where linear regression falls short and we should use polynomial regression instead.\n\n\n\n\n\nMar 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Analysis Overview\n\n\nA quick summary about linear and nonlinear regression\n\n\n\npython\n\n\n\nWhether you want to do statistics, machine learning, or economic analysis, it‚Äôs likely that you will have to use regression analysis. Regression analysis is one of the most important fields in statistics, economics and machine learning. We shall briefly explore the different techniques about regression.\n\n\n\n\n\nFeb 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCohort Analysis\n\n\nUnderstanding Customer Behavior\n\n\n\npython\n\n\n\nCohort analysis is an extremely helpful tool that can be used to improve business practices and can effectively increase user retention if businesses implement necessary changes according to the test results. In today‚Äôs world where data is everywhere, cohort analysis is effective in extracting useful information by analyzing the behavioral patterns of customers in order to predict the future of the business. Observing cohorts over time gives insight into user experience and helps in developing better tactics.\n\n\n\n\n\nFeb 9, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the Power of Big Data with PySpark\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn today‚Äôs data-driven world, we‚Äôre constantly bombarded with massive amounts of information. Analyzing this data efficiently and effectively is crucial for businesses and researchers alike. That‚Äôs where PySpark comes in. It‚Äôs a powerful tool that brings the distributed computing capabilities of Apache Spark to the familiar and versatile Python ecosystem.\n\n\n\n\n\nJan 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMissing People in Mexico\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUS Sales Dashboard\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nLooker Studio\n\n\n\n\n\n\nlooker-studio\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nTableau Charts example\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDiving into the World of SQL\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nJesus L. Monroy\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio2/posts/tableau/looker-studio.html",
    "href": "portfolio2/posts/tableau/looker-studio.html",
    "title": "Looker Studio",
    "section": "",
    "text": "We used Looker Studio to present results about customer behavior and online sales and conversions from Google Analytics.\nGoogle Analytics is a powerful tool for any business, but it becomes especially crucial for e-commerce businesses. It provides a wealth of data and insights into how customers interact with your online store, allowing you to optimize your website and marketing efforts to drive sales."
  },
  {
    "objectID": "portfolio2/posts/tableau/looker-studio.html#summary",
    "href": "portfolio2/posts/tableau/looker-studio.html#summary",
    "title": "Looker Studio",
    "section": "",
    "text": "We used Looker Studio to present results about customer behavior and online sales and conversions from Google Analytics.\nGoogle Analytics is a powerful tool for any business, but it becomes especially crucial for e-commerce businesses. It provides a wealth of data and insights into how customers interact with your online store, allowing you to optimize your website and marketing efforts to drive sales."
  },
  {
    "objectID": "portfolio2/posts/tableau/looker-studio.html#e-commerce-weekly-dashboard",
    "href": "portfolio2/posts/tableau/looker-studio.html#e-commerce-weekly-dashboard",
    "title": "Looker Studio",
    "section": "E-Commerce Weekly Dashboard",
    "text": "E-Commerce Weekly Dashboard\n\nProject Goal: This dashboard was designed to track the principal indicators about e-commerce from websites and mobile apps.\nData Sources: The data mainly come from Google Analytics tracking websites and mobile apps.\nKey Metrics: The dashboard shows users, new users, pageviews, sessions, orders, revenue and conversion rates.\nVisualizations: The types of charts used are time series for behavior, KPI cards, bar charts for categories and tables for detailed information.\nTarget Audience: The dashboard was designed for marketing managers and their teams."
  },
  {
    "objectID": "portfolio2/posts/tableau/looker-studio.html#e-commerce-weekly-report",
    "href": "portfolio2/posts/tableau/looker-studio.html#e-commerce-weekly-report",
    "title": "Looker Studio",
    "section": "E-Commerce Weekly Report",
    "text": "E-Commerce Weekly Report\n\nProject Goal: This report was designed to track the principal indicators about e-commerce from websites and mobile apps.\nData Sources: The data mainly come from Google Analytics tracking websites and mobile apps.\nKey Metrics: The report shows users, new users, pageviews, sessions, orders, revenue and conversion rates by year and month.\nVisualizations: The types of charts used are sparklines for behavior of the different metrics.\nTarget Audience: The report was designed for the marketing team for operational monitoring."
  },
  {
    "objectID": "portfolio2/posts/tableau/looker-studio.html#references",
    "href": "portfolio2/posts/tableau/looker-studio.html#references",
    "title": "Looker Studio",
    "section": "References",
    "text": "References\n\nAdvantages of Looker Studio"
  },
  {
    "objectID": "portfolio2/posts/tableau/looker-studio.html#contact",
    "href": "portfolio2/posts/tableau/looker-studio.html#contact",
    "title": "Looker Studio",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/tableau/sports.html",
    "href": "portfolio2/posts/tableau/sports.html",
    "title": "Sports Commerce",
    "section": "",
    "text": "This dashboard was designed to show sales by store and time intervals.\n\nData Sources: The data mainly come from sales department.\nKey Metrics: The dashboard shows sales by stores, quantities of items sold and sales by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories.\nTarget Audience: The dashboard was designed for sales department team with a sales target parameter to analyze which stores exceeds the target and which keeps below."
  },
  {
    "objectID": "portfolio2/posts/tableau/sports.html#overview",
    "href": "portfolio2/posts/tableau/sports.html#overview",
    "title": "Sports Commerce",
    "section": "",
    "text": "This dashboard was designed to show sales by store and time intervals.\n\nData Sources: The data mainly come from sales department.\nKey Metrics: The dashboard shows sales by stores, quantities of items sold and sales by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories.\nTarget Audience: The dashboard was designed for sales department team with a sales target parameter to analyze which stores exceeds the target and which keeps below."
  },
  {
    "objectID": "portfolio2/posts/tableau/sports.html#contact",
    "href": "portfolio2/posts/tableau/sports.html#contact",
    "title": "Sports Commerce",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html",
    "href": "portfolio2/posts/tableau/sundries.html",
    "title": "Tableau Charts example",
    "section": "",
    "text": "In this section we present some examples of charts created with Tableau.\nKeep in mind that dashboards are instrinsically ‚Äúexploratory‚Äù, i.e.¬†you need to¬†actively explore and interact by yourself with dashboards. There is no guided path."
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#summary",
    "href": "portfolio2/posts/tableau/sundries.html#summary",
    "title": "Tableau Charts example",
    "section": "",
    "text": "In this section we present some examples of charts created with Tableau.\nKeep in mind that dashboards are instrinsically ‚Äúexploratory‚Äù, i.e.¬†you need to¬†actively explore and interact by yourself with dashboards. There is no guided path."
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#dynamics-of-personnel-deployed-in-mexico",
    "href": "portfolio2/posts/tableau/sundries.html#dynamics-of-personnel-deployed-in-mexico",
    "title": "Tableau Charts example",
    "section": "Dynamics of Personnel Deployed in Mexico",
    "text": "Dynamics of Personnel Deployed in Mexico\n\n\n\n\n\n\n\nNote\n\n\n\nClick below to wath the video"
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#example-of-cloud-chart",
    "href": "portfolio2/posts/tableau/sundries.html#example-of-cloud-chart",
    "title": "Tableau Charts example",
    "section": "Example of cloud chart",
    "text": "Example of cloud chart"
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#profits-by-year-and-category",
    "href": "portfolio2/posts/tableau/sundries.html#profits-by-year-and-category",
    "title": "Tableau Charts example",
    "section": "Profits by year and category",
    "text": "Profits by year and category"
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#profitloss-by-sub-category",
    "href": "portfolio2/posts/tableau/sundries.html#profitloss-by-sub-category",
    "title": "Tableau Charts example",
    "section": "Profit/Loss by sub-category",
    "text": "Profit/Loss by sub-category"
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#profitloss-in-function-of-minimum-expected-level-by-category",
    "href": "portfolio2/posts/tableau/sundries.html#profitloss-in-function-of-minimum-expected-level-by-category",
    "title": "Tableau Charts example",
    "section": "Profit/Loss in function of minimum expected level by category",
    "text": "Profit/Loss in function of minimum expected level by category"
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#identification-of-products-by-profit-or-loss-contribution",
    "href": "portfolio2/posts/tableau/sundries.html#identification-of-products-by-profit-or-loss-contribution",
    "title": "Tableau Charts example",
    "section": "Identification of products by profit or loss contribution",
    "text": "Identification of products by profit or loss contribution"
  },
  {
    "objectID": "portfolio2/posts/tableau/sundries.html#contact",
    "href": "portfolio2/posts/tableau/sundries.html#contact",
    "title": "Tableau Charts example",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/tableau/us-sales.html",
    "href": "portfolio2/posts/tableau/us-sales.html",
    "title": "US Sales Dashboard",
    "section": "",
    "text": "This dashboard was designed to track sales, profits and customers by categories, months and locations.\n\nData Sources: The data mainly come from the sales department with a detailed information about sales, customers and profits categories.\nKey Metrics: The dashboard shows sales, profit and customers KPIs.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for sales managers and their teams."
  },
  {
    "objectID": "portfolio2/posts/tableau/us-sales.html#overview",
    "href": "portfolio2/posts/tableau/us-sales.html#overview",
    "title": "US Sales Dashboard",
    "section": "",
    "text": "This dashboard was designed to track sales, profits and customers by categories, months and locations.\n\nData Sources: The data mainly come from the sales department with a detailed information about sales, customers and profits categories.\nKey Metrics: The dashboard shows sales, profit and customers KPIs.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for sales managers and their teams."
  },
  {
    "objectID": "portfolio2/posts/tableau/us-sales.html#contact",
    "href": "portfolio2/posts/tableau/us-sales.html#contact",
    "title": "US Sales Dashboard",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/big-query.html#extract-phase",
    "href": "portfolio2/posts/python/big-query.html#extract-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Extract Phase",
    "text": "Extract Phase\nThe extraction phase entails retrieving data from your source. This may involve interacting with:\n\nFlat files\nDatabases\nXML files\nAPIs\nOther\n\n\nFlat files\n\n\nCode\nusers = (\n    pl.read_csv('users.csv', dtypes={'phone': pl.Utf8, 'id_atg':pl.Utf8})\n    .with_columns(\n        pl.col('entry_data').str.strptime(pl.Datetime, strict=False)\n    )\n)\n\n\n\n\nCode\nusers_profiling = (\n    # read csv file\n    pl.read_csv('profiles.csv', dtypes={'contact_phone': pl.Utf8,'post_code':pl.Utf8})\n        # change column dtypes\n    .with_columns(\n        pl.col('entry_data','entry_data_gep','update_date').str.strptime(pl.Datetime, strict=False),\n        pl.col('contact_phone').cast(pl.Utf8),\n    )\n)\n\n\n\n\nCode\norders = (\n    pl.read_csv('orders.csv').with_columns(\n        pl.col('entry_date','delivery_date').str.strptime(pl.Datetime, strict=False)\n    )\n)\n\n\n\n\nCode\norder_details = (\n    pl.read_csv('order-details.csv')\n)\n\n\n\n\nCode\npromotions = (\n    pl.read_csv('promotions.csv', dtypes={'short_description':pl.Utf8})\n    .with_columns(pl.col('key').cast(pl.Utf8))\n).unique(subset='key')\n\n\n\n\nCode\norder_status = pl.read_csv('order-status.csv')\n\n\n\n\nCode\nsocial_networks = (\n    pl.read_csv('social_networks.csv').select(\n        pl.col('id_social_network','social_network','description')\n    )\n)\n\n\n\n\nParquet files\n\n\nCode\ntypes = pl.read_parquet('types.parquet')\n\n\n\n\nCode\ngenre = pl.read_parquet('genre.parquet')\n\n\n\n\njson files\n\n\nCode\nwarehouses = pl.read_json('warehuse_catalog.json')\n\n\n\n\nDatabases\n\n\nCode\n# Connection to MS Access\nconn = pyodbc.connect(r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'r'DBQ=C:\\Users\\user\\folder\\file.accdb;')\n# Create cursor\ncursor = conn.cursor()\n\n\n\n\nCode\n# Write query\nquery = 'select * from sales where year=2022'\n# Convert to pandas dataframe\ndf = pd.read_sql(query, con=conn)\ndf.head()"
  },
  {
    "objectID": "portfolio2/posts/python/big-query.html#transform-phase",
    "href": "portfolio2/posts/python/big-query.html#transform-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Transform Phase",
    "text": "Transform Phase\nCleanse, validate, and manipulate the extracted data based on your analysis requirements. This might include:\n\nData Cleaning\n\nHandle missing values, inconsistent formatting, or errors.\n\nData Type Conversion\n\nEnsure consistent data types for columns based on their intended use in BigQuery.\n\nFiltering/Aggregation\n\nSelect or aggregate specific data subsets for targeted analysis.\n\nEnrichment\n\nMerge extracted data with additional sources to enhance its value.\n\n\nCode\n# join types, orders, order_details, promotions and warehouses\nsheet = (\n    types.join(orders, on='id_type', how='left')\n    .join(order_details, on='id_order', how='left')\n    .join(promotions, on='key', how='left')\n    .join(warehouses, on='id_warehouse', how='left')\n).rename({'id_warehouse':'id_warehouse_promo', 'active':'promo_active'})"
  },
  {
    "objectID": "portfolio2/posts/python/big-query.html#load-phase",
    "href": "portfolio2/posts/python/big-query.html#load-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Load Phase",
    "text": "Load Phase\nThere are two primary options for loading data:\n\nStaging Table\n\nCreate a staging table and load the transformed data into it for temporary storage before validating and potentially modifying it:\n\nDirect Load\n\nLoad the data directly into your target table, bypassing the staging step. However, this approach can be less flexible for complex transformations:\n\n\nCode\n# create dataset\nclient.create_dataset('database')\n\n\nDataset(DatasetReference('gepp-538', 'database'))\n\n\n\n\nCode\n# convert to pandas\nsheet = sheet.to_pandas()\n# upload to big query\nsheet.to_gbq('dw.transformation.catalog',\n                    project_id='repository-538',\n                    if_exists='replace',\n                    credentials=bq_credentials)\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 7626.01it/s]"
  },
  {
    "objectID": "portfolio2/posts/python/big-query.html#execute-queries-from-big-query",
    "href": "portfolio2/posts/python/big-query.html#execute-queries-from-big-query",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Execute queries from Big Query",
    "text": "Execute queries from Big Query\n\n\nCode\n# create sql query\nquery = '''\n    SELECT *\n    FROM `dw.transformation.catalog`\n'''\n# convert query to pandas dataframe\ncatalog = pd.read_gbq(query, credentials=bq_credentials)"
  },
  {
    "objectID": "portfolio2/posts/python/big-query.html#contact",
    "href": "portfolio2/posts/python/big-query.html#contact",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/ceferesos.html",
    "href": "portfolio2/posts/python/ceferesos.html",
    "title": "Federal Prisons in Mexico",
    "section": "",
    "text": "Figure¬†1: Cefereso No.¬†12 Vehicule Gate"
  },
  {
    "objectID": "portfolio2/posts/python/ceferesos.html#main-functions",
    "href": "portfolio2/posts/python/ceferesos.html#main-functions",
    "title": "Federal Prisons in Mexico",
    "section": "Main Functions",
    "text": "Main Functions\n\nAdministration of the Federal Penitentiary System\nResponsible for the management and operation of federal social rehabilitation centers (CEFERESOS) throughout the country.\nImplementation of social reintegration policies\nDevelops and implements programs and strategies to facilitate the reintegration of inmates into society through educational, employment, sports, and health activities.\nGuarantee of Human Rights\nEnsures respect for the human rights of persons deprived of their liberty, ensuring decent living conditions in prisons.\nCrime Prevention\nContributes to crime prevention through the implementation of effective social reintegration programs and collaboration with other public security institutions."
  },
  {
    "objectID": "portfolio2/posts/python/ceferesos.html#importance",
    "href": "portfolio2/posts/python/ceferesos.html#importance",
    "title": "Federal Prisons in Mexico",
    "section": "Importance",
    "text": "Importance\nThe OADPRS plays a fundamental role in the criminal justice system, as social rehabilitation is a key element in reducing recidivism and strengthening public security. Its work is essential to ensuring that persons deprived of their liberty have the opportunity to reintegrate into society in a productive and law-abiding manner. The work of the OADPRS is monitored by the National Human Rights Commission to ensure that inmates‚Äô rights are respected."
  },
  {
    "objectID": "portfolio2/posts/python/ceferesos.html#challenges",
    "href": "portfolio2/posts/python/ceferesos.html#challenges",
    "title": "Federal Prisons in Mexico",
    "section": "Challenges",
    "text": "Challenges\nThe OADPRS faces significant challenges, such as overcrowding in some prisons, the need to improve inmates‚Äô living conditions, and the fight against corruption. In addition, the CNDH has issued various recommendations stemming from the lack of timely medical care within penitentiary centers.\nIn short, the OADPRS is a crucial institution for public safety and criminal justice in Mexico, responsible for administering the federal penitentiary system and promoting the social reintegration of inmates."
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html",
    "href": "portfolio2/posts/python/gdrive_project.html",
    "title": "Stop Building Dashboards",
    "section": "",
    "text": "Figure¬†1: ETL Phases"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#overview",
    "href": "portfolio2/posts/python/gdrive_project.html#overview",
    "title": "Stop Building Dashboards",
    "section": "Overview",
    "text": "Overview\nSome reasons why spreadsheets and slideshows persist in the office workflow, include:\n\nFamiliarity and Ease of Use\nFlexibility and Control\nStorytelling and Communication\nCollaboration and Sharing\nAd-hoc Analysis and Exploration\nCost and Accessibility\n\nWhile BI tools may be transforming the way we analyze data, it‚Äôs clear that spreadsheets and slideshows aren‚Äôt going anywhere anytime soon. They serve a different purpose, filling a gap that BI tools often miss.\nBuilding a Python-Powered Data Pipeline\nBusiness Intelligence (BI) tools are powerful, but they can also be expensive and complex. What if you could build a custom, flexible, and potentially more cost-effective solution using Python?\nThis post explores how you can leverage Python to collect, transform, and deliver data to Spreadsheets and Slides for compelling presentations. Why Python?\nPython has become a powerhouse in data science and automation. Its rich ecosystem of libraries makes it ideal for data manipulation, and seamless slides interaction. This combination offers a powerful alternative to traditional BI tools for certain use cases.\nIn this post we shall propose the use of Python (to collect, cleanse and transform data), Google Spreadsheets (to store transformed data) and Google Slides (to showcase visualizations). Proposed Workflow\nImagine you need to generate a weekly sales report and all you have to do is to run the next command:\n%%bash\njupyter-execute ./projects/weekly-report.ipynb\nAnd, voila! you have your weekly report updated and ready to present in Google Slides."
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#environment-settings",
    "href": "portfolio2/posts/python/gdrive_project.html#environment-settings",
    "title": "Stop Building Dashboards",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nShow code\n# Import authenticator and gspread to manage g-sheets\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\n\n# Import other libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow code\n# get token\nfilename = 'credentials.json'\n\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n\n# read credentials\ntoken = keys['md_token']"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#extract-phase",
    "href": "portfolio2/posts/python/gdrive_project.html#extract-phase",
    "title": "Stop Building Dashboards",
    "section": "Extract Phase",
    "text": "Extract Phase\n\n\nShow code\n# connect to motherduck cloud\nconn = db.connect(f'md:?motherduck_token={token}')\n\n\n\n\nShow code\nconn.sql('show databases')\n\n\n\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     database_name     ‚îÇ\n‚îÇ        varchar        ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ md_information_schema ‚îÇ\n‚îÇ my_db                 ‚îÇ\n‚îÇ my_portfolio          ‚îÇ\n‚îÇ sample_data           ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\nTable¬†1: Databases\n\n\n\n\n\n\nShow code\n# select specific database\nconn.sql('use my_portfolio')\n\n\n\n\nShow code\n# show tables in database\nconn.sql('show tables')\n\n\n\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       name       ‚îÇ\n‚îÇ     varchar      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ airports         ‚îÇ\n‚îÇ appl_stock       ‚îÇ\n‚îÇ cdmx_subway      ‚îÇ\n‚îÇ colors           ‚îÇ\n‚îÇ contains_null    ‚îÇ\n‚îÇ houses           ‚îÇ\n‚îÇ people           ‚îÇ\n‚îÇ prevalencia      ‚îÇ\n‚îÇ restaurants      ‚îÇ\n‚îÇ retail_sales     ‚îÇ\n‚îÇ sales            ‚îÇ\n‚îÇ sales_info       ‚îÇ\n‚îÇ sets             ‚îÇ\n‚îÇ water_collection ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     14 rows      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\nTable¬†2: Tables in database\n\n\n\n\n\n\nShow code\n# dataset\ndataset = conn.sql('select * from restaurants').df()\n\n(\n    dataset.head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nGreek Gyros\n770\n$68.39\nNew York\nFrench\n1\n\n\nTaste of Italy\n4,420\n$88.23\nAmsterdam\nChinese\n0\n\n\nMidnight Diner\n2,155\n$12.97\nLisbon\nMexican\n1\n\n\nTaste of Italy\n3,375\n$52.79\nSydney\nChinese\n1\n\n\n\n\n\n\nTable¬†3: Dataset Preview"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#transform-phase",
    "href": "portfolio2/posts/python/gdrive_project.html#transform-phase",
    "title": "Stop Building Dashboards",
    "section": "Transform Phase",
    "text": "Transform Phase\n\nWhich restaurant chain has the maximum number of restaurants?\n\n\nShow code\nchains = (\n    conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n    ''').df()\n)\nchains\n\n\n\n\n\n\n\n\n\n\n\n\nname\nno_of_chains\n\n\n\n\n0\nThe Burger Joint\n721\n\n\n1\nPizza Palace\n703\n\n\n2\nGreek Gyros\n696\n\n\n3\nCafe Delight\n692\n\n\n4\nFrench Delights\n681\n\n\n5\nThe BBQ Shack\n671\n\n\n6\nThe Golden Wok\n667\n\n\n7\nOcean Breeze\n665\n\n\n8\nSpice & Bloom\n665\n\n\n9\nMidnight Diner\n657\n\n\n\n\n\n\n\n\nTable¬†4: Data grouped by restaurant chains\n\n\n\n\n\n\nWhich restaurant chain has generated maximum revenue?\n\n\nShow code\nrevenue = (\n    conn.sql('''\n    select name, sum(rating_count * cost) as revenue\n    from restaurants\n    group by name\n    order by revenue DESC\n    limit 10\n    ''').df()\n)\n\n(\n    revenue\n        .style\n        .hide()    \n        .format({'revenue': '{:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrevenue\n\n\n\n\nThe Burger Joint\n108,820,424.64\n\n\nPizza Palace\n100,382,853.92\n\n\nCafe Delight\n98,037,063.93\n\n\nGreek Gyros\n96,403,445.25\n\n\nThe BBQ Shack\n96,286,414.02\n\n\nOcean Breeze\n95,664,612.77\n\n\nThe Golden Wok\n94,860,125.75\n\n\nSpice & Bloom\n91,824,854.56\n\n\nFrench Delights\n91,236,701.38\n\n\nMidnight Diner\n91,170,162.30\n\n\n\n\n\n\nTable¬†5: Data grouped by restaurant and revenue\n\n\n\n\n\n\nWhich city has generated maximum revenue?\n\n\nShow code\ncities = (\n    conn.sql('''\n    select city, sum(rating_count * cost) as revenue\n    from restaurants\n    group by city\n    order by revenue DESC\n    limit 10\n    ''').df()\n)\n\n(\n    cities\n        .style\n        .hide()    \n        .format({'revenue': '${:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nrevenue\n\n\n\n\nAmsterdam\n$148,839,878.62\n\n\nTokyo\n$148,035,421.32\n\n\nMadrid\n$141,487,618.97\n\n\nParis\n$141,219,374.56\n\n\nLondon\n$140,876,613.54\n\n\nRome\n$139,622,129.63\n\n\nNew York\n$138,621,609.28\n\n\nLisbon\n$136,814,247.27\n\n\nBerlin\n$136,434,163.25\n\n\nSydney\n$131,656,513.97\n\n\n\n\n\n\nTable¬†6: Data grouped by city and revenue"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#load-phase",
    "href": "portfolio2/posts/python/gdrive_project.html#load-phase",
    "title": "Stop Building Dashboards",
    "section": "Load Phase",
    "text": "Load Phase\n\n\nShow code\n# Create scope to authenticate\nSCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n\n# Read credentials\nGOOGLE_SHEETS_KEY_FILE = 'arkham-538.json'\ncredentials = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_SHEETS_KEY_FILE, SCOPES)\ngc = gspread.authorize(credentials)\n\n\n\n\nShow code\nimport pytz\nimport datetime\n\ntz = pytz.timezone('America/Mexico_City')\nupdate = datetime.datetime.now(tz).strftime('%b %d, %Y')\nperiod = update\n\n\n\n\nShow code\ndef save_to_gsheets(df, sheet_name, worksheet_name, period):\n    creds = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_SHEETS_KEY_FILE, SCOPES)\n    client = gspread.authorize(creds)\n    sheet = client.open(sheet_name)\n    worksheet = sheet.worksheet(worksheet_name)\n\n    # Convert datetimes to strings in advance\n    for column in df.columns[df.dtypes == 'datetime64[ns]']:\n        df[column] = df[column].astype(str)\n\n    # Prepare data for batch update\n    data = [df.columns.values.tolist()] + df.fillna('').values.tolist()\n\n    # Freeze rows and update cell values with a single batch update\n    worksheet.freeze(4)\n    worksheet.update('A4:M', data)\n\n    #fija fecha de consulta o actualizacion\n    update_data = {\n    'Last update': [\n        period,]\n    }\n\n    # convert to dataframe\n    update_data = pd.DataFrame(update_data, columns=['Last update'])\n\n    worksheet.update([update_data.columns.values.tolist()] + update_data.fillna('').values.tolist(),'A1:A2',)\n\n    print(f'DataFrame uploaded to: workbook: {sheet_name}, sheet: {worksheet_name}')\n\n\n\n\nShow code\nsave_to_gsheets(dataset, 'restaurants', 'data', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: data\n\n\n\n\nShow code\nsave_to_gsheets(chains, 'restaurants', 'chains', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: chains\n\n\n\n\nShow code\nsave_to_gsheets(revenue, 'restaurants', 'revenue', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: revenue\n\n\n\n\nShow code\nsave_to_gsheets(cities, 'restaurants', 'cities', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: cities"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#close-connection",
    "href": "portfolio2/posts/python/gdrive_project.html#close-connection",
    "title": "Stop Building Dashboards",
    "section": "Close connection",
    "text": "Close connection\n\n\nShow code\n# close connection\nconn.close()"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#retrieve-data-from-gsheets",
    "href": "portfolio2/posts/python/gdrive_project.html#retrieve-data-from-gsheets",
    "title": "Stop Building Dashboards",
    "section": "Retrieve data from gsheets",
    "text": "Retrieve data from gsheets\n\n\nShow code\n# Access worksheet id\ndf_id = '1JNAWb2QkFwh61v7QwEEVZnNhTPS0csbdMdll9y1csEg'\ndf_workbook = gc.open_by_key(df_id)\n# Access data by worksheet sheet\ndf = df_workbook.worksheet('data')\n# Save data to table\ndf = df.get_all_values()\n# Save accessed data from google sheets to dataframe\ndf = pd.DataFrame(df[1:], columns=df[0])\n\n\n\n\nShow code\ndf.head()\n\n\n\n\n\n\n\n\n\n\n\n\nLast update\n\n\n\n\n\n\n\n\n\n0\nFeb 24, 2025\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n3\nThe Golden Wok\n1477\n33.62048759\nBerlin\nAmerican\n5\n\n\n4\nGreek Gyros\n770\n68.38887409\nNew York\nFrench\n1\n\n\n\n\n\n\n\n\nTable¬†7: Data Saved on Gogle Sheets"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#google-sheets-report-data",
    "href": "portfolio2/posts/python/gdrive_project.html#google-sheets-report-data",
    "title": "Stop Building Dashboards",
    "section": "Google Sheets Report Data",
    "text": "Google Sheets Report Data\n\n\n\n\n\n\nFigure¬†2: Google Sheets Data for Presentation Report"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#sync-between-google-sheets-and-google-slides",
    "href": "portfolio2/posts/python/gdrive_project.html#sync-between-google-sheets-and-google-slides",
    "title": "Stop Building Dashboards",
    "section": "Sync between Google Sheets and Google Slides",
    "text": "Sync between Google Sheets and Google Slides\nSimply we copy and paste with sync for each table and chart and customize our slides.\n\n\n\n\n\n\nFigure¬†3: Synchronization between Google Sheets and Slides\n\n\n\n\n\n\n\n\n\nGoogle Slides\n\n\n\nYou can see the final report on Google Slides"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#conclusions",
    "href": "portfolio2/posts/python/gdrive_project.html#conclusions",
    "title": "Stop Building Dashboards",
    "section": "Conclusions",
    "text": "Conclusions\nWhile BI tools are valuable, Python offers a compelling alternative for building custom data pipelines. By leveraging the power of Python using polars and duckdb libraries for data collection and transformation, and libraries like plotly for visualization you can create a flexible, cost-effective, and automated solution for delivering data to Google Spreadsheets, using gspread, and Google Slides for impactful presentations, by sync between these Google apps.\nThis approach empowers you to take control of your data and create highly tailored reporting solutions by replacing BI license costs."
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#references",
    "href": "portfolio2/posts/python/gdrive_project.html#references",
    "title": "Stop Building Dashboards",
    "section": "References",
    "text": "References\n\nBusiness (2023) How to Design a Dashboard Presentation: A Step-by-Step Guide in slidemodel.com\nKarlson, P. (2022) Are Spreadsheets Secretly Running Your Business? in Forbes\nMonroy, Jesus (2024) Why BI Tools Fall Short: PowerPoint and Excel Still Rule the Business World in Medium\nMoore J. (2024) But, Can I Export it to Excel? in Do Mo(o)re with Data\nSchwab, P. (2021) Excel dominates the business world‚Ä¶ and that‚Äôs not about to change in Into the Minds"
  },
  {
    "objectID": "portfolio2/posts/python/gdrive_project.html#contact",
    "href": "portfolio2/posts/python/gdrive_project.html#contact",
    "title": "Stop Building Dashboards",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html",
    "href": "portfolio2/posts/python/crimes-web.html",
    "title": "Mexico City Crime According to the Statistics",
    "section": "",
    "text": "Figure¬†1: Mexico City Crimes"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#import-libraries",
    "href": "portfolio2/posts/python/crimes-web.html#import-libraries",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Import libraries",
    "text": "Import libraries\n\n\nShow code\n#import libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport datetime as datetime\nimport json\nimport plotly.express as px\nimport folium\nfrom folium.plugins import Fullscreen"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#database-connection",
    "href": "portfolio2/posts/python/crimes-web.html#database-connection",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Database connection",
    "text": "Database connection\n\n\nShow code\n# get token\nfilename = 'credentials.json'\n\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n\n# read credentials\ntoken = keys['md_token']\n\n\nDuckdb is a powerful tool for data analysts and developers who need to perform fast and efficient analytical queries on large datasets, especially in environments where simplicity and portability are crucial.\n\n\nShow code\n# connect to motherduck cloud\nconn = db.connect(f'md:?motherduck_token={token}')\n\n\n\n\nShow code\n# retrieve dataframe\ndf = conn.execute('select * from projects.cdmx_fgj_uncleaned').pl()\n\n\n\n\nShow code\n# close connection\nconn.close()\n\n\n\n\nShow code\nprint(f'The dataset contains {df.shape[0]:,.0f} rows')\n\n\nThe dataset contains 1,415,763 rows"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#missing-values",
    "href": "portfolio2/posts/python/crimes-web.html#missing-values",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Missing values",
    "text": "Missing values\n\n\nShow code\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1415763 entries, 0 to 1415762\nData columns (total 22 columns):\n #   Column             Non-Null Count    Dtype  \n---  ------             --------------    -----  \n 0   anio_inicio        1415748 non-null  float64\n 1   mes_inicio         1415748 non-null  object \n 2   fecha_inicio       1415748 non-null  object \n 3   hora_inicio        1415748 non-null  object \n 4   anio_hecho         1415343 non-null  float64\n 5   mes_hecho          1415343 non-null  object \n 6   fecha_hecho        1415342 non-null  object \n 7   hora_hecho         1415353 non-null  object \n 8   delito             1415749 non-null  object \n 9   categoria_delito   1415749 non-null  object \n 10  sexo               1168059 non-null  object \n 11  edad               931053 non-null   float64\n 12  tipo_persona       1408196 non-null  object \n 13  calidad_juridica   1415748 non-null  object \n 14  competencia        1415749 non-null  object \n 15  colonia_hecho      1340796 non-null  object \n 16  colonia_catalogo   1323317 non-null  object \n 17  alcaldia_hecho     1413284 non-null  object \n 18  alcaldia_catalogo  1372214 non-null  object \n 19  municipio_hecho    1413284 non-null  object \n 20  latitud            1340994 non-null  float64\n 21  longitud           1340994 non-null  float64\ndtypes: float64(5), object(17)\nmemory usage: 237.6+ MB\n\n\n\n\nShow code\ndf = df.dropna(subset=['latitud'])\n\n\n\n\nShow code\nprint(f'The dataset now has {df.shape[0]:,.0f} rows')\n\n\nThe dataset now has 1,340,994 rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the original dataset includes 1,415,763 rows.\nHowever, there are many null values in different fields for considerable gaps.\nAs the dataset can be considered large, we decided to drop all null latitude rows, obtaining 1,340,994 rows.\n\n\n\n\nDue to the missing rows in many fields, we kept around with 94.72% of the original dataset"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#polars-dataframe",
    "href": "portfolio2/posts/python/crimes-web.html#polars-dataframe",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Polars dataframe",
    "text": "Polars dataframe\nPolars is a modern DataFrame library that prioritizes performance and efficiency. Its Rust-based architecture, combined with features like lazy evaluation and parallel processing, makes it a powerful tool for data professionals.\n\n\nShow code\n# convert to polars dataframe\ndf = pl.from_pandas(df)\n\n\n\n\nShow code\ndf.schema\n\n\nSchema([('anio_inicio', Float64),\n        ('mes_inicio', String),\n        ('fecha_inicio', String),\n        ('hora_inicio', String),\n        ('anio_hecho', Float64),\n        ('mes_hecho', String),\n        ('fecha_hecho', String),\n        ('hora_hecho', String),\n        ('delito', String),\n        ('categoria_delito', String),\n        ('sexo', String),\n        ('edad', Float64),\n        ('tipo_persona', String),\n        ('calidad_juridica', String),\n        ('competencia', String),\n        ('colonia_hecho', String),\n        ('colonia_catalogo', String),\n        ('alcaldia_hecho', String),\n        ('alcaldia_catalogo', String),\n        ('municipio_hecho', String),\n        ('latitud', Float64),\n        ('longitud', Float64)])"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#data-cleansing",
    "href": "portfolio2/posts/python/crimes-web.html#data-cleansing",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Data cleansing",
    "text": "Data cleansing\n\n\nShow code\ndf = (\n    df.filter(\n    # drop null and 2018 years\n        (pl.col('anio_inicio')!=2018)\n    ).with_columns(\n    # create datetime field\n        (pl.col('fecha_inicio').cast(pl.String) + ' ' + pl.col('hora_inicio')\n             .cast(pl.String)).alias('fecha_inicio')\n    ).select(\n    # exclude columns\n        pl.exclude('anio_inicio','mes_inicio','hora_inicio',\n                   'anio_hecho','mes_hecho','fecha_hecho','hora_hecho')\n    ).with_columns(\n        fecha_inicio=pl.col('fecha_inicio').str.to_datetime()\n    ).drop_nulls(subset='fecha_inicio')\n)\n\n\n\n\nShow code\ndf.select(pl.all().is_null().sum()).to_dicts()\n\n\n[{'fecha_inicio': 0,\n  'delito': 0,\n  'categoria_delito': 0,\n  'sexo': 237070,\n  'edad': 449780,\n  'tipo_persona': 6858,\n  'calidad_juridica': 1,\n  'competencia': 0,\n  'colonia_hecho': 382,\n  'colonia_catalogo': 17677,\n  'alcaldia_hecho': 3,\n  'alcaldia_catalogo': 953,\n  'municipio_hecho': 3,\n  'latitud': 0,\n  'longitud': 0}]\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that even if we dropped around 75,000 rows, there continue to be many fields with empty rows, such as sex, age, neighborhood, mayorship and municipality."
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#data-cleaning-for-age",
    "href": "portfolio2/posts/python/crimes-web.html#data-cleaning-for-age",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Data cleaning for age",
    "text": "Data cleaning for age\n\n\nAge goes from 0.0 up to 369.0 years old!\n\n\n\n\nShow code\n# case for age\ndf = df.with_columns(\n    pl.when(pl.col('edad') &lt; 18).then(18)\n    .when(pl.col('edad')&gt;99).then(99)\n    .otherwise('edad')\n    .alias('edad')\n)\n\n\nWe have cleaned age values by setting age less than 18 to 18, and age values gretar than 99 to 99."
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#accrued-crimes-by-year",
    "href": "portfolio2/posts/python/crimes-web.html#accrued-crimes-by-year",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Accrued crimes by year",
    "text": "Accrued crimes by year\n\n\nShow code\nyears = (\n    df.sort('fecha_inicio')\n        .group_by_dynamic('fecha_inicio', every='1y')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n(\n    years\n        .to_pandas()\n        .style.format(\n            {\n            'fecha_inicio':'{:%Y}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nfecha_inicio\ncrimes\n\n\n\n\n2019\n256,827\n\n\n2020\n204,659\n\n\n2021\n228,627\n\n\n2022\n237,659\n\n\n2023\n239,402\n\n\n2024\n173,803\n\n\n\n\n\n\nTable¬†1: Accrued Crimes in Mexico City by Year\n\n\n\n\n\n\nShow code\nfig = px.bar(years, \n             x='fecha_inicio',\n             y='crimes',\n             orientation='v',\n             hover_data=['fecha_inicio','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Year',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Crimes')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†2: Crimes in Mexico City by Year"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#time-behavior-of-crimes",
    "href": "portfolio2/posts/python/crimes-web.html#time-behavior-of-crimes",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Time behavior of crimes",
    "text": "Time behavior of crimes\n\n\nShow code\nmonths = (\n    df\n        .sort('fecha_inicio', descending=False)\n        .group_by_dynamic('fecha_inicio', every='1mo')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n(\n    months\n        .to_pandas()\n        .tail(10)\n        .style.format(\n            {\n            'fecha_inicio':'{:%b, %Y}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nfecha_inicio\ncrimes\n\n\n\n\nDec, 2023\n17,358\n\n\nJan, 2024\n18,354\n\n\nFeb, 2024\n18,750\n\n\nMar, 2024\n19,797\n\n\nApr, 2024\n19,923\n\n\nMay, 2024\n20,879\n\n\nJun, 2024\n19,209\n\n\nJul, 2024\n19,429\n\n\nAug, 2024\n19,149\n\n\nSep, 2024\n18,313\n\n\n\n\n\n\nTable¬†2: Accrued Crimes in Mexico City by Month\n\n\n\n\n\n\nShow code\nfig = px.line(months, \n             x='fecha_inicio',\n             y='crimes',\n             hover_data=['fecha_inicio','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City 2019-2024',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Crimes')),\n    )\n\nfig.update_traces(line_color='#7f0000',\n                  line={'width':3},\n                  )\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†3: Time Behavior of Crimes in Mexico City"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#crimes-by-sex",
    "href": "portfolio2/posts/python/crimes-web.html#crimes-by-sex",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by sex",
    "text": "Crimes by sex\n\n\nShow code\ndf_sex = (\n    df.group_by('sexo')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n# convert to pandas\ndf_sex = df_sex.to_pandas()\n# rename row\ndf_sex['sexo'] = df_sex['sexo'].replace({None:'NA'})\n\n\n\n\nShow code\n(\n    df_sex\n        .sort_values('crimes')\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nsexo\ncrimes\n\n\n\n\nNA\n237,070\n\n\nFemenino\n535,960\n\n\nMasculino\n567,947\n\n\n\n\n\n\nTable¬†3: Accrued Crimes in Mexico City by Sex\n\n\n\n\n\n\nShow code\nfig = px.bar(df_sex.sort_values('crimes'),\n             y='sexo',\n             x='crimes',\n             orientation='h',\n             hover_data=['sexo','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Sex',\n             template='ggplot2',\n             text='crimes',\n             )\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Sex')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†4: Accrued Crimes in Mexico City by Sex"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#crimes-by-age",
    "href": "portfolio2/posts/python/crimes-web.html#crimes-by-age",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by age",
    "text": "Crimes by age\n\n\nShow code\ndf_edad = (\n    df.group_by('edad')\n        .agg(pl.len().alias('crimes'))\n        .sort('edad')\n        .drop_nulls()\n)\n\n\n\n\nShow code\n(\n    df_edad\n        .to_pandas()\n        .sample(10)\n        .style.format(\n            {\n            'edad':'{:,.0f}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nedad\ncrimes\n\n\n\n\n56\n10,789\n\n\n47\n16,414\n\n\n44\n16,818\n\n\n90\n313\n\n\n85\n789\n\n\n37\n21,022\n\n\n82\n1,181\n\n\n88\n493\n\n\n50\n16,098\n\n\n73\n3,296\n\n\n\n\n\n\nTable¬†4: Accrued Crimes in Mexico City by Age\n\n\n\n\n\n\nShow code\nfig = px.bar(df_edad,\n             x='edad',\n             y='crimes',\n             orientation='v',\n             hover_data=['edad','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Age',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Age')),\n    )\n\nfig.update_traces(marker_color='#7f0000',)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†5: Crimes in Mexico City by Age Distribution"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#crimes-by-neighborhood",
    "href": "portfolio2/posts/python/crimes-web.html#crimes-by-neighborhood",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by neighborhood",
    "text": "Crimes by neighborhood\n\n\nShow code\ndf_colonia = (\n    df.group_by('colonia_hecho')\n        .agg(pl.len().alias('crimes'))\n        .top_k(10, by='crimes')\n        .sort('crimes', descending=False)\n)\n\n\n\n\nShow code\n(\n    df_colonia\n        .to_pandas()\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\ncolonia_hecho\ncrimes\n\n\n\n\nPEDREGAL DE SANTO DOMINGO\n10,073\n\n\nJU√ÅREZ\n11,423\n\n\nBUENAVISTA\n11,749\n\n\nNARVARTE\n11,906\n\n\nAGR√çCOLA ORIENTAL\n12,024\n\n\nMORELOS\n12,518\n\n\nROMA NORTE\n14,878\n\n\nDEL VALLE CENTRO\n17,178\n\n\nDOCTORES\n24,711\n\n\nCENTRO\n40,066\n\n\n\n\n\n\nTable¬†5: Accrued Crimes in Mexico City by Neighborhood\n\n\n\n\n\n\nShow code\nfig = px.bar(df_colonia,\n             y='colonia_hecho',\n             x='crimes',\n             orientation='h',\n             hover_data=['colonia_hecho','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City - Top 10 Neighborhoods',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Alcaldia')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†6: Crimes in Mexico City by Neighborhood"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#crimes-by-mayorship",
    "href": "portfolio2/posts/python/crimes-web.html#crimes-by-mayorship",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by mayorship",
    "text": "Crimes by mayorship\n\n\nShow code\ndf_alcaldia = (\n    df.group_by('alcaldia_hecho')\n        .agg(pl.len().alias('crimes'))\n        .sort('crimes')\n        .filter(pl.col('alcaldia_hecho')!='FUERA DE CDMX')\n)\n\n\n\n\nShow code\n(\n    df_alcaldia\n        .to_pandas()\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nalcaldia_hecho\ncrimes\n\n\n\n\nMILPA ALTA\n13,184\n\n\nCUAJIMALPA DE MORELOS\n23,329\n\n\nLA MAGDALENA CONTRERAS\n27,285\n\n\nTLAHUAC\n41,547\n\n\nXOCHIMILCO\n45,924\n\n\nIZTACALCO\n60,427\n\n\nAZCAPOTZALCO\n64,561\n\n\nVENUSTIANO CARRANZA\n78,446\n\n\nMIGUEL HIDALGO\n84,242\n\n\nTLALPAN\n84,382\n\n\nCOYOACAN\n93,953\n\n\nALVARO OBREGON\n94,083\n\n\nBENITO JUAREZ\n101,759\n\n\nGUSTAVO A. MADERO\n139,294\n\n\nCUAUHTEMOC\n188,964\n\n\nIZTAPALAPA\n199,591\n\n\n\n\n\n\nTable¬†6: Accrued Crimes in Mexico City by Mayorship\n\n\n\n\n\n\nShow code\nfig = px.bar(df_alcaldia,\n             y='alcaldia_hecho',\n             x='crimes',\n             orientation='h',\n             hover_data=['alcaldia_hecho','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Mayorship',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Alcaldia')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†7: Crimes in Mexico City by Mayorship"
  },
  {
    "objectID": "portfolio2/posts/python/crimes-web.html#heat-map-of-crimes",
    "href": "portfolio2/posts/python/crimes-web.html#heat-map-of-crimes",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Heat map of crimes",
    "text": "Heat map of crimes\n\n\nShow code\ndf_map = (\n    df.with_columns(\n        (pl.col('colonia_hecho') + ', ' + pl.col('alcaldia_hecho')).alias('neighborhood')\n    )\n        .filter(pl.col('alcaldia_hecho')!='FUERA DE CDMX')\n        .group_by('neighborhood', maintain_order=True)\n        .agg(latitude=pl.col('latitud').mean(),\n             longitude=pl.col('longitud').mean(),\n             crimes=pl.col('delito').len()\n            )\n)\n\n\n\n\nShow code\nheat_map = df_map.to_pandas()\n\n\n\n\nShow code\n(\n    heat_map.sort_values('crimes', ascending=False)\n        .head(10)\n        .style.format(\n        {\n            'latitude':'{:,.4f}',\n            'longitude':'{:,.4f}',\n            'crimes':'{:,.0f}',\n        }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nneighborhood\nlatitude\nlongitude\ncrimes\n\n\n\n\nCENTRO, CUAUHTEMOC\n19.4327\n-99.1375\n40,042\n\n\nDOCTORES, CUAUHTEMOC\n19.4200\n-99.1486\n24,711\n\n\nDEL VALLE CENTRO, BENITO JUAREZ\n19.3831\n-99.1682\n17,178\n\n\nROMA NORTE, CUAUHTEMOC\n19.4184\n-99.1627\n14,878\n\n\nAGR√çCOLA ORIENTAL, IZTACALCO\n19.3947\n-99.0708\n12,008\n\n\nNARVARTE, BENITO JUAREZ\n19.3930\n-99.1542\n11,906\n\n\nJU√ÅREZ, CUAUHTEMOC\n19.4268\n-99.1628\n11,408\n\n\nPEDREGAL DE SANTO DOMINGO, COYOACAN\n19.3275\n-99.1677\n10,073\n\n\nPOLANCO, MIGUEL HIDALGO\n19.4335\n-99.1956\n10,019\n\n\nAGR√çCOLA PANTITLAN, IZTACALCO\n19.4104\n-99.0649\n9,662\n\n\n\n\n\n\nTable¬†7: Crimes in Mexico City Heatmap\n\n\n\n\n\n\nShow code\nfrom folium.plugins import HeatMap\n\nbasemap = folium.Map(location=[19.35, -99.12], zoom_start=10.4)\n\n\n\n\nShow code\nHeatMap(heat_map[['latitude', 'longitude', 'crimes']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x310ecc530&gt;\n\n\n\n\nShow code\nfolium.plugins.Fullscreen().add_to(basemap)\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†8: Mexico City Crime Heatmap by Mayorship\n\n\n\n\n\n\nShow code\n# mexico city crime map\nm = folium.Map(\n    location=[19.35, -99.12],\n    zoom_start=10,\n    control_scale=False,\n)\n# Layers\nCrime = folium.FeatureGroup(name='&lt;u&gt;&lt;b&gt;Place&lt;/b&gt;&lt;/u&gt;', show=True)\nm.add_child(Crime)\n#draw marker with symbol you want at base\nmy_symbol_css_class= \"\"\" &lt;style&gt;\n.fa-mysymbol3:before {\n    font-family: Gill Sans; \n    font-weight: bold;\n    font-size: 11px;\n    color: white;\n    background-color:'';\n    border-radius: 10px; \n    white-space: pre;\n    content: 'P';\n    }\n&lt;/style&gt;    \n        \"\"\"\n# the below is just add above  CSS class to folium root map      \nm.get_root().html.add_child(folium.Element(my_symbol_css_class))\n# then we just create marker and specific your css class in icon like below\nfor i in heat_map.index:\n   html=f\"\"\"\n        &lt;p style=\"font-size: 14px;\"&gt;{heat_map.iloc[i]['neighborhood']}&lt;/font&gt;&lt;/p&gt;\n        &lt;p style=\"font-size: 14px;\"&gt;Total crimes: {heat_map.iloc[i]['crimes']}&lt;/font&gt;&lt;/p&gt;\n        \"\"\"\n   iframe = folium.IFrame(html=html, width=220, height=90)\n   popup = folium.Popup(iframe, max_width=250)\n   folium.Marker(\n        location = [heat_map.iloc[i]['latitude'], heat_map.iloc[i]['longitude']],\n        icon = folium.Icon(color='darkred', prefix='fa', icon='fa-mysymbol3'),\n        popup = popup,\n        tooltip = heat_map.iloc[i]['neighborhood']\n    ).add_to(Crime)\nfolium.plugins.Fullscreen().add_to(m)\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†9: Mexico City Crime Hubs by Neighborhood"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pop1950-2023.html",
    "href": "portfolio2/posts/python/mexico_pop1950-2023.html",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "",
    "text": "In this article we shall see Mexico‚Äôs Population evolution over years.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n\n\nCode\n# Read tsv file\nmex = pd.read_csv('datasets/mexico_pop.tsv', sep='\\t')\n# Drop spaces\nmex = mex.rename(columns={'Year ':'Year','Population ':'Population','Growth Rate':'Growth rate'})\n\n\n\n\nCode\nmex.sample(5)\n\n\n\n\n\n\n\n\n\nYear\nPopulation\nGrowth rate\n\n\n\n\n41\n1982\n70,656,783\n2.06%\n\n\n54\n1969\n48,714,394\n3.27%\n\n\n69\n1954\n30,616,896\n2.67%\n\n\n42\n1981\n69,233,769\n2.26%\n\n\n47\n1976\n60,452,543\n3.00%\n\n\n\n\n\n\n\n\n\nCode\nmex['Population'] = pd.to_numeric(mex['Population'].str.replace(',',''))\n\n\n\n\nCode\nmex['Growth rate'] = pd.to_numeric(mex['Growth rate'].str.replace('%','', regex=True))\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Population'))\n    + geom_line(color='#b30000', size=1.5, linetype='solid')\n    + labs(x='Year', y='Population', \n           title='Total Population in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 150_000_000))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw() # Removes \"&lt;Figure Size: (640 x 480)&gt;\" below chart\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Growth rate'))\n    + geom_line(color='#767600', size=1.5, linetype='solid')\n    + labs(x='Year', y='Grwoth rate %', \n           title='Population Growth in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 5))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw()"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pop1950-2023.html#introduction",
    "href": "portfolio2/posts/python/mexico_pop1950-2023.html#introduction",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "",
    "text": "In this article we shall see Mexico‚Äôs Population evolution over years.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n\n\nCode\n# Read tsv file\nmex = pd.read_csv('datasets/mexico_pop.tsv', sep='\\t')\n# Drop spaces\nmex = mex.rename(columns={'Year ':'Year','Population ':'Population','Growth Rate':'Growth rate'})\n\n\n\n\nCode\nmex.sample(5)\n\n\n\n\n\n\n\n\n\nYear\nPopulation\nGrowth rate\n\n\n\n\n41\n1982\n70,656,783\n2.06%\n\n\n54\n1969\n48,714,394\n3.27%\n\n\n69\n1954\n30,616,896\n2.67%\n\n\n42\n1981\n69,233,769\n2.26%\n\n\n47\n1976\n60,452,543\n3.00%\n\n\n\n\n\n\n\n\n\nCode\nmex['Population'] = pd.to_numeric(mex['Population'].str.replace(',',''))\n\n\n\n\nCode\nmex['Growth rate'] = pd.to_numeric(mex['Growth rate'].str.replace('%','', regex=True))\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Population'))\n    + geom_line(color='#b30000', size=1.5, linetype='solid')\n    + labs(x='Year', y='Population', \n           title='Total Population in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 150_000_000))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw() # Removes \"&lt;Figure Size: (640 x 480)&gt;\" below chart\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Growth rate'))\n    + geom_line(color='#767600', size=1.5, linetype='solid')\n    + labs(x='Year', y='Grwoth rate %', \n           title='Population Growth in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 5))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw()"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pop1950-2023.html#conclusions",
    "href": "portfolio2/posts/python/mexico_pop1950-2023.html#conclusions",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "Conclusions",
    "text": "Conclusions\nMexico boasts a population of nearly 130 million, ranking it as the 10th most populous country globally.\nThe population is steadily growing, with estimates suggesting an annual increase of around 1.2%.\nMexico is a highly urbanized nation, with 88% of the population residing in cities. This trend is expected to continue, driven by economic opportunities and infrastructure development.\nThe overall population density is moderate, at around 66 individuals per square kilometer, but varies significantly across regions.\nThe population is relatively young, with a median age of 29.8 years. This indicates a large segment in their working years, potentially driving economic growth.\nHowever, an aging population is expected in the coming decades, requiring adjustments to social security and healthcare systems.\nMexico is a multicultural nation with a rich indigenous heritage. While mestizos (mixed Indigenous and European ancestry) form the majority, Indigenous groups still comprise a significant portion of the population, contributing to cultural diversity.\nRapid urbanization brings challenges like managing infrastructure, providing essential services, and tackling inequality.\nInvesting in education, healthcare, and job creation can harness the demographic dividend presented by the young population.\nAddressing the potential impact of an aging population and promoting sustainable development are crucial for long-term prosperity."
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pop1950-2023.html#references",
    "href": "portfolio2/posts/python/mexico_pop1950-2023.html#references",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "References",
    "text": "References\n\nWorldometer\nINEGI - National Institute of Statistics and Geography\nCIA World Factbook"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pop1950-2023.html#contact",
    "href": "portfolio2/posts/python/mexico_pop1950-2023.html#contact",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/spotipy.html#what-is-spotipy",
    "href": "portfolio2/posts/python/spotipy.html#what-is-spotipy",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "What is Spotipy?",
    "text": "What is Spotipy?\nSpotipy is a lightweight, easy-to-use Python library that simplifies interacting with the Spotify API. It handles the nitty-gritty details of authentication, making requests, and parsing responses, letting you focus on the fun part: exploring and manipulating music data."
  },
  {
    "objectID": "portfolio2/posts/python/spotipy.html#why-use-spotipy",
    "href": "portfolio2/posts/python/spotipy.html#why-use-spotipy",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Why Use Spotipy?",
    "text": "Why Use Spotipy?\n\nAccess a Universe of Music Data\nRetrieve information about artists, albums, tracks, playlists, audio features (like danceability and energy), and much more.\nAutomate Tasks\nCreate scripts to manage your playlists, discover new music based on your taste, analyze your listening habits, or even build your own music recommendation system.\nIntegrate with Other Tools\nCombine Spotipy with other Python libraries like Pandas for data analysis, Matplotlib for visualization, or Flask for building web applications.\nBuild Music-Focused Apps\nDevelop custom applications that leverage Spotify‚Äôs vast music catalog and user data. Think of creating personalized radio stations, music visualizations, or tools for music discovery.\nEasy to Learn and Use\nSpotipy‚Äôs well-documented API and intuitive design make it accessible to both beginners and experienced Python developers."
  },
  {
    "objectID": "portfolio2/posts/python/spotipy.html#key-features-and-capabilities",
    "href": "portfolio2/posts/python/spotipy.html#key-features-and-capabilities",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Key Features and Capabilities",
    "text": "Key Features and Capabilities\n\nAuthentication\nHandles the OAuth 2.0 flow for authenticating users and obtaining access tokens, allowing you to access both public and private data.\nSearching\nSearch for artists, tracks, albums, and playlists using keywords.\nRetrieving Information\nFetch detailed information about specific artists, tracks, albums, and playlists, including metadata, audio features, and related artists.\nPlaylist Management\nCreate, modify, and manage playlists, including adding and removing tracks.\nUser Profile Access\nAccess user profile information, including listening history, followed artists, and saved tracks.\nAudio Features Analysis\nRetrieve audio features for tracks, such as danceability, energy, tempo, valence, and more. This data can be used to analyze music and build interesting applications.\n\n\nEnvironment settings\n\n\nShow code\nimport json\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\n\n\n\n\nShow code\n# Token gotten from spotify api\nfilename = '../APIs/credentials.json'\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n# read credentials\nclientID = keys['spotipy_client_id']\nclientSecret = keys['spotipy_client_secret']\n\n\n\n\nShow code\nclient_credential_manager = SpotifyClientCredentials(client_id=clientID, client_secret=clientSecret)\nsp = spotipy.Spotify(client_credentials_manager=client_credential_manager)\n\n\n\n\nArtist\n\n\nShow code\nresults = sp.artist_albums('spotify:artist:06HL4z0CvFAxyc27GXpf02', album_type='album')\nalbums = results['items']\nwhile results['next']:\n    results = sp.next(results)\n    albums.extend(results['items'])\n\nfor album in albums:\n    print(album['name'])\n\n\n1989 (Taylor's Version) [Deluxe]\n1989 (Taylor's Version)\nSpeak Now (Taylor's Version)\nMidnights (The Til Dawn Edition)\nMidnights (3am Edition)\nMidnights\nRed (Taylor's Version)\nFearless (Taylor's Version)\nevermore (deluxe version)\nevermore\nfolklore: the long pond studio sessions (from the Disney+ special) [deluxe edition]\nfolklore (deluxe version)\nfolklore\nLover\nreputation\nreputation Stadium Tour Surprise Song Playlist\n1989 (Deluxe)\n1989\nRed (Deluxe Edition)\nSpeak Now World Tour Live\nSpeak Now\nSpeak Now (Deluxe Package)\nFearless (Platinum Edition)\nFearless (International Version)\nLive From Clear Channel Stripped 2008\nTaylor Swift\n\n\n\n\nAlbum\n\n\nShow code\nalbum = sp.album_tracks('spotify:album:6MeLjaERUK6fJ58YZpPlyC')\nlista_canciones = album['items']\n\nfor cancion in lista_canciones:\n    print(cancion['name'])\n\n\nY Nos Dieron las Diez\nConductores Suicidas\nYo Quiero Ser una Chica Almodovar\nA la Orilla de la Chimenea\nTodos Menos T√∫\nLa del Pirata Cojo\nLa Canci√≥n de las Noches Perdidas\nLos Cuentos Que Yo Cuento\nPeor para el Sol\nAmor Se Llama el Juego\nPastillas para No So√±ar"
  },
  {
    "objectID": "portfolio2/posts/python/spotipy.html#search",
    "href": "portfolio2/posts/python/spotipy.html#search",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Search",
    "text": "Search\n\n\nShow code\nartist_name = []\ntrack_name = []\npopularity = []\ntrack_id = []\n\nfor i in range(0,1_000,50):\n    track_results = sp.search(q='year:2020', type='track', limit=50, offset=i)\n    for i, t in enumerate(track_results['tracks']['items']):\n        artist_name.append(t['artists'][0]['name'])\n        track_name.append(t['name'])\n        track_id.append(t['id'])\n        popularity.append(t['popularity'])\n\n\n\n\nShow code\ntrack_dataframe = pd.DataFrame(\n        {'artist_name':artist_name,\n         'track_name':track_name,\n         'track_id':track_id,\n         'popularity':popularity}\n)\n\n\n\n\nShow code\ntrack_dataframe\n\n\n\n\n\n\n\n\n\nartist_name\ntrack_name\ntrack_id\npopularity\n\n\n\n\n0\nDream Supplier\nClean Baby Sleep White Noise (Loopable)\n0zirWZTcXBBwGsevrsIpvT\n94\n\n\n1\nHotel Ugly\nShut up My Moms Calling\n3hxIUxnT27p5WcmjGUXNwx\n90\n\n\n2\nBrent Faiyaz\nClouded\n2J6OF7CkpdQGSfm1wdclqn\n86\n\n\n3\n21 Savage\nGlock In My Lap\n6pcywuOeGGWeOQzdUyti6k\n87\n\n\n4\nSteve Lacy\nInfrunami\n0f8eRy9A0n6zXpKSHSCAEp\n86\n\n\n...\n...\n...\n...\n...\n\n\n995\nEdith Whiskers\nHome\n18V1UiYRvWYwn01CRDbbuR\n73\n\n\n996\nDuke Dumont\nOcean Drive\n4b93D55xv3YCH5mT4p6HPn\n74\n\n\n997\nBad Bunny\nTE DESEO LO MEJOR\n23XjN1s3DZC8Q9ZwuorYY4\n73\n\n\n998\nJunior H\nNo Me Pesa\n4YU704KDCv4tyE6qQxliY3\n69\n\n\n999\nDestroy Lonely\nIn The Air\n2eJBpNlTTPatjec4SDQvuo\n64\n\n\n\n\n1000 rows √ó 4 columns"
  },
  {
    "objectID": "portfolio2/posts/python/spotipy.html#download-music-songs-from-spotify",
    "href": "portfolio2/posts/python/spotipy.html#download-music-songs-from-spotify",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Download music songs from Spotify",
    "text": "Download music songs from Spotify\n\n\nShow code\nimport spotdl\nimport spotify_dl\n\n\n\n\nShow code\n%%bash\ncd ~/Downloads;\nspotify_dl -l 'https://open.spotify.com/track/3D0bXrSv7O73vOaGOG8J9c?si=2eb4c15e2d7c4adf' \\\n&gt; /dev/null;\n\n\n\n\nShow code\n%%bash\ncd ~/Downloads;\nspotdl 'https://open.spotify.com/track/3D0bXrSv7O73vOaGOG8J9c?si=2eb4c15e2d7c4adf' \\\n&gt; /dev/null;"
  },
  {
    "objectID": "portfolio2/posts/python/spotipy.html#conclusions",
    "href": "portfolio2/posts/python/spotipy.html#conclusions",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Conclusions",
    "text": "Conclusions\nSpotipy is a valuable tool for anyone interested in working with Spotify‚Äôs music data. It bridges the gap between the Spotify API and the Python programming language, enabling developers to create innovative and data-driven music experiences. While there are considerations related to API limitations and authentication, the benefits of using Spotipy generally outweigh the challenges.\nConsiderations and Limitations\n\nAPI Rate Limits: Spotify‚Äôs API has rate limits, which can restrict the number of requests you can make within a given time period. This necessitates careful planning and optimization of API calls, especially for large-scale data retrieval.\nAuthentication Complexity: While Spotipy simplifies authentication, understanding OAuth 2.0 and managing access tokens can still be a hurdle for some users.\nData Structure Awareness: Effective use of Spotipy requires a good understanding of the structure of Spotify‚Äôs data, including the various object types (artists, tracks, playlists) and their attributes.\nDependence on Spotify API: Spotipy‚Äôs functionality is entirely dependent on the Spotify Web API. Any changes or limitations to the API will directly affect the library‚Äôs capabilities.\nMaintaining Token Refreshing: Applications that use spotipy and run for long periods of time, need to implement robust token refreshing, or the application will cease to function."
  },
  {
    "objectID": "portfolio2/posts/python/spotipy.html#contact",
    "href": "portfolio2/posts/python/spotipy.html#contact",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_gn.html",
    "href": "portfolio2/posts/python/mexico_gn.html",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "",
    "text": "GN personnel"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_gn.html#national-guard",
    "href": "portfolio2/posts/python/mexico_gn.html#national-guard",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "National Guard",
    "text": "National Guard\nMexico‚Äôs National Guard is a relatively new security force established in 2019. It was created to address the country‚Äôs high crime rates and complement traditional law enforcement.\n\nFormation\n\nIn 2019, it emerged by merging elements of the Federal Police, Military Police, and Naval Police.\n\nMission\n\nThe National Guard was intended to be a gendarmerie, focusing on territorial defense and public security tasks like crime prevention and patrolling.\n\nStructure\n\nInitially envisioned under civilian control, a 2022 reform transferred command to the Ministry of National Defense, sparking controversy.\n\nCurrent Status\n\nThe National Guard‚Äôs role is evolving. It still tackles crime, but also handles tasks like border control and disaster relief."
  },
  {
    "objectID": "portfolio2/posts/python/mexico_gn.html#display-interactive-chart",
    "href": "portfolio2/posts/python/mexico_gn.html#display-interactive-chart",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure¬†1: Datawrapper Interactive Chart\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "portfolio2/posts/python/mexico_gn.html#references",
    "href": "portfolio2/posts/python/mexico_gn.html#references",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "References",
    "text": "References\n\nWhat is Guardia Nacional?\nGobierno de Mexico"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_gn.html#contact",
    "href": "portfolio2/posts/python/mexico_gn.html#contact",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pol_scenario.html#mexicos-political-scenario",
    "href": "portfolio2/posts/python/mexico_pol_scenario.html#mexicos-political-scenario",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Mexico‚Äôs Political Scenario",
    "text": "Mexico‚Äôs Political Scenario\nA complex mix of democratic strides and ongoing hurdles\n\nElectoral Democracy\n\nSince 2000, Mexico has functioned as an electoral democracy with regular power transitions between parties, both nationally and regionally.\n\nMulti-Party System\n\nMexico boasts a vibrant multi-party system, with several parties competing for power, but the current dominant force is the National Regeneration Movement (MORENA).\n\nChallenges\n\nRule of Law\n\nDeficiencies in the rule of law weaken citizen confidence in political institutions. Issues like corruption, violence by criminal groups, and limited accountability create a complex environment.\n\nPublic Security\n\nDrug trafficking and organized crime pose a major threat to public safety and stability. The government‚Äôs strategies to combat these issues are a source of ongoing debate.\n\nMilitary‚Äôs Role\n\nThe expanding role of the military in public security raises concerns about potential human rights abuses and a shift away from civilian control.\n\n\n\n\n\n\nFigure¬†1: Governors of States in Mexico\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pol_scenario.html#mexico-citys-political-landscape",
    "href": "portfolio2/posts/python/mexico_pol_scenario.html#mexico-citys-political-landscape",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Mexico City‚Äôs Political Landscape",
    "text": "Mexico City‚Äôs Political Landscape\nMexico City‚Äôs political scene is fascinating due to its unique position.\n\nNational Influence\n\nAs the capital, Mexico City is the hub of national politics. Federal government buildings are located here, and residents elect representatives to the national Congress. This creates a tight link between the local and the national.\n\nShifting Power\n\nHistorically, the mayor was appointed by the president. Since 1997, residents directly elect their leader for a six-year term.\n\nRecent Elections (2021)\n\nThe MORENA party solidified its power in many states, including Mexico City. However, opposition parties gained control of most city boroughs, chipping away at MORENA‚Äôs local dominance.\n\nActive Citizenry\n\nMexico City residents have a strong voice due to their large numbers and ability to protest. This makes them a powerful force that politicians need to consider.\n\nChallenges\n\nLike other parts of Mexico, concerns exist about democratic processes and security during protests. Violence against women‚Äôs rights demonstrations is a recent point of tension.\nMexico City‚Äôs political situation is dynamic and reflects the national landscape. The interplay between federal and local power, recent election results, and an engaged citizenry all contribute to a complex and interesting political environment.\n\n\n\n\n\n\nFigure¬†2: Mayors of Mexico City\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pol_scenario.html#conclusions",
    "href": "portfolio2/posts/python/mexico_pol_scenario.html#conclusions",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Conclusions",
    "text": "Conclusions\nMexico‚Äôs political future hinges on its ability to address these challenges.\nStrengthening the rule of law, tackling corruption, and finding effective solutions for security issues will be crucial for a more stable and prosperous democracy."
  },
  {
    "objectID": "portfolio2/posts/python/mexico_pol_scenario.html#contact",
    "href": "portfolio2/posts/python/mexico_pol_scenario.html#contact",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/working_with_json_files.html#overview",
    "href": "portfolio2/posts/python/working_with_json_files.html#overview",
    "title": "Manipulating JSON Files with Python",
    "section": "Overview",
    "text": "Overview\nDiving into the world of data science and machine learning, one of the fundamental skills you‚Äôll encounter is the art of reading data.\nIf you have already some experience with it, you‚Äôre probably familiar with JSON (JavaScript Object Notation) files.\nThink of how NoSQL databases like MongoDB love to store data in JSON, or how REST APIs often respond in the same format.\nHowever, JSON, while perfect for storage and exchange, isn‚Äôt quite ready for in-depth analysis in its raw form.\nThis is where we transform it into something more analytically friendly ‚Äì a tabular format.\n\nEnvironment settings\n\n\nShow code\n# Create a json file\n\nsimple_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n}\n\nsimple_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n}\n\nsimple_json_list = [\n    simple_json, \n    simple_json_2\n\n]\n\n\n\n\nShow code\nimport pandas as pd\n\n\n\n\nManipulating json files\n\n\nShow code\npd.json_normalize(simple_json)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nDavid\nLondon\n80000\n\n\n\n\n\n\n\n\n\nShow code\npd.json_normalize(simple_json_2)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nTaylor\nChicago\n120000\n\n\n\n\n\n\n\n\n\nShow code\npd.json_normalize(simple_json_list)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nDavid\nLondon\n80000\n\n\n1\nTaylor\nChicago\n120000\n\n\n\n\n\n\n\nIn case we just want to transform some specific fields into a tabular pandas DataFrame, the json_normalize() command does not allow us to choose what fields to transform.\nTherefore, a small preprocessing of the JSON should be performed where we filter just those columns of interest.\n\n\nShow code\n# Fields to include\nfields = ['name', 'city']\n\n# Filter the JSON data\nfiltered_json_list = [{key: value for key, value in item.items() if key in fields}\n                      for item in simple_json_list]\n\npd.json_normalize(filtered_json_list)\n\n\n\n\n\n\n\n\n\nname\ncity\n\n\n\n\n0\nDavid\nLondon\n\n\n1\nTaylor\nChicago\n\n\n\n\n\n\n\nWhen dealing with multiple-leveled JSONs we find ourselves with nested JSONs within different levels.\nThe procedure is the same as before, but in this case, we can choose how many levels we want to transform.\nBy default, the command will always expand all levels and generate new columns containing the concatenated name of all the nested levels.\n\n\nShow code\n# Create a nested json file\n\nmultiple_levels_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n    'skills': {\n        'python': 'advanced',\n        'SQL': 'advanced',\n        'GCP': 'mid'\n    },\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":True,\n        \"data analyst\":False,\n        }\n    }\n\nmultiple_levels_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n    'skills': {\n        'python': 'mid',\n        'SQL': 'advanced',\n        'GCP': 'beginner'\n    },\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":False,\n        \"data analyst\":True\n        }\n    }\n\nmultiple_level_json_list = [\n    multiple_levels_json, \n    multiple_levels_json_2\n\n]\n\n\nWe would get the following table with 3 columns under the field skills:\n\nskills.python\nskills.SQL\nskills.GCP\n\nand 4 columns under the field roles\n\nroles.project manager\nroles.data engineer\nroles.data scientist\nroles.data analyst\n\nHowever, imagine we just want to transform our top level.\nWe can do so by specifically defining the parameter max_level to 0\n\n\nShow code\npd.json_normalize(multiple_level_json_list, max_level = 0)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\nskills\nroles\n\n\n\n\n0\nDavid\nLondon\n80000\n{'python': 'advanced', 'SQL': 'advanced', 'GCP...\n{'project manager': False, 'data engineer': Fa...\n\n\n1\nTaylor\nChicago\n120000\n{'python': 'mid', 'SQL': 'advanced', 'GCP': 'b...\n{'project manager': False, 'data engineer': Fa...\n\n\n\n\n\n\n\n\n\nShow code\nnested_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n    'skills': [\"python\", \"SQL\",\"GCP\"],\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":True,\n        \"data analyst\":False,\n        }\n    }\n\nnested_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n    'skills': [\"python\", \"SQL\",\"PowerBI\",\"Looker\"],\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":False,\n        \"data analyst\":True\n        }\n    }\n\nnested_json_list = [\n    nested_json, \n    nested_json_2\n\n]\n\n\n\n\nShow code\npd.json_normalize(nested_json_list, record_path=['skills'], meta=['name','city'])\n\n\n\n\n\n\n\n\n\n0\nname\ncity\n\n\n\n\n0\npython\nDavid\nLondon\n\n\n1\nSQL\nDavid\nLondon\n\n\n2\nGCP\nDavid\nLondon\n\n\n3\npython\nTaylor\nChicago\n\n\n4\nSQL\nTaylor\nChicago\n\n\n5\nPowerBI\nTaylor\nChicago\n\n\n6\nLooker\nTaylor\nChicago"
  },
  {
    "objectID": "portfolio2/posts/python/working_with_json_files.html#conclusion",
    "href": "portfolio2/posts/python/working_with_json_files.html#conclusion",
    "title": "Manipulating JSON Files with Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, the transformation of JSON data into CSV files using Python‚Äôs Pandas library is easy and effective.\nJSON is still the most common format in modern data storage and exchange, notably in NoSQL databases and REST APIs. However, it presents some important analytic challenges when dealing with data in its raw format."
  },
  {
    "objectID": "portfolio2/posts/python/working_with_json_files.html#references",
    "href": "portfolio2/posts/python/working_with_json_files.html#references",
    "title": "Manipulating JSON Files with Python",
    "section": "References",
    "text": "References\n\nFerrer, J. (2024) Converting JSONs to Pandas DataFrames: Parsing Them the Right Way in Data Science"
  },
  {
    "objectID": "portfolio2/posts/python/working_with_json_files.html#contact",
    "href": "portfolio2/posts/python/working_with_json_files.html#contact",
    "title": "Manipulating JSON Files with Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/cdmx-subway.html",
    "href": "portfolio2/posts/python/cdmx-subway.html",
    "title": "Mexico City‚Äôs Underground",
    "section": "",
    "text": "Figure¬†1: Mexico City‚Äôs Subway Network"
  },
  {
    "objectID": "portfolio2/posts/python/cdmx-subway.html#early-beginnings-and-expansion",
    "href": "portfolio2/posts/python/cdmx-subway.html#early-beginnings-and-expansion",
    "title": "Mexico City‚Äôs Underground",
    "section": "Early Beginnings and Expansion",
    "text": "Early Beginnings and Expansion\nThe STC‚Äôs inception dates back to the late 1960s, a time when Mexico City was experiencing unprecedented urban growth. Recognizing the need for a modern transportation system, the government embarked on an ambitious project to construct an underground railway. The first line, Line 1, opened its doors in 1969, connecting the historic center of the city with the northern suburbs.\nOver the following decades, the STC expanded rapidly, adding new lines and stations to accommodate the increasing population. Today, the system comprises 12 lines and serves over 5 million passengers daily. It has become an integral part of the city‚Äôs infrastructure, connecting diverse neighborhoods and facilitating economic activity."
  },
  {
    "objectID": "portfolio2/posts/python/cdmx-subway.html#challenges-and-overcoming-adversity",
    "href": "portfolio2/posts/python/cdmx-subway.html#challenges-and-overcoming-adversity",
    "title": "Mexico City‚Äôs Underground",
    "section": "Challenges and Overcoming Adversity",
    "text": "Challenges and Overcoming Adversity\nDespite its success, the STC has faced numerous challenges throughout its history. One of the most significant issues has been overcrowding, particularly during peak hours. The system‚Äôs capacity has often been strained, leading to long wait times and uncomfortable conditions for commuters. To address this problem, the authorities have implemented various measures, including expanding the network and improving train frequency.\nAnother challenge has been the maintenance and upkeep of the system. The STC‚Äôs infrastructure is aging, and some sections require significant repairs and upgrades. Ensuring the safety and reliability of the system has been a constant priority for the authorities. In recent years, there have been efforts to modernize the infrastructure and improve maintenance practices.\n\n\n\n\n\n\nFigure¬†2: Inside the Subway"
  },
  {
    "objectID": "portfolio2/posts/python/cdmx-subway.html#the-future-of-the-stc",
    "href": "portfolio2/posts/python/cdmx-subway.html#the-future-of-the-stc",
    "title": "Mexico City‚Äôs Underground",
    "section": "The Future of the STC",
    "text": "The Future of the STC\nLooking ahead, the STC faces both opportunities and challenges. The government has ambitious plans to further expand the system, connecting new areas of the city and improving accessibility. However, these projects require substantial investment and careful planning. Additionally, the STC will need to adapt to changing demographics and transportation trends, such as the rise of electric vehicles and ride-sharing services.\nDespite the challenges, the STC remains a vital component of Mexico City‚Äôs urban landscape. Its history is a testament to the city‚Äôs resilience and its ability to adapt to changing circumstances. As Mexico City continues to grow and evolve, the STC will play a crucial role in shaping its future.\n\nCreating Beautiful Tables with Python and Great-Tables\n\n\nCode\n# import libraries\nimport polars as pl\nimport duckdb as db\nfrom great_tables import GT, md, html, loc, style\n\n\n\n\nCode\n# connect to dabase\nconn = db.connect('my_database.db')\n\n\n\n\nCode\n# retrieve data from table to dataframe\ndf = conn.sql('select * from cdmx_subway').pl()\n\n\n\n\nCode\n# close connection\nconn.close()\n\n\n\n\nCode\n# Change datatypes\ndf = df.select(pl.exclude('Line'))\n\n\n\n\nCode\n# Create table with great-tables\ncdmx = (\n    GT(df)\n    .tab_header(\n        title=md(\"### Mexico City's Subway Lines and Stations\"),\n        subtitle=html('''&lt;h4 align=\"left\"&gt;\n        Mexico City's metro system is a vital artery of the bustling metropolis.\n        It is the largest and busiest in Latin America, serving more than 5.5 million\n        passengers daily in its 195 stations and 12 lines.&lt;/h4&gt;''')\n    )\n    #.tab_options(table_width=\"100%\")\n    .cols_align(align='center', columns=['icon','Opening date','Stations'])\n    .cols_label(\n        icon=\"Line Sation\"\n    )\n    .fmt_date(columns='Opening date', date_style=\"m_day_year\")\n    .fmt_image(\"icon\", path=\"cdmx_metro_lines\")\n    .sub_missing(missing_text=\"\")\n    .tab_source_note(source_note=md('''**Jesus L. Monroy**&lt;br&gt;*Economist & Data Scientist*&lt;br&gt;&lt;br&gt;'''))\n    .tab_source_note(\n        source_note=md('''Source: [Wikipedia](https://en.wikipedia.org/wiki/Mexico_City_Metro&gt;Wikipedia)'''))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City's Subway Lines and Stations\n\n\nMexico City's metro system is a vital artery of the bustling metropolis. It is the largest and busiest in Latin America, serving more than 5.5 million passengers daily in its 195 stations and 12 lines.\n\n\nLine Sation\nOpening date\nStations\n\n\n\n\n\nAug 22, 1984\n20\n\n\n\nAug 22, 1984\n24\n\n\n\nDec 1, 1979\n21\n\n\n\nAug 29, 1981\n10\n\n\n\nDec 19, 1981\n13\n\n\n\nDec 21, 1983\n11\n\n\n\nNov 29, 1988\n14\n\n\n\nJul 20, 1994\n19\n\n\n\nAug 26, 1987\n12\n\n\n\nOct 30, 2012\n20\n\n\n\nAug 12, 1991\n10\n\n\n\nDec 15, 1999\n21\n\n\n\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "portfolio2/posts/python/cdmx-subway.html#contact",
    "href": "portfolio2/posts/python/cdmx-subway.html#contact",
    "title": "Mexico City‚Äôs Underground",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/sql_duckdb.html",
    "href": "portfolio2/posts/python/sql_duckdb.html",
    "title": "Supercharge Your SQL Analysis with Python and DuckDB",
    "section": "",
    "text": "Overview\nIn this post, we‚Äôll delve into the seamless integration of DuckDB with popular Python libraries, enabling efficient data ingestion, transformation, and analysis. Through practical examples, we demonstrate how to harness the full potential of DuckDB for complex SQL queries, real-time data processing, and exploratory data analysis. By the end of this post, readers will gain the knowledge and skills to supercharge their SQL analysis projects with Python and DuckDB.\n\n\nDatabase Creation\nDatabase: retail_db\nTable: retail_sales\n\n\nShow code\n# Import libraries\nimport polars as pl\nimport duckdb as db\nimport plotly.express as px\n\n\n\n\nShow code\n# Create database\nconn = db.connect('datasets/retail_db.db')\n\n\n\n\nShow code\n# Create table\nconn.sql('''\n    create table if not exists retail_sales (\n        id INT,\n        sale_date DATE,\n        sale_time TIME,\n        customer_id INT,\n        gender VARCHAR(10),\n        age INT,\n        category VARCHAR(35),\n        quantity INT,\n        price_per_unit FLOAT,\n        cogs FLOAT,\n        total_sale FLOAT\n        )\n''')\n\n\n\n\nData Ingestion\n\n\nShow code\n# Insert data into table from csv file\nconn.sql('''\n    INSERT INTO retail_sales\n    SELECT * FROM read_csv('datasets/sales.csv')\n''')\n\n\n\n\nData Exploration and Cleaning\n\nRecord Count: Determine the total number of records in the dataset.\nCustomer Count: Find out how many unique customers are in the dataset.\nCategory Count: Identify all unique product categories in the dataset.\nNull Value Check: Check for any null values in the dataset and delete records with missing data\n\n\n\nShow code\n# Show first 10 records \nconn.sql('select * exclude(cogs) from retail_sales limit 10').pl()\n\n\n\nshape: (10, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n10\n2023-11-22\n12:30:00\n110\n\"Female\"\n37\n\"Clothing\"\n1\n99.989998\n99.989998\n\n\n\n\n\n\nRecord count\n\n\nShow code\nconn.sql('select count(*) as records from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\nrecords\n\n\ni64\n\n\n\n\n448\n\n\n\n\n\n\nCustomer count\n\n\nShow code\nconn.sql('select count(distinct customer_id) customers from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\ncustomers\n\n\ni64\n\n\n\n\n55\n\n\n\n\n\n\nCategory count\n\n\nShow code\nconn.sql('select distinct category from retail_sales').pl()\n\n\n\nshape: (9, 1)\n\n\n\ncategory\n\n\nstr\n\n\n\n\n\"Toys\"\n\n\n\"Electronics\"\n\n\n\"Books\"\n\n\n\"Home Appliances\"\n\n\n\"Groceries\"\n\n\n\"Sports Equipment\"\n\n\n\"Clothing\"\n\n\n\"Home Goods\"\n\n\n\"Beauty Products\"\n\n\n\n\n\n\nNull value check\n\n\nShow code\nconn.table('retail_sales').pl().null_count()\n\n\n\nshape: (1, 11)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ncogs\ntotal_sale\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nData Analysis\nWrite a SQL query to retrieve all columns for sales made on ‚Äò2023-11-23‚Äô\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where sale_date = '2023-11-23'\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n\n\n\n\nWrite a SQL query to retrieve all transactions where the category is ‚ÄòClothing‚Äô and the quantity sold is more than 4 in the month of Nov-2022\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where category = 'Clothing'\n        and extract('month' from sale_date) = '11'\n        and quantity &gt;= 2\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n14\n2023-11-22\n13:30:00\n114\n\"Female\"\n27\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n22\n2023-11-20\n15:30:00\n122\n\"Female\"\n28\n\"Clothing\"\n2\n49.990002\n99.980003\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n30\n2023-11-17\n17:30:00\n130\n\"Female\"\n42\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n32\n2023-11-16\n14:15:00\n456\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n40\n2023-11-14\n18:30:00\n2859\n\"Female\"\n27\n\"Clothing\"\n4\n39.990002\n159.960007\n\n\n48\n2023-11-06\n19:00:00\n5443\n\"Female\"\n35\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n\n\n\n\nWrite a SQL query to calculate the total sales for each category\n\n\nShow code\nsales = conn.sql('''\n    select\n        category\n        , round(sum(total_sale),2) as net_sale\n        , count(*) as total_orders\n    from retail_sales\n    group by 1\n    order by total_orders desc\n''').pl()\n\n\n\n\nShow code\nsales\n\n\n\nshape: (9, 3)\n\n\n\ncategory\nnet_sale\ntotal_orders\n\n\nstr\nf64\ni64\n\n\n\n\n\"Electronics\"\n68878.32\n96\n\n\n\"Clothing\"\n12557.76\n96\n\n\n\"Books\"\n4133.04\n80\n\n\n\"Home Goods\"\n8437.84\n56\n\n\n\"Toys\"\n1639.04\n24\n\n\n\"Beauty Products\"\n1359.44\n24\n\n\n\"Home Appliances\"\n9599.76\n24\n\n\n\"Groceries\"\n3069.84\n24\n\n\n\"Sports Equipment\"\n3039.68\n24\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(sales,\n             x=\"net_sale\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','net_sale',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Sales by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to find the average age of customers who purchased items from the ‚ÄòClothing‚Äô category\n\n\nShow code\nconn.sql('''\n    select\n        round(avg(age), 2) as avg_age\n    from retail_sales\n    where category = 'Clothing'\n''').pl()\n\n\n\nshape: (1, 1)\n\n\n\navg_age\n\n\nf64\n\n\n\n\n33.25\n\n\n\n\n\n\nWrite a SQL query to find all transactions where the total_sale is greater than 1,000\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where total_sale &gt; 999\n''').pl()\n\n\n\nshape: (24, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n\n\n\n\nWrite a SQL query to find the total number of transactions made by each gender in each category\n\n\nShow code\nconn.sql('''\n    select\n        category\n        , gender\n        , count(*) as total_trans\n    FROM retail_sales\n    group by category\n            , gender\n    order by 2\n''').pl()\n\n\n\nshape: (9, 3)\n\n\n\ncategory\ngender\ntotal_trans\n\n\nstr\nstr\ni64\n\n\n\n\n\"Clothing\"\n\"Female\"\n96\n\n\n\"Home Goods\"\n\"Female\"\n56\n\n\n\"Toys\"\n\"Female\"\n24\n\n\n\"Home Appliances\"\n\"Female\"\n24\n\n\n\"Beauty Products\"\n\"Female\"\n24\n\n\n\"Groceries\"\n\"Male\"\n24\n\n\n\"Electronics\"\n\"Male\"\n96\n\n\n\"Sports Equipment\"\n\"Male\"\n24\n\n\n\"Books\"\n\"Male\"\n80\n\n\n\n\n\n\nWrite a SQL query to calculate the average sale for each month\n\n\nShow code\nconn.sql('''\n    select\n        year\n        , month\n        , avg_sale\n    from\n        (\n        select\n            extract(year from sale_date) as year\n            , EXTRACT(month from sale_date) as month\n            , round(avg(total_sale),2) as avg_sale\n            , rank() over(partition by extract(year from sale_date)\n            order by avg(total_sale) desc) as rank\n        from retail_sales\n        group by 1, 2\n        ) as t1\n''').pl()\n\n\n\nshape: (2, 3)\n\n\n\nyear\nmonth\navg_sale\n\n\ni64\ni64\nf64\n\n\n\n\n2023\n11\n254.61\n\n\n2023\n10\n198.3\n\n\n\n\n\n\nWrite a SQL query to find the top 5 customers based on the highest total sales\n\n\nShow code\nconn.sql('''\n    select customer_id\n            , round(sum(total_sale),2) as total_sales\n    from retail_sales\n    group by 1\n    order by 2 desc\n    limit 5\n''').pl()\n\n\n\nshape: (5, 2)\n\n\n\ncustomer_id\ntotal_sales\n\n\ni32\nf64\n\n\n\n\n129\n7999.92\n\n\n105\n7999.92\n\n\n109\n7999.84\n\n\n113\n6399.92\n\n\n117\n6399.84\n\n\n\n\n\n\nWrite a SQL query to find the number of unique customers who purchased items from each category\n\n\nShow code\ncustomers = conn.sql('''\n    select category\n            , count(distinct customer_id) as count_unique\n    from retail_sales\n    group by category\n    order by 2 desc\n''').pl()\n\n\n\n\nShow code\ncustomers\n\n\n\nshape: (9, 2)\n\n\n\ncategory\ncount_unique\n\n\nstr\ni64\n\n\n\n\n\"Clothing\"\n12\n\n\n\"Electronics\"\n12\n\n\n\"Books\"\n10\n\n\n\"Home Goods\"\n7\n\n\n\"Beauty Products\"\n3\n\n\n\"Toys\"\n3\n\n\n\"Groceries\"\n3\n\n\n\"Home Appliances\"\n3\n\n\n\"Sports Equipment\"\n3\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(customers,\n             x=\"count_unique\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','count_unique',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Unique Customers Purchases by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to create each shift and number of orders (Example Morning &lt;12, Afternoon Between 12 & 17, Evening &gt;17)\n\n\nShow code\nconn.sql('''\n    with hourly_sale as\n        (\n        select *\n            , case \n                when extract(hour from sale_time) &lt;12 then 'Morning'\n                when extract(hour from sale_time) between 12 and 17 then 'Afternoon'\n            else 'Evening'\n            end as shift\n        from retail_sales\n        )\n    select\n        shift\n        , count(*) as total_orders\n    from hourly_sale\n    group by shift\n''').pl()\n\n\n\nshape: (3, 2)\n\n\n\nshift\ntotal_orders\n\n\nstr\ni64\n\n\n\n\n\"Evening\"\n32\n\n\n\"Morning\"\n136\n\n\n\"Afternoon\"\n280\n\n\n\n\n\n\n\n\nClose connection\n\n\nShow code\n# Close connection\nconn.close()\n\n\n\n\nConclusions\nThis project demonstrates the power of combining Python and DuckDB for efficient and insightful SQL analysis. By mastering these tools, data analysts can streamline their workflows, uncover valuable insights, and make data-driven decisions that drive business success.\nWe‚Äôve explored the fundamental concepts of SQL and its practical applications in data analysis. By leveraging the capabilities of Python and DuckDB, we‚Äôve been able to efficiently query, clean, and analyze data. This knowledge and skillset can be applied to a wide range of data-driven tasks, from simple data exploration to complex predictive modeling. As we continue to delve deeper into the world of data, the combination of Python and DuckDB promises to be a powerful tool for data analysts.\nThese insights can be used to optimize marketing strategies, improve customer satisfaction, and drive revenue growth. As data continues to proliferate, the ability to harness its power through SQL analysis will become increasingly important for businesses to stay competitive.\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/pyspark example.html",
    "href": "portfolio2/posts/python/pyspark example.html",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "",
    "text": "Figure¬†1: Pyspark Logo"
  },
  {
    "objectID": "portfolio2/posts/python/pyspark example.html#conclusions",
    "href": "portfolio2/posts/python/pyspark example.html#conclusions",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "Conclusions",
    "text": "Conclusions\nPySpark is a powerful and versatile tool for working with big data. Its combination of Spark‚Äôs distributed computing capabilities and Python‚Äôs ease of use makes it an excellent choice for data scientists and analysts looking to tackle large-scale data processing tasks.\nWhether you‚Äôre performing complex data transformations, building machine learning models, or analyzing real-time streams, PySpark can help you unlock the potential of your data. So, dive in, explore its features, and unleash the power of big data!"
  },
  {
    "objectID": "portfolio2/posts/python/pyspark example.html#references",
    "href": "portfolio2/posts/python/pyspark example.html#references",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "References",
    "text": "References\n\nOfficial PySpark Documentation\nGetting Started Guide\nTutorials & Examples"
  },
  {
    "objectID": "portfolio2/posts/python/pyspark example.html#contact",
    "href": "portfolio2/posts/python/pyspark example.html#contact",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/lego_history.html",
    "href": "portfolio2/posts/python/lego_history.html",
    "title": "Exploring Lego Toys with Polars",
    "section": "",
    "text": "Figure¬†1: Lego Toys"
  },
  {
    "objectID": "portfolio2/posts/python/lego_history.html#reading-datasets",
    "href": "portfolio2/posts/python/lego_history.html#reading-datasets",
    "title": "Exploring Lego Toys with Polars",
    "section": "Reading datasets",
    "text": "Reading datasets\nA comprehensive database of lego blocks is provided by Rebrickable.\nThe data is available as csv file and the schema is shown below\n\n\n\n\nDatabase schema\n\nLet us start by reading in the colors data to get a sense of the diversity of lego sets!"
  },
  {
    "objectID": "portfolio2/posts/python/why_bi_tools_fall_short.html",
    "href": "portfolio2/posts/python/why_bi_tools_fall_short.html",
    "title": "Why BI Tools Fall Short, A Failure to Capture the Office Workflow",
    "section": "",
    "text": "Overview\nCompanies use data warehouses or data lakes for centralized data storage, consistency, data quality, scalability and easy access. Business Intelligence (BI) solutions in conjunction with data warehouses are used to make more informed, data-driven decisions by means of dahsboards for stakeholders.\nIn this fashion, a dashboard is created using a BI application, connected to a data warehouse with the aim to be consumed by end users for their business activities.\nIt is an open secret, nonetheless, that staff steadily use spreadsheets to store information and manipulate data sets coming from data warehouses, other information systems and dashboards.\nIn a similar fashion, staff steadily use slide presentations to showcase insights and reports to managers and other stake holders. This means there are countless presentations and data analyses stored in local Excel and PowerPoint files.\n\n\nBeyond the Dashboard: Reporting with Slideshows\nDashboards can be used to gather and analyze data, while slideshows can be used to present the findings in a clear and concise manner. Besides, it enables you to use your existing data insights in the tools you‚Äôre most familiar with, without having to switch to more complex ones.\n\nYou can just do your data analysis in Excel and then present it in PowerPoint. This provides you with just the flexibility you need for.\n\nWhile a dashboard is a centralized section that displays your data visually typically by using a license BI tool (Tableau, Power BI), staff prefers to present data insights to potential customers or coworkers in Excel or PowerPoint by copying and pasting charts and tables from dashboards.\n\n\n\n\n\n\nIndeed, BI tools like Tableau or Power BI offer options to download data to Excel or csv files, PowerPoint and images.\n\n\n\nAutomating Spreadsheet Data with¬†Python\nI propose the process of creating an ETL from the data warehouse to a spreadsheet using Python, and synchronizing tables and charts from Excel to PowerPoint to get an automated reporting in a local file with the needs of the end user.\nETL (Extract, Transform, Load) is a data integration process that involves 03 main steps:\n\nExtract Phase. Retrieving data from a source system (in this case, a data warehouse).\nTransform Phase. Manipulating, cleaning, and aggregating the extracted data.\nLoad Phase. Storing the transformed data into a target system (in this case, an Excel file).\n\n\n\n\n\n\n\nFigure¬†1: Image by El Mehdi Ettaki\n\n\n\n\n\nCase Study\nI‚Äôll show an example using Snowflake as data warehouse, Python for ETL process, and Excel as destination. Finally, PowerPoint will present the data insights.\n\nBy foregoing BI tools, we can substantially reduce project expenses.\n\n\n\n\n\n\n\nFigure¬†2: Image by author\n\n\n\n\nImport libraries\n\n\nimport pandas as pd\nfrom snowflake.snowpark import Session, Window\nimport snowflake.snowpark.functions as F\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nRead credentials\n\n\n# Credentials\nfile = 'credentials.json'\n# read file\nwith open(file) as f:\n    keys = json.load(f)\n\n\nConnect to Data Warehouse using Snowpark\n\n\n# Snowflake's Snowpark Connection\nconnection = {\n    \"account\": keys['account'],\n    \"user\": keys['user'],\n    \"role\": keys['role'],\n    \"authenticator\": keys['authenticator'],\n    \"warehouse\": keys['warehouse'],\n    \"database\": keys['database'],\n    \"schema\": keys['schema'],\n}\n\ndef snowflake_connection():\n    try:\n        session = Session.builder.configs(connection).create()\n        print(\"Connection successful!\")\n    except:\n        raise ValueError(\"Connection failed!\")\n    return session\n\nsession = snowflake_connection()\n\n\nExtract data using Snowpark\n\n\n# Extract sales data\nsales = conn.sql('''\n  SELECT\n      store_id,\n      SUM(sales_amount) AS total_sales\n  FROM\n      dm_sales\n  WHERE\n      YEAR(dm_sales) = YEAR(CURDATE())\n  GROUP BY\n      store_id\n''')\n# Extract stores data\nstores = (\n  conn.table('dm_stores')\n      .select('location','id','responsible')\n)\n# Label stores region\nstores = stores.with_column(\n        'region',\n        F.when(F.col('region_abv')=='AS', 'ASPAC')\n          .when(F.col('region_abv')=='LA', 'LATAM')\n          .when(F.col('region_abv')=='EU', 'EMEA')\n          .when(F.col('region_abv')=='NA', 'NA')\n          .otherwise('null')\n    )\n# Combine tables\ndata = (sales.join(stores, sales.store_id == stores.id))\n# Save to pandas\ndata = data.to_pandas()\n\n\nTransform data using Pandas\n\n\n# Calculate top 10 products\ntop_10 = data.nlargest(10, 'total_sales')\n\n\nLoad data using Pandas\n\n\n# Write to Excel\ntop_10.to_excel('top_10_products.xlsx', index=False)\n\n\n\nFrom Excel to PowerPoint: Automating Report¬†Creation\nNow, you can customize your tables and charts in Excel with the data saved from Python.¬†\nTo automate your customized tables and charts created in Excel onto PowerPoint, you just need to follow the next steps:\n\n\n\n\n\n\n\n\n\n\nFinally, after customizing your slideshow, you will get a PowerPoint like the following:\n\n\n\n\n\n\nFigure¬†3: Image by author\n\n\n\n\n\nConclusions\nThe use of spreadsheets and slideshows in businesses is not going to disappear soon. Hence, even if BI tools like Tableau or Power BI are used in businesses, Excel and PowerPoint are going to be used by staff to reporting and presentations to coworkers and managers.\nBy following the above steps and leveraging the power of Python, you can efficiently extract, transform, and load data from your data warehouse into Excel for further analysis and insights that fulfills end user‚Äôs requirements.\nMaybe is it time to recalculate the cost-benefit implications for companies to abandon expensive BI licenses in favor of flexible, cost-effective open-source solutions like Python.\n\n\nReferences\n\nBusiness (2023) How to Design a Dashboard Presentation: A Step-by-Step Guide in slidemodel.com\nKarlson, P. (2022) Are Spreadsheets Secretly Running Your Business? in Forbes\nMoore J. (2024) But, Can I Export it to Excel? in Do Mo(o)re with Data\nSchwab, P. (2021) Excel dominates the business world.. and that‚Äôs not about to change in Into the Minds\n\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/gasoline-web-nocode.html",
    "href": "portfolio2/posts/python/gasoline-web-nocode.html",
    "title": "Gasoline Prices in Mexico",
    "section": "",
    "text": "Figure¬†1: Gas Prices in Mexico Report"
  },
  {
    "objectID": "portfolio2/posts/python/gasoline-web-nocode.html#table-overview",
    "href": "portfolio2/posts/python/gasoline-web-nocode.html#table-overview",
    "title": "Gasoline Prices in Mexico",
    "section": "Table overview",
    "text": "Table overview\nThe table below provides a preview of the working dataset, offering a glimpse into the structure and type of data being utilized.\n\n\n\n\n\n\n\n\n\n\nstate\nmunicipality\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nCHIHUAHUA\nJUAREZ\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nCOAHUILA DE ZARAGOZA\nSABINAS\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nCOAHUILA DE ZARAGOZA\nTORREON\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nDURANGO\nGOMEZ PALACIO\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nNUEVO LEON\nABASOLO\n$21.63\n$23.31\n$1.68\n7.76%\n\n\n\n\n\n\nTable¬†1: Dataset preview"
  },
  {
    "objectID": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-states-with-highest-gasoline-prices",
    "href": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-states-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with highest gasoline prices",
    "text": "Top 05 States with highest gasoline prices\nThe following table presents a snapshot of the States with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nQUINTANA ROO\n$21.46\n$24.80\n$3.34\n15.55%\n\n\nYUCATAN\n$21.46\n$24.79\n$3.33\n15.52%\n\n\nNAYARIT\n$22.54\n$24.76\n$2.23\n9.90%\n\n\nGUERRERO\n$22.37\n$24.76\n$2.39\n10.67%\n\n\nSINALOA\n$22.36\n$24.72\n$2.36\n10.59%\n\n\n\n\n\n\nTable¬†2: Top 05 States with highest prices"
  },
  {
    "objectID": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-states-with-lowest-gasoline-prices",
    "href": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-states-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with lowest gasoline prices",
    "text": "Top 05 States with lowest gasoline prices\nThe following table presents a snapshot of the States with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nTAMAULIPAS\n$21.22\n$23.10\n$1.87\n8.79%\n\n\nCOAHUILA DE ZARAGOZA\n$22.25\n$23.12\n$0.87\n4.01%\n\n\nCHIHUAHUA\n$21.47\n$23.36\n$1.89\n8.90%\n\n\nNUEVO LEON\n$21.41\n$23.40\n$1.99\n9.32%\n\n\nVERACRUZ DE IGNACIO DE LA LLAVE\n$21.59\n$23.44\n$1.86\n8.62%\n\n\n\n\n\n\nTable¬†3: Top 05 States with lowest prices"
  },
  {
    "objectID": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "href": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with highest gasoline prices",
    "text": "Top 05 Municipalities with highest gasoline prices\nThe following table presents a snapshot of the Municipalities with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nNUEVA ITALIA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nZIRACUARETIRO, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nGABRIEL ZAMORA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nHUIRAMBA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nNAHUATZEN, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\n\n\n\n\nTable¬†4: Top 05 Municipalities with highest prices"
  },
  {
    "objectID": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "href": "portfolio2/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with lowest gasoline prices",
    "text": "Top 05 Municipalities with lowest gasoline prices\nThe following table presents a snapshot of the Municipalities with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nANAHUAC, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nCD. GUERRERO, TAMAULIPAS\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nLAMPAZOS DE NARANJO, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nALLENDE, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\nMORELOS, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\n\n\n\n\nTable¬†5: Top 05 Municipalities with lowest prices"
  },
  {
    "objectID": "portfolio2/posts/sql/sql.html#what-is-sql",
    "href": "portfolio2/posts/sql/sql.html#what-is-sql",
    "title": "Diving into the World of SQL",
    "section": "What is SQL?",
    "text": "What is SQL?\nSQL stands for Structured Query Language. It is a standardized language for managing data in relational databases. Think of a relational database as a highly organized digital filing cabinet, where information is stored in tables with rows and columns. SQL provides the tools to not only retrieve specific data from these tables but also to create, modify, and delete tables and their contents."
  },
  {
    "objectID": "portfolio2/posts/sql/sql.html#why-learn-sql",
    "href": "portfolio2/posts/sql/sql.html#why-learn-sql",
    "title": "Diving into the World of SQL",
    "section": "Why Learn SQL?",
    "text": "Why Learn SQL?\nThe widespread adoption of relational databases like MySQL, PostgreSQL, Microsoft SQL Server, and Oracle Database makes SQL a highly sought-after skill. Learning SQL opens doors to a variety of opportunities, including:\n\nData Analysis\n\nSQL allows you to extract meaningful insights from vast datasets, identify trends, and generate reports that drive business decisions.\n\nDatabase Administration\n\nManaging and maintaining databases, ensuring data integrity, and optimizing performance often requires a deep understanding of SQL.\n\nWeb Development\n\nMany web applications rely on databases to store and retrieve information. SQL is essential for building dynamic websites and applications.\n\nData Science\n\nSQL is a fundamental tool for data scientists, enabling them to clean, prepare, and explore data before applying more advanced statistical and machine learning techniques."
  },
  {
    "objectID": "portfolio2/posts/sql/sql.html#key-sql-concepts",
    "href": "portfolio2/posts/sql/sql.html#key-sql-concepts",
    "title": "Diving into the World of SQL",
    "section": "Key SQL Concepts:",
    "text": "Key SQL Concepts:\nSQL is built around a set of commands that allow you to perform various operations. Here are some of the most important concepts:\n\nSELECT\n\nThis command is used to retrieve data from one or more tables based on specified criteria. You can select specific columns, filter data using conditions, and sort the results. For example: SELECT name, age FROM users WHERE city = ‚ÄòNew York‚Äô;\n\nINSERT\n\nThis command adds new rows of data into a table. For example: INSERT INTO users (name, age, city) VALUES (‚ÄòJohn Doe‚Äô, 30, ‚ÄòLondon‚Äô);\n\nUPDATE\n\nThis command modifies existing data in a table. For example: UPDATE users SET age = 31 WHERE name = ‚ÄòJohn Doe‚Äô;\n\nDELETE\n\nThis command removes rows from a table. For example: DELETE FROM users WHERE age = 65;\n\nCREATE\n\nThis command is used to create new database objects, such as tables, views, and indexes.\n\nDROP: This command deletes existing database objects."
  },
  {
    "objectID": "portfolio2/posts/sql/sql.html#tasks-performed-with-sql",
    "href": "portfolio2/posts/sql/sql.html#tasks-performed-with-sql",
    "title": "Diving into the World of SQL",
    "section": "Tasks performed with SQL",
    "text": "Tasks performed with SQL\n\nCreate and drop tables\nInsert and update records\nRetrieve data\nDelete data\nManage permissions\nCreate views and procedures"
  },
  {
    "objectID": "portfolio2/posts/sql/sql.html#advanced-tasks-in-sql",
    "href": "portfolio2/posts/sql/sql.html#advanced-tasks-in-sql",
    "title": "Diving into the World of SQL",
    "section": "Advanced tasks in SQL",
    "text": "Advanced tasks in SQL\n\nJOINs: Combining data from multiple tables based on related columns.\nSubqueries: Embedding queries within other queries to perform more sophisticated data retrieval.\n\nAggregate Functions: Performing calculations on groups of data, such as calculating averages, sums, and counts.\n\nWindow functions:"
  },
  {
    "objectID": "portfolio2/posts/sql/sql.html#example-queries",
    "href": "portfolio2/posts/sql/sql.html#example-queries",
    "title": "Diving into the World of SQL",
    "section": "Example queries",
    "text": "Example queries\n-- COUNT CUSTOMERS\nSELECT COUNT(1) AS customers\nFROM addresses\n;\n-- GET RETAIL SALES DATASET\nSELECT p.id_purchase\n    , d.priceInfo\n    , d.quantity\n    , s.id_shopper\n    , s.id_region\n    , r.description AS region\n    , s.id_territory\n    , t.description AS territory\n    , s.id_store\n    , b.description AS store\n    , s.NUD\n    , s.shop\n    , s.route\n    , s.purchase_date\n    , p.cancel\nFROM purchases AS p\nINNER JOIN purchase_details AS d ON p.id_purchase = d.id_purchase \nINNER JOIN shops AS s ON p.id_shopper = s.id_shopper\nINNER JOIN territories AS t ON s.id_territory = t.id_territory\nINNER JOIN stores AS b ON s.id_store = b.id_store\nINNER JOIN regions AS r ON s.id_region = r.id_region\n;\n-- GET PURCHASES BY CUSTOMER\nSELECT o.no_order\n    , o.id_order_status\n    , e.description\n    , o.id_social_network\n    , CASE\n        WHEN o.id_social_network = 1 THEN 'I'\n        WHEN o.id_social_network = 2 THEN 'F'\n        ELSE 'W'\n    END AS origin\n    , o.quantity \n    , o.total AS total_order\n    , o.purchase_date\n    , d.route AS delivery_route\n    , d.id_store \n    , d.nud\n    , d.email \n    , d.phone \nFROM orders o\nLEFT JOIN orderStatus e ON o.id_order_status = e.id_order_status\nLEFT JOIN addresses d ON o.id_address = d.id_address\nWHERE 1 = 1\n-- AND DATE(fs_ConvertDateToMexico(o.purchase_date))\n-- BETWEEN '2023-04-01' AND '2023-04-30'\n;\n-- GET UNIQUE BRANDS\nSELECT DISTINCT g.brand_code as Brand_Code\n, g.brand_name as Brand_Name\n, g.style_num_offset as Style_Num_Offset\n, g.active as Active\n, g.parent_brand as Parent_Brand\n, g.reporting_brand as Reporting_Brand\nFROM\ntp_brand g\n;\n-- GET FACTORY DATA WITH NULL COUNTRY OR CITY\nSELECT CONVERT(e.id,char) as Factory_Id\n, e.factory_name  as PO_FTY\n, ifnull(e.factory_city,'No City') as City\n, ifnull(e.factory_country,'No Country') as Country\nFROM\ntp_factories e\n;\n-- INSER DATA INTO TABLE\ninsert into CatAlumnos\nvalues(‚ÄòJuan‚Äô, ‚ÄòPerez‚Äô, ‚ÄòHuerta‚Äô, 1990-02-25, 2, ‚Äòa‚Äô)\n;\n/* GET STUDENT AGE POSTGRES FUNCTION */\nWITH cte_ages AS (\n    SELECT name, age(CURRENT_DATE, bod) AS student_age\n    FROM students\n)\nSELECT name, age\nFROM ages\nWHERE age &lt; 18\n;\n-- CREATE VIEW IN SQL SERVER\nCREATE VIEW student_profiles AS\n    SELECT concat(name, ' ' surname) AS student_name,\n/* Using SQL Server datediff function to reckon age */\n    DATEDIFF(YY, getdate(), bod) as age\n    FROM students\n    ORDER BY name\n;\n/* CONCAT AND CAST DATA TYPES */\nSELECT \n  concat('gsn','_', cast(campaign_id as text)) AS id_gsn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_2\nTOP 5\n;\n/* CREATE A UNION OF TABLES FROM 02 TEMP TABLES (CTEs) */\nWITH table_2_temp AS (\nSELECT\n  concat('gsn', '_', cast(campaign_id as text)) AS id_gsn\n, start_date \n, end_date\n, clicks\n, views\nFROM table_2\n)\n, table_3_temp AS (\nSELECT \nconcat('fbn', '_', cast(campaign_id as text)) AS id_fbn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_3\nWHERE cliks &gt; 0\n)\nSELECT \n  t2.id_gsn\nFROM table_2_temp t2\nUNION ALL\nSELECT \n  t3.id_fbn\nFROM table_3_temp t3\n;\n/* Window functions\nAdd row numbers to the placings table */\nSELECT\n    'year'\n    , host_country\n    , first_place\n    , total_goals\n    , row_number() OVER() AS row_num\nFROM world_cup_placings\n;\n\n/* Using SUM() within our window function */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , SUM(total_goals) OVER() AS all_goals\nFROM world_cup_placings\n;\n\n/* Computing the average number of goals */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , round(avg(total_goals) OVER(), 0) AS mean_goals\nFROM world_cup_placings\n;\n-- Big Query platform\n-- covid cases in North America 2020-2023\nwith years as (\n    select country_region\n        , extract(year from date) as year\n        , sum(confirmed) as total_confirmed\n        , sum(deaths) as total_deaths\n        , sum(cast(recovered as integer)) as total_recovered\n        , sum(active) as total_active\n    from `big-query.public-data.covid19_jhu_csse.summary`\n    where country_region in ('Mexico', 'US', 'Canada')\n    group by 1, 2\n    order by 1, 2\n)\nselect *\nfrom (\n    select \n        year\n        , country_region\n        , total_confirmed\n    from years\n)\npivot(sum(total_confirmed) as confirmed for year in (2020, 2021, 2022, 2023))\n;"
  },
  {
    "objectID": "portfolio2/posts/sql/sql.html#contact",
    "href": "portfolio2/posts/sql/sql.html#contact",
    "title": "Diving into the World of SQL",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html",
    "href": "portfolio2/posts/python/cohort_analysis.html",
    "title": "Cohort Analysis",
    "section": "",
    "text": "Cohort analysis is a type of analysis that follows a group of customers (cohorts) over time to understand their behavior, based on a characteristic that identifies them during a period of time. Periods could be a day, week, month, or even a year. Cohort analysis takes into account time series data over the lifespan of a user to predict and analyze their future consumption patterns.\nCohorts are commonly created in accordance with acquisition date shared by a group of users. An acquisition cohort refers to a group of users who were newly acquired (e.g account creation, first purchase) by a business. On the other hand, users are assigned to behavioural cohorts when they perform a specific action during a time frame.\nBy analysing each cohort in isolation, behavioural differences between groups can be identified. These observations can then be used to inform marketing and product development decisions.\nCohort analyses are important because churn is one of the main problems faced by companies nowadays, and every information that can lead to less churning is a valuable business information.\nSpecifically, cohort analysis can be used to:\nMeasure Customer Lifetime Value (CLV)\nCohort analysis can be used to identify patterns in retention & monetary value of different user groups. By identifying valuable groups, strategies can be developed to replicate their characteristics among other groups.\nEvaluate Marketing Campaigns\nMonitoring how groups respond to different campaigns can help reveal the true effectiveness of marketing. For example, a big discount promotion may drive high levels of acquisition, but these users may have little interest in full price goods ‚Äî making them likely to churn. Over time, it may be observed that CLV is higher for campaigns that drove lower levels of acquisition.\nImprove Product Development\nAssigning users to cohorts, based on the actions they have taken within a product, can provide insights into the features that promote increases in user retention and engagement.\ne-commerce\nCustomers who have started using the service just two or three days ago or had just made their first purchase can be grouped into one group or cohort whereas people who have been on the website for 1 or 2 years may be grouped into another cohort and their purchase activity over the year can be used to predict how many times they will visit the website in the future or how many times they will actually click on a product and purchase it. This can be used to personalize their own home feed so that the click-through rate increases and their retention for that particular website don‚Äôt subside.\nWebsites\nCohort analysis can be used to predict viewer retention when a new person logs in for the first time vs a person who has been on the platform for a long time. To keep the new user on the platform, their home feed will be personalized with the most popular videos that have a huge amount of views, clickbait thumbnails, or videos with popular search terms based on that particular user‚Äôs location data whereas a long-time user will mostly receive video updates from their subscriptions and their most watched channels."
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#what-is-cohort-analysis",
    "href": "portfolio2/posts/python/cohort_analysis.html#what-is-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "",
    "text": "Cohort analysis is a type of analysis that follows a group of customers (cohorts) over time to understand their behavior, based on a characteristic that identifies them during a period of time. Periods could be a day, week, month, or even a year. Cohort analysis takes into account time series data over the lifespan of a user to predict and analyze their future consumption patterns.\nCohorts are commonly created in accordance with acquisition date shared by a group of users. An acquisition cohort refers to a group of users who were newly acquired (e.g account creation, first purchase) by a business. On the other hand, users are assigned to behavioural cohorts when they perform a specific action during a time frame.\nBy analysing each cohort in isolation, behavioural differences between groups can be identified. These observations can then be used to inform marketing and product development decisions.\nCohort analyses are important because churn is one of the main problems faced by companies nowadays, and every information that can lead to less churning is a valuable business information.\nSpecifically, cohort analysis can be used to:\nMeasure Customer Lifetime Value (CLV)\nCohort analysis can be used to identify patterns in retention & monetary value of different user groups. By identifying valuable groups, strategies can be developed to replicate their characteristics among other groups.\nEvaluate Marketing Campaigns\nMonitoring how groups respond to different campaigns can help reveal the true effectiveness of marketing. For example, a big discount promotion may drive high levels of acquisition, but these users may have little interest in full price goods ‚Äî making them likely to churn. Over time, it may be observed that CLV is higher for campaigns that drove lower levels of acquisition.\nImprove Product Development\nAssigning users to cohorts, based on the actions they have taken within a product, can provide insights into the features that promote increases in user retention and engagement.\ne-commerce\nCustomers who have started using the service just two or three days ago or had just made their first purchase can be grouped into one group or cohort whereas people who have been on the website for 1 or 2 years may be grouped into another cohort and their purchase activity over the year can be used to predict how many times they will visit the website in the future or how many times they will actually click on a product and purchase it. This can be used to personalize their own home feed so that the click-through rate increases and their retention for that particular website don‚Äôt subside.\nWebsites\nCohort analysis can be used to predict viewer retention when a new person logs in for the first time vs a person who has been on the platform for a long time. To keep the new user on the platform, their home feed will be personalized with the most popular videos that have a huge amount of views, clickbait thumbnails, or videos with popular search terms based on that particular user‚Äôs location data whereas a long-time user will mostly receive video updates from their subscriptions and their most watched channels."
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#steps-for-cohort-analysis",
    "href": "portfolio2/posts/python/cohort_analysis.html#steps-for-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "Steps for Cohort Analysis",
    "text": "Steps for Cohort Analysis\nThere are 05 main steps to performing cohort analysis. They are as follows:\n\nDetermining the main objective of the cohort analysis: First and foremost, we need to determine the main intent of performing the analysis, such as to analyze why people on YouTube don‚Äôt watch videos after the 6-minute mark. This sets the ultimate goal of the analysis and uses the huge pool of information for practical issues. This helps in pinpointing the root issue or cause and companies can then work towards improved business practices to provide a better user experience.\nDefining the metrics to respond to the question: The next step would be to identify what defines the problem. In simpler words, from the above example, we need to analyze when a viewer leaves a video or at what minute before moving on to something else, his/her watch time, and click-through rates on YouTube.\nIdentify the particular groups or cohorts that will be relevant: To analyze users we need to pick out a group of viewers who display common behavioral patterns. In order to do this we need to analyze data from different user inputs and identify relevant similarities and differences between them and then separate it into specified cohorts.\nPerforming the Cohort Analysis: Now we will use data visualization techniques to perform the cohort analysis based on the objective of the problem. This can be done using many programming languages out of which the preferred languages are python and R. Cohort analysis in python can be done using libraries such as NumPy and seaborn. Heat maps are usually used to display user retention and visualize data in a tabular form.\nTesting the Results: Last but not the least, the results need to be checked and tested in order to make sure that they can actually reduce company losses and optimize business practices. We will obtain retention rates from the analysis and a heatmap(or any other suitable graph) of user retention and retention rate will help us analyze and improve experiences for the users."
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#environment-setting",
    "href": "portfolio2/posts/python/cohort_analysis.html#environment-setting",
    "title": "Cohort Analysis",
    "section": "Environment Setting",
    "text": "Environment Setting\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nShow code\n# read dataset from github\ndf = pd.read_csv('https://raw.githubusercontent.com/nickmancol/python-cohorts/main/data/scanner_data.csv')\n\n\n\n\nShow code\n# show dataframe subset\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\n\n\n\n\n0\n1\n02/01/2016\n2547\n1\nX52\n0EM7L\n1.0\n3.13\n\n\n1\n2\n02/01/2016\n822\n2\n2ML\n68BRQ\n1.0\n5.46\n\n\n2\n3\n02/01/2016\n3686\n3\n0H2\nCZUZX\n1.0\n6.35\n\n\n3\n4\n02/01/2016\n3719\n4\n0H2\n549KK\n1.0\n5.59\n\n\n4\n5\n02/01/2016\n9200\n5\n0H2\nK8EHH\n1.0\n6.88\n\n\n\n\n\n\n\n\n\nShow code\n# drop unnamed column\ndf = df.drop(['Unnamed: 0'], axis = 1)\n# convert column to datetime\ndf.Date = pd.to_datetime(df.Date, format='%d/%m/%Y')\n\n\n\n\nShow code\n# get descriptive stats\ndf.describe(include='number')\n\n\n\n\n\n\n\n\n\nCustomer_ID\nTransaction_ID\nQuantity\nSales_Amount\n\n\n\n\ncount\n131706.000000\n131706.000000\n131706.000000\n131706.000000\n\n\nmean\n12386.450367\n32389.604187\n1.485311\n11.981524\n\n\nstd\n6086.447552\n18709.901238\n3.872667\n19.359699\n\n\nmin\n1.000000\n1.000000\n0.010000\n0.020000\n\n\n25%\n7349.000000\n16134.000000\n1.000000\n4.230000\n\n\n50%\n13496.000000\n32620.000000\n1.000000\n6.920000\n\n\n75%\n17306.000000\n48548.000000\n1.000000\n12.330000\n\n\nmax\n22625.000000\n64682.000000\n400.000000\n707.730000\n\n\n\n\n\n\n\n\n\nShow code\n# count duplicates\ndf.duplicated([\"Date\",\"Customer_ID\"]).sum()\n\n\nnp.int64(68979)\n\n\n\n\nShow code\nprint(f'The dataset has {df.size:,.2f} rows.')\n\n\nThe dataset has 921,942.00 rows.\n\n\n\n\nShow code\n# Group by date and customer and sum quantity and sales selecting last items\ndf = pd.DataFrame(df.groupby([\"Date\",\"Customer_ID\"]).agg({'Transaction_ID':'max'\n                                                          ,'SKU_Category':'max'\n                                                          ,'SKU':'max'\n                                                          ,'Quantity':'sum'\n                                                          ,'Sales_Amount':'sum'})\n                 ).reset_index()\n\n\n\n\nShow code\nprint(f'The dataset now has {df.size:,.2f} rows.')\n\n\nThe dataset now has 439,089.00 rows."
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#data-preparation-for-cohort-analysis",
    "href": "portfolio2/posts/python/cohort_analysis.html#data-preparation-for-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "Data Preparation for Cohort Analysis",
    "text": "Data Preparation for Cohort Analysis\nTo run a cohort analysis, we‚Äôll need to:\n\nSplit the data into groups that can be analyzed on the basis of time\nAssign a cohort index for each transaction\nCreate two new columns by applying a lambda function to the date column in order to:\n\nCreate the tx_month column\nTransform tx_month to get the minimum value of tx_month per customer\nAssign tx_month per customer to the acq_month column\n\n\n\n\nShow code\n# create transaction month column with day 1\ndf['tx_month'] = df['Date'].apply(lambda x: dt.date(x.year, x.month,1))\n# create acquisition column based on minimum transaction month column by customer\ndf['acq_month'] = df.groupby('Customer_ID')['tx_month'].transform('min')\n# select rows where transaction dates differ from acquisition date\ndf.loc[df['tx_month'] != df['acq_month']].head()\n\n\n\n\n\n\n\n\n\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\ntx_month\nacq_month\n\n\n\n\n4701\n2016-02-01\n50\n5004\nW41\nZWFSY\n5.0\n2.86\n2016-02-01\n2016-01-01\n\n\n4702\n2016-02-01\n91\n4976\n2ML\n68BRQ\n1.0\n5.79\n2016-02-01\n2016-01-01\n\n\n4709\n2016-02-01\n366\n4852\nJ4R\nVGIW5\n2.0\n15.64\n2016-02-01\n2016-01-01\n\n\n4720\n2016-02-01\n889\n4900\nTEU\nA233P\n2.0\n11.87\n2016-02-01\n2016-01-01\n\n\n4723\n2016-02-01\n1115\n4933\nYMJ\nJNWFX\n1.0\n7.43\n2016-02-01\n2016-01-01\n\n\n\n\n\n\n\n\n\nShow code\n# Claculate the number of months elapsed between transaction and acquisition\ndef diff_month(x):\n    d1 = x['tx_month']\n    d2 = x[\"acq_month\"]\n    return ((d1.year - d2.year) * 12 + d1.month - d2.month)+1\n\n\n\n\nShow code\n# Store the number of months between transaction and acquisition month\ndf['cohort_idx'] = df.apply(lambda x: diff_month(x), axis=1)\n\n\n\n\nShow code\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\ntx_month\nacq_month\ncohort_idx\n\n\n\n\n0\n2016-01-02\n3\n90\nTW8\nY1M2E\n4.0\n10.92\n2016-01-01\n2016-01-01\n1\n\n\n1\n2016-01-02\n178\n84\nR6E\nHO1M5\n2.0\n58.99\n2016-01-01\n2016-01-01\n1\n\n\n2\n2016-01-02\n195\n107\nLGI\nVY2UB\n2.0\n13.10\n2016-01-01\n2016-01-01\n1\n\n\n3\n2016-01-02\n343\n134\nXG4\nZSVWE\n1.0\n6.75\n2016-01-01\n2016-01-01\n1\n\n\n4\n2016-01-02\n399\n136\nP42\nXJLWY\n2.0\n10.43\n2016-01-01\n2016-01-01\n1\n\n\n\n\n\n\n\n\n\nShow code\n# create cohort matrix by counting unique customers\ndef get_cohort_matrix(data, var='Customer_ID', fun=pd.Series.nunique):\n    # group by acquisition and cohort_id\n    cd = data.groupby(['acq_month', 'cohort_idx'])[var].apply(fun).reset_index()\n    # create pivot table with acquisition, cohort_id and counting customers\n    cc = cd.pivot_table(index = 'acq_month',\n                        columns = 'cohort_idx',\n                        values = var)\n    # calculate retention rate\n    # select first column of pivot table, i.e., first cohort_id\n    cs = cc.iloc[:,0]\n    # divide all values in the cohort matrix by the corresponding initial cohort_id size\n    retention = cc.divide(cs, axis = 0)\n    # create percentage and round decimals\n    retention = retention.round(3)\n    # return the matrix and retention rate for each cohort_id\n    return cc, retention\n\n\n\n\nShow code\ncc, retention = get_cohort_matrix(df)\n\n\n\n\nShow code\n# format index datetie\nretention.index = pd.to_datetime(retention.index).strftime('%b %Y')\n\n\nThe rows of the pivot table consist of the beginning of user activity or the month from which the user has started visiting the ecommerce website or has made the first purchase. The columns represent the user‚Äôs retention rate or how long has the user been coming back to purchase since his first time.\n\n\nShow code\nretention\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 2016\n1.0\n0.385\n0.302\n0.176\n0.078\n0.058\n0.045\n0.040\n0.027\n0.016\n0.012\n0.012\n\n\nFeb 2016\n1.0\n0.215\n0.131\n0.066\n0.043\n0.031\n0.031\n0.026\n0.010\n0.008\n0.006\nNaN\n\n\nMar 2016\n1.0\n0.282\n0.246\n0.227\n0.201\n0.197\n0.201\n0.194\n0.194\n0.201\nNaN\nNaN\n\n\nApr 2016\n1.0\n0.288\n0.250\n0.219\n0.224\n0.235\n0.224\n0.224\n0.230\nNaN\nNaN\nNaN\n\n\nMay 2016\n1.0\n0.238\n0.213\n0.192\n0.216\n0.210\n0.198\n0.199\nNaN\nNaN\nNaN\nNaN\n\n\nJun 2016\n1.0\n0.177\n0.175\n0.178\n0.177\n0.177\n0.191\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nJul 2016\n1.0\n0.142\n0.151\n0.156\n0.150\n0.166\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAug 2016\n1.0\n0.154\n0.130\n0.135\n0.119\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSep 2016\n1.0\n0.190\n0.162\n0.183\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nOct 2016\n1.0\n0.141\n0.150\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNov 2016\n1.0\n0.140\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nDec 2016\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12, 11))\nsns.heatmap(data=retention,\n            annot=True,\n            fmt='0.1%',\n            vmin=0.0,\n            vmax=0.1,\n            cmap='Blues')\nplt.show()"
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#segmented-by-quantity",
    "href": "portfolio2/posts/python/cohort_analysis.html#segmented-by-quantity",
    "title": "Cohort Analysis",
    "section": "Segmented by Quantity",
    "text": "Segmented by Quantity\n\n\nShow code\ncc_q, ret_q = get_cohort_matrix(df, var='Quantity', fun=pd.Series.mean)\ncc_q\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01\n3.164760\n3.345814\n3.134125\n3.043550\n3.509021\n3.644632\n3.640000\n2.857143\n3.851575\n2.989362\n6.000000\n2.494737\n\n\n2016-02-01\n2.986317\n2.720292\n3.063029\n2.815029\n3.089138\n3.820513\n2.364706\n2.484375\n6.640000\n1.650000\n1.823529\nNaN\n\n\n2016-03-01\n2.919831\n3.354397\n3.753517\n3.625239\n3.923795\n3.637677\n3.749833\n3.790764\n3.421667\n3.419633\nNaN\nNaN\n\n\n2016-04-01\n2.811928\n3.308361\n3.530516\n2.856732\n3.268678\n3.268426\n3.589691\n3.126519\n3.459594\nNaN\nNaN\nNaN\n\n\n2016-05-01\n2.710384\n3.160431\n3.235338\n3.091508\n3.111241\n3.214499\n3.625133\n3.346479\nNaN\nNaN\nNaN\nNaN\n\n\n2016-06-01\n2.712752\n3.067361\n3.405827\n2.577969\n3.165492\n2.727486\n3.466247\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-07-01\n2.730425\n2.467593\n2.674009\n3.529661\n2.740444\n3.260802\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-08-01\n2.607402\n2.758663\n2.699218\n3.588889\n2.625000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-09-01\n2.918562\n3.190777\n3.692153\n3.604326\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-10-01\n2.674215\n2.563017\n3.124542\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-11-01\n2.553461\n2.943689\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-12-01\n2.655255\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12,11))\nsns.heatmap(cc_q, annot=True,cmap='Blues', fmt='.1%')\nplt.show()"
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#segmented-by-sales-amount",
    "href": "portfolio2/posts/python/cohort_analysis.html#segmented-by-sales-amount",
    "title": "Cohort Analysis",
    "section": "Segmented by Sales Amount",
    "text": "Segmented by Sales Amount\n\n\nShow code\ncc_sa, ret_sa = get_cohort_matrix(df, var='Sales_Amount', fun=pd.Series.median)\ncc_sa\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01\n11.460\n13.290\n12.67\n12.470\n14.685\n15.730\n13.70\n12.05\n12.110\n11.165\n11.78\n13.75\n\n\n2016-02-01\n12.075\n12.360\n12.08\n11.190\n14.265\n14.705\n11.05\n9.91\n12.160\n9.860\n11.94\nNaN\n\n\n2016-03-01\n12.000\n13.650\n13.02\n13.785\n14.325\n13.140\n14.41\n16.25\n13.845\n14.870\nNaN\nNaN\n\n\n2016-04-01\n11.860\n12.620\n13.44\n12.630\n13.210\n13.430\n13.44\n13.80\n14.290\nNaN\nNaN\nNaN\n\n\n2016-05-01\n11.590\n13.535\n12.61\n13.100\n12.190\n12.160\n13.41\n13.12\nNaN\nNaN\nNaN\nNaN\n\n\n2016-06-01\n12.330\n12.690\n11.93\n11.500\n11.705\n12.645\n13.50\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-07-01\n12.030\n11.570\n12.08\n12.715\n13.750\n13.590\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-08-01\n11.880\n10.020\n11.38\n12.205\n11.620\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-09-01\n11.930\n10.740\n12.43\n13.780\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-10-01\n11.985\n11.010\n12.46\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-11-01\n12.130\n14.050\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-12-01\n12.740\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12,10))\nsns.heatmap(cc_sa, annot=True,cmap='Blues', fmt='.1%')\nplt.show()"
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#references",
    "href": "portfolio2/posts/python/cohort_analysis.html#references",
    "title": "Cohort Analysis",
    "section": "References",
    "text": "References\n\n(2021). Bohorquez, N. Cohort Analysis with Python‚Äôs matplotlib, pandas, numpy and datetime in ActiveState, retrieved from https://www.activestate.com/blog/cohort-analysis-with-python"
  },
  {
    "objectID": "portfolio2/posts/python/cohort_analysis.html#contact",
    "href": "portfolio2/posts/python/cohort_analysis.html#contact",
    "title": "Cohort Analysis",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/regression_analysis_overview.html",
    "href": "portfolio2/posts/python/regression_analysis_overview.html",
    "title": "Regression Analysis Overview",
    "section": "",
    "text": "Regression searches for relationships among variables. For example, you can want to understand how salaries depend on diverse features among employees of a company, such as experience, education level, role, city of employment, etc.\nData related to each employee represents one observation. The presumption is that salary depends on experience, education, role, and city.\nBy other hand, you may want to establish the relationship among housing prices on a particular area and number of bedrooms, distance to the city center, among other features.\nIn regression analysis, we consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that at least one of the features depends on the others, you‚Äôll need to establish a relationship among them. In scientific jargon, you need to find a function that maps some features to others sufficiently well.\nCommonly, we call the dependent feature dependent variable or response (generally denoted as y), and the independent features are called independent variables, inputs, or features.\nRegression problems usually works with continuous variables. Features, however,can be continuous, discrete, or even categorical.\nFinally, regression analysis is very important when you want to forecast a response using a new set of features. For example, you may want to predict gasoline consumption of a household for the next period given its price, number of residents in that household, car model, etc."
  },
  {
    "objectID": "portfolio2/posts/python/regression_analysis_overview.html#contact",
    "href": "portfolio2/posts/python/regression_analysis_overview.html#contact",
    "title": "Regression Analysis Overview",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_peace_index.html#what-does-it-measue",
    "href": "portfolio2/posts/python/mexico_peace_index.html#what-does-it-measue",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "What does it measue?",
    "text": "What does it measue?\n\nMeasures peace across the entire country.\nAnalyzes recent violence and peace trends.\nEstimates the economic cost of violence in Mexico.\nIdentifies factors that drive peace and instability."
  },
  {
    "objectID": "portfolio2/posts/python/mexico_peace_index.html#how-its-measured",
    "href": "portfolio2/posts/python/mexico_peace_index.html#how-its-measured",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "How it‚Äôs measured",
    "text": "How it‚Äôs measured\nThe MPI isn‚Äôt just about crime rates. It considers 12 sub-indicators grouped into five major categories:\n\nOngoing Conflict\nSafety and Security\nDomesticized Conflict\nMilitarization\nIncarceration"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_peace_index.html#display-interactive-chart",
    "href": "portfolio2/posts/python/mexico_peace_index.html#display-interactive-chart",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure¬†1: Mexico Peace Index\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\nYou can hover the mouse over the map to get additional information.\n\n\n\n\n\n\n\n\nFigure¬†2: Mexico Peace Index Variation"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_peace_index.html#references",
    "href": "portfolio2/posts/python/mexico_peace_index.html#references",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "References",
    "text": "References\n\nMexico Peace Index"
  },
  {
    "objectID": "portfolio2/posts/python/mexico_peace_index.html#contact",
    "href": "portfolio2/posts/python/mexico_peace_index.html#contact",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html",
    "href": "portfolio2/posts/python/dw_motherduck.html",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "",
    "text": "Figure¬†1: Data Warehouse by Niklas Lang"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#extraction-from-data-sources",
    "href": "portfolio2/posts/python/dw_motherduck.html#extraction-from-data-sources",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Extraction from data sources",
    "text": "Extraction from data sources\n\ncsv files\n\n\nCode\ncsv_files = glob.glob('./datasets/*.csv')\n\n\n\n\nCode\nlist(enumerate(csv_files))\n\n\n[(0, './datasets/watercollection.csv'),\n (1, './datasets/ContainsNull.csv'),\n (2, './datasets/sales_info.csv'),\n (3, './datasets/cdmx-subway.csv'),\n (4, './datasets/airports.csv'),\n (5, './datasets/colors.csv'),\n (6, './datasets/sets.csv'),\n (7, './datasets/appl_stock.csv'),\n (8, './datasets/sales.csv')]\n\n\n\n\njson files\n\n\nCode\njson_files = glob.glob('./datasets/*.json')\n\n\n\n\nCode\nlist(enumerate(json_files))\n\n\n[(0, './datasets/prevalencia.json'), (1, './datasets/people.json')]\n\n\n\n\ndatabase tables\n\n\nCode\ndb_files = glob.glob('datasets/*.db')\n\n\n\n\nCode\nlist(enumerate(db_files))\n\n\n[(0, 'datasets/retail_db.db'), (1, 'datasets/restaurants.db')]"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#data-warehouse-creation",
    "href": "portfolio2/posts/python/dw_motherduck.html#data-warehouse-creation",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data warehouse creation",
    "text": "Data warehouse creation\n\n\nCode\nconn = db.connect('my_database.db')"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#data-warehouse-load",
    "href": "portfolio2/posts/python/dw_motherduck.html#data-warehouse-load",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data warehouse load",
    "text": "Data warehouse load\n\n\nCode\nconn.sql(f\"create or replace table water_collection as select * from '{csv_files[0]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table contains_null as select * from '{csv_files[1]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sales_info as select * from '{csv_files[2]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table cdmx_subway as select * from '{csv_files[3]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table airports as select * from '{csv_files[4]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table colors as select * from '{csv_files[5]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sets as select * from '{csv_files[6]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table appl_stock as select * from '{csv_files[7]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sales as select * from '{csv_files[8]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table prevalencia as select * from '{json_files[0]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table people as select * from '{json_files[1]}' \")\n\n\n\n\nCode\nretail = db.connect('./datasets/retail_db.db')\nretail.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     name     ‚îÇ\n‚îÇ   varchar    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ retail_sales ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nretail_sales_pl = retail.sql('select * from retail_sales').pl()\n\n\n\n\nCode\nconn.execute(\"create or replace table retail_sales as from retail_sales_pl\");\n\n\n\n\nCode\nrestaurants = db.connect('./datasets/restaurants.db')\nrestaurants.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    name     ‚îÇ\n‚îÇ   varchar   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ restaurants ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nrestaurants_pl = restaurants.sql('select * from restaurants').pl()\n\n\n\n\nCode\nconn.execute(\"create or replace table restaurants as from restaurants_pl\");"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#data-retrieval",
    "href": "portfolio2/posts/python/dw_motherduck.html#data-retrieval",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data retrieval",
    "text": "Data retrieval\n\n\nCode\nconn.sql('show databases')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ database_name ‚îÇ\n‚îÇ    varchar    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ my_database   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nconn.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       name       ‚îÇ\n‚îÇ     varchar      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ airports         ‚îÇ\n‚îÇ appl_stock       ‚îÇ\n‚îÇ cdmx_subway      ‚îÇ\n‚îÇ colors           ‚îÇ\n‚îÇ contains_null    ‚îÇ\n‚îÇ people           ‚îÇ\n‚îÇ prevalencia      ‚îÇ\n‚îÇ restaurants      ‚îÇ\n‚îÇ retail_sales     ‚îÇ\n‚îÇ sales            ‚îÇ\n‚îÇ sales_info       ‚îÇ\n‚îÇ sets             ‚îÇ\n‚îÇ water_collection ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     13 rows      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nconn.sql('select * from restaurants limit 5').pl()\n\n\n\nshape: (5, 6)\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\nstr\ni64\nf64\nstr\nstr\ni64\n\n\n\n\n\"The Golden Wok\"\n1477\n33.620488\n\"Berlin\"\n\"American\"\n5\n\n\n\"Greek Gyros\"\n770\n68.388874\n\"New York\"\n\"French\"\n1\n\n\n\"Taste of Italy\"\n4420\n88.23168\n\"Amsterdam\"\n\"Chinese\"\n0\n\n\n\"Midnight Diner\"\n2155\n12.965985\n\"Lisbon\"\n\"Mexican\"\n1\n\n\n\"Taste of Italy\"\n3375\n52.785226\n\"Sydney\"\n\"Chinese\"\n1"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#cloud-data-warehouse-with-motherduck",
    "href": "portfolio2/posts/python/dw_motherduck.html#cloud-data-warehouse-with-motherduck",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Cloud Data Warehouse with Motherduck",
    "text": "Cloud Data Warehouse with Motherduck\n\n\nCode\ndw = db.connect('md')\n\n\n\n\nCode\ndw.sql('select current_database()').show()\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_database() ‚îÇ\n‚îÇ      varchar       ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ my_portfolio       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#convert-queries-from-local-database-to-polars",
    "href": "portfolio2/posts/python/dw_motherduck.html#convert-queries-from-local-database-to-polars",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Convert queries from local database to polars",
    "text": "Convert queries from local database to polars\n\n\nCode\nairports_pl = conn.sql('select * from airports').pl()\nappl_stock_pl = conn.sql('select * from appl_stock').pl()\ncdmx_subway_pl = conn.sql('select * from cdmx_subway').pl()\ncolors_pl = conn.sql('select * from colors').pl()\ncontains_null_pl = conn.sql('select * from contains_null').pl()\npeople_pl = conn.sql('select * from people').pl()\nprevalencia_pl = conn.sql('select * from prevalencia').pl()\nrestaurants_pl = conn.sql('select * from restaurants').pl()\nretail_sales_pl = conn.sql('select * from retail_sales').pl()\nsales_pl = conn.sql('select * from sales').pl()\nsales_info_pl = conn.sql('select * from sales_info').pl()\nsets_pl = conn.sql('select * from sets').pl()\nwater_collection_pl = conn.sql('select * from water_collection').pl()"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#upload-dataframes-to-motherduck",
    "href": "portfolio2/posts/python/dw_motherduck.html#upload-dataframes-to-motherduck",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Upload dataframes to MotherDuck",
    "text": "Upload dataframes to MotherDuck\n\n\nCode\ndw.sql(f\"create or replace table airports as select * from airports_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table appl_stock as select * from appl_stock_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table cdmx_subway as select * from cdmx_subway_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table colors as select * from colors_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table contains_null as select * from contains_null_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table people as select * from people_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table prevalencia as select * from prevalencia_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table restaurants as select * from restaurants_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table retail_sales as select * from retail_sales_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sales as select * from sales_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sales_info as select * from sales_info_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sets as select * from sets_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table water_collection as select * from water_collection_pl\");"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#check-uploaded-tables",
    "href": "portfolio2/posts/python/dw_motherduck.html#check-uploaded-tables",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Check uploaded tables",
    "text": "Check uploaded tables\n\n\nCode\ndw.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       name       ‚îÇ\n‚îÇ     varchar      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ airports         ‚îÇ\n‚îÇ appl_stock       ‚îÇ\n‚îÇ cdmx_subway      ‚îÇ\n‚îÇ colors           ‚îÇ\n‚îÇ contains_null    ‚îÇ\n‚îÇ people           ‚îÇ\n‚îÇ prevalencia      ‚îÇ\n‚îÇ restaurants      ‚îÇ\n‚îÇ retail_sales     ‚îÇ\n‚îÇ sales            ‚îÇ\n‚îÇ sales_info       ‚îÇ\n‚îÇ sets             ‚îÇ\n‚îÇ water_collection ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     13 rows      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#close-all-database-connections",
    "href": "portfolio2/posts/python/dw_motherduck.html#close-all-database-connections",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Close all database connections",
    "text": "Close all database connections\n\n\nCode\n# close db connections\nconn.close()\nretail.close()\nrestaurants.close()\ndw.close()"
  },
  {
    "objectID": "portfolio2/posts/python/dw_motherduck.html#contact",
    "href": "portfolio2/posts/python/dw_motherduck.html#contact",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#beers-table",
    "href": "portfolio2/posts/python/duckdb_example.html#beers-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Beers table",
    "text": "Beers table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS beers (\n        CodC integer,\n        Package varchar(255),\n        Capacity float,\n        Stock integer\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO beers\n    VALUES (1, 'Botella', 0.2, 3600),\n        (2, 'Botella', 0.33, 1200),\n        (3, 'Lata', 0.33, 2400),\n        (4, 'Botella', 1, 288),\n        (5, 'Barril', 60, 30)\n''')\n\n\nRetrieve data\n\n\nShow code\n# Showing in polars dataframe\ndb.sql('SELECT * FROM beers').df()\n\n\n\n\n\n\n\n\n\nCodC\nPackage\nCapacity\nStock\n\n\n\n\n0\n1\nBotella\n0.20\n3600\n\n\n1\n2\nBotella\n0.33\n1200\n\n\n2\n3\nLata\n0.33\n2400\n\n\n3\n4\nBotella\n1.00\n288\n\n\n4\n5\nBarril\n60.00\n30"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#bars-table",
    "href": "portfolio2/posts/python/duckdb_example.html#bars-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Bars table",
    "text": "Bars table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS bars (\n        CodB integer,\n        Name varchar(255),\n        Cif varchar(255),\n        Location varchar(255)\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO bars\n    VALUES (1, 'Stop', '111111X', 'Villa Botijo'),\n        (2, 'Las Vegas', '222222Y', 'Villa Botijo'),\n        (3, 'Club Social', '', 'Las Ranas'),\n        (4, 'Otra Ronda', '333333Z', 'La Esponja')\n''')\n\n\nRetrieve data\n\n\nShow code\n# showing in pandas dataframe\ndb.sql('SELECT * FROM bars').df()\n\n\n\n\n\n\n\n\n\nCodB\nName\nCif\nLocation\n\n\n\n\n0\n1\nStop\n111111X\nVilla Botijo\n\n\n1\n2\nLas Vegas\n222222Y\nVilla Botijo\n\n\n2\n3\nClub Social\n\nLas Ranas\n\n\n3\n4\nOtra Ronda\n333333Z\nLa Esponja"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#employees-table",
    "href": "portfolio2/posts/python/duckdb_example.html#employees-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Employees table",
    "text": "Employees table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS employees (\n        CodE integer,\n        Name varchar(255),\n        Salary float\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO employees\n    VALUES (1, 'John Doe', 120000),\n        (2, 'Vicent Meren', 110000),\n        (3, 'Tom Simpson', 100000)\n''')\n\n\nRetrieve data\n\n\nShow code\ndb.sql('SELECT * FROM employees').df()\n\n\n\n\n\n\n\n\n\nCodE\nName\nSalary\n\n\n\n\n0\n1\nJohn Doe\n120000.0\n\n\n1\n2\nVicent Meren\n110000.0\n\n\n2\n3\nTom Simpson\n100000.0"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#delivery-table",
    "href": "portfolio2/posts/python/duckdb_example.html#delivery-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Delivery table",
    "text": "Delivery table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS delivery (\n        CodE integer,\n        CodB integer,\n        CodC integer,\n        Delivery_date date,\n        Quantity integer\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO delivery\n    VALUES (1, 1, 1, '2005-10-21', 240),\n        (1, 1, 2, '2005-10-21', 48),\n        (1, 2, 3, '2005-10-22', 60),\n        (1, 4, 5, '2005-10-22', 4),\n        (2, 2, 3, '2005-10-23', 48),\n        (2, 2, 5, '2005-10-23', 2),\n        (2, 4, 1, '2005-10-24', 480),\n        (2, 4, 2, '2005-10-24', 72),\n        (3, 3, 3, '2005-10-24', 48),\n        (3, 3, 4, '2005-10-25', 20)\n''')\n\n\nRetrieve data\n\n\nShow code\ndb.sql('SELECT * FROM delivery').df()\n\n\n\n\n\n\n\n\n\nCodE\nCodB\nCodC\nDelivery_date\nQuantity\n\n\n\n\n0\n1\n1\n1\n2005-10-21\n240\n\n\n1\n1\n1\n2\n2005-10-21\n48\n\n\n2\n1\n2\n3\n2005-10-22\n60\n\n\n3\n1\n4\n5\n2005-10-22\n4\n\n\n4\n2\n2\n3\n2005-10-23\n48\n\n\n5\n2\n2\n5\n2005-10-23\n2\n\n\n6\n2\n4\n1\n2005-10-24\n480\n\n\n7\n2\n4\n2\n2005-10-24\n72\n\n\n8\n3\n3\n3\n2005-10-24\n48\n\n\n9\n3\n3\n4\n2005-10-25\n20"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-1",
    "href": "portfolio2/posts/python/duckdb_example.html#query-1",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 1",
    "text": "Query 1\nObtain the name of the employees who delivered to the Stop bar during the week of October 17 to 23, 2005.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT r.Delivery_date\n        ,e.Name\n        ,b.Name\n    FROM delivery AS r\n    LEFT JOIN employees AS e ON r.CodE = e.CodE\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    WHERE b.Name = 'Stop'\n        AND r.Delivery_date BETWEEN '2005-10-17' AND '2005-10-23'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Delivery_date ‚îÇ   Name   ‚îÇ  Name   ‚îÇ\n‚îÇ     date      ‚îÇ varchar  ‚îÇ varchar ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 2005-10-21    ‚îÇ John Doe ‚îÇ Stop    ‚îÇ\n‚îÇ 2005-10-21    ‚îÇ John Doe ‚îÇ Stop    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-2",
    "href": "portfolio2/posts/python/duckdb_example.html#query-2",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 2",
    "text": "Query 2\nObtain the CIF and name of the bars to which bottle-type beer with a capacity of less than 1 liter has been distributed, ordered by location.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Cif\n        ,b.Name\n        ,c.Package\n        ,c.Capacity\n        ,b.Location\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Package = 'Botella'\n        AND c.Capacity &lt; 1.0\n    ORDER BY b.Location\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Cif   ‚îÇ    Name    ‚îÇ Package ‚îÇ Capacity ‚îÇ   Location   ‚îÇ\n‚îÇ varchar ‚îÇ  varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ   varchar    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 333333Z ‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ      0.2 ‚îÇ La Esponja   ‚îÇ\n‚îÇ 333333Z ‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ     0.33 ‚îÇ La Esponja   ‚îÇ\n‚îÇ 111111X ‚îÇ Stop       ‚îÇ Botella ‚îÇ      0.2 ‚îÇ Villa Botijo ‚îÇ\n‚îÇ 111111X ‚îÇ Stop       ‚îÇ Botella ‚îÇ     0.33 ‚îÇ Villa Botijo ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-3",
    "href": "portfolio2/posts/python/duckdb_example.html#query-3",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 3",
    "text": "Query 3\nObtain the deliveries (name of the bar, container and capacity of the drink, date and quantity) made by Prudencio Caminero.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n        ,r.Delivery_date\n        ,r.Quantity\n        ,e.Name\n        ,e.CodE\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    LEFT JOIN employees AS e ON e.CodE = r.CodE\n    WHERE e.Name ILIKE '%doe%'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name    ‚îÇ Package ‚îÇ Capacity ‚îÇ Delivery_date ‚îÇ Quantity ‚îÇ   Name   ‚îÇ CodE  ‚îÇ\n‚îÇ  varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ     date      ‚îÇ  int32   ‚îÇ varchar  ‚îÇ int32 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Stop       ‚îÇ Botella ‚îÇ      0.2 ‚îÇ 2005-10-21    ‚îÇ      240 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îÇ Stop       ‚îÇ Botella ‚îÇ     0.33 ‚îÇ 2005-10-21    ‚îÇ       48 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îÇ Las Vegas  ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ 2005-10-22    ‚îÇ       60 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îÇ Otra Ronda ‚îÇ Barril  ‚îÇ     60.0 ‚îÇ 2005-10-22    ‚îÇ        4 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-4",
    "href": "portfolio2/posts/python/duckdb_example.html#query-4",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 4",
    "text": "Query 4\nObtain the bars to which bottle-type containers with a capacity of 0.2 or 0.33 have been distributed.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Package = 'Botella'\n        AND c.Capacity IN (0.2, 0.33)\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name    ‚îÇ Package ‚îÇ Capacity ‚îÇ\n‚îÇ  varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Stop       ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Stop       ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-5",
    "href": "portfolio2/posts/python/duckdb_example.html#query-5",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 5",
    "text": "Query 5\nName of the employees who have distributed bottled beers to the ‚ÄúStop‚Äù and ‚ÄúLas Vegas‚Äù bars.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT e.Name\n        ,b.Name\n        ,c.Package\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    LEFT JOIN employees AS e On e.CodE = r.CodE\n    WHERE b.Name IN ('Stop', 'Las Vegas')\n        AND c.Package = 'Botella'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Name   ‚îÇ  Name   ‚îÇ Package ‚îÇ\n‚îÇ varchar  ‚îÇ varchar ‚îÇ varchar ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ John Doe ‚îÇ Stop    ‚îÇ Botella ‚îÇ\n‚îÇ John Doe ‚îÇ Stop    ‚îÇ Botella ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-6",
    "href": "portfolio2/posts/python/duckdb_example.html#query-6",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 6",
    "text": "Query 6\nObtain the name and number of trips that each employee has made outside of Villa Botijo.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT e.Name\n        ,COUNT(b.Location) AS Travles\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN employees AS e On e.CodE = r.CodE\n    WHERE b.Location &lt;&gt; 'Villa Botijo'\n    GROUP BY 1\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     Name     ‚îÇ Travles ‚îÇ\n‚îÇ   varchar    ‚îÇ  int64  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ John Doe     ‚îÇ       1 ‚îÇ\n‚îÇ Tom Simpson  ‚îÇ       2 ‚îÇ\n‚îÇ Vicent Meren ‚îÇ       2 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-7",
    "href": "portfolio2/posts/python/duckdb_example.html#query-7",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 7",
    "text": "Query 7\nObtain the name and location of the bar that has purchased the most liters of beer.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,b.Location\n        ,MAX(r.Quantity) AS Liters\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    GROUP BY 1, 2\n    ORDER BY 3 DESC\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name     ‚îÇ   Location   ‚îÇ Liters ‚îÇ\n‚îÇ   varchar   ‚îÇ   varchar    ‚îÇ int32  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Otra Ronda  ‚îÇ La Esponja   ‚îÇ    480 ‚îÇ\n‚îÇ Stop        ‚îÇ Villa Botijo ‚îÇ    240 ‚îÇ\n‚îÇ Las Vegas   ‚îÇ Villa Botijo ‚îÇ     60 ‚îÇ\n‚îÇ Club Social ‚îÇ Las Ranas    ‚îÇ     48 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-8",
    "href": "portfolio2/posts/python/duckdb_example.html#query-8",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 8",
    "text": "Query 8\nObtain bars that have purchased all types of beer with bottle packaging and a capacity less than 1 liter.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Capacity IN (SELECT c.Capacity\n                            FROM beers AS c\n                            WHERE c.Package = 'Botella'\n                                AND c.Capacity &lt; 1.0)\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name     ‚îÇ Package ‚îÇ Capacity ‚îÇ\n‚îÇ   varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Stop        ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Stop        ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îÇ Las Vegas   ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ\n‚îÇ Las Vegas   ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ\n‚îÇ Otra Ronda  ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Otra Ronda  ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îÇ Club Social ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-9",
    "href": "portfolio2/posts/python/duckdb_example.html#query-9",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 9",
    "text": "Query 9\nRaise the salary of the employee who has worked the most days by 5%.\n\n\nShow code\n# modify database records\ndb.sql(\n    '''\n    -- Raise 5% salary of workers\n    INSERT INTO employees (CodE, Name, Salary)\n    VALUES (1, 'John Doe', 120000*1.05),\n            (2, 'Vicent Meren', 110000*1.05)\n    '''\n)\n\n\n\n\nShow code\n# verify inserted record\ndb.sql(\n    '''\n    SELECT e.*\n    FROM employees AS e\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ CodE  ‚îÇ     Name     ‚îÇ  Salary  ‚îÇ\n‚îÇ int32 ‚îÇ   varchar    ‚îÇ  float   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     1 ‚îÇ John Doe     ‚îÇ 120000.0 ‚îÇ\n‚îÇ     2 ‚îÇ Vicent Meren ‚îÇ 110000.0 ‚îÇ\n‚îÇ     3 ‚îÇ Tom Simpson  ‚îÇ 100000.0 ‚îÇ\n‚îÇ     1 ‚îÇ John Doe     ‚îÇ 126000.0 ‚îÇ\n‚îÇ     2 ‚îÇ Vicent Meren ‚îÇ 115500.0 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/duckdb_example.html#query-10",
    "href": "portfolio2/posts/python/duckdb_example.html#query-10",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 10",
    "text": "Query 10\nInsert a new distribution from the employee ‚ÄúVicent Meren‚Äù to the ‚ÄúStop‚Äù bar of 48 canned beers on 2005-10-26.\n\n\nShow code\n# Insert new record\ndb.sql(\n    '''\n    INSERT INTO delivery (CodE, CodB, CodC, Delivery_date, Quantity)\n    VALUES (2, 1, 3, '2005-10-26', 48)\n    '''\n)\n\n\n\n\nShow code\n# verify inserted record\ndb.sql(\n    '''\n    SELECT * \n    FROM delivery\n    WHERE Delivery_date = '2005-10-26'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ CodE  ‚îÇ CodB  ‚îÇ CodC  ‚îÇ Delivery_date ‚îÇ Quantity ‚îÇ\n‚îÇ int32 ‚îÇ int32 ‚îÇ int32 ‚îÇ     date      ‚îÇ  int32   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     2 ‚îÇ     1 ‚îÇ     3 ‚îÇ 2005-10-26    ‚îÇ       48 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio2/posts/python/water collection system.html",
    "href": "portfolio2/posts/python/water collection system.html",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "",
    "text": "Rainwater harvesting systems are a prominent water collection method in Mexico, especially in areas facing water scarcity.\nThese systems, called ‚ÄúSistemas de Captaci√≥n de Agua de Lluvia‚Äù (SCALL) offer several benefits:"
  },
  {
    "objectID": "portfolio2/posts/python/water collection system.html#contact",
    "href": "portfolio2/posts/python/water collection system.html#contact",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/dashboards.html",
    "href": "portfolio2/posts/python/dashboards.html",
    "title": "Creating Dashboards with Python",
    "section": "",
    "text": "Customization\n\nUnlike pre-built tools, Python allows you to tailor dashboards to your specific needs. Present the data exactly how you want, focusing on the metrics that matter most.\n\nInteractivity\n\nPython lets you create dynamic dashboards where users can explore data, filter information, and drill down for deeper analysis. This fosters engagement and a more intuitive understanding of the data.\n\nPython‚Äôs Ecosystem\n\nPython boasts a rich landscape of libraries like Plotly, Altair and Pyshiny. These libraries provide a plethora of visualization options, enabling you to create clear, compelling, and informative charts, graphs, and other visual elements.\n\nEfficiency\n\nPython streamlines the process of data manipulation and analysis. Automate repetitive tasks and integrate data from various sources, saving you valuable time and effort.\n\nOpen-Source\n\nBeing open-source, Python offers significant cost savings compared to proprietary dashboarding tools. Plus, the large and active Python community provides ample resources and support to help you on your journey.\n\nSharability\n\nPython-built dashboards are readily shareable across your organization. Disseminate valuable insights to colleagues and stakeholders, fostering better collaboration and data-driven decision-making.\nBy leveraging Python for dashboard creation, you gain control, flexibility, and a powerful toolset to unlock the true potential of your data. Get ready to transform information into clear, actionable insights that drive informed decisions and positive outcomes.\n\n\n\n\n\n\n\n\nInfo\n\n\n\nYou can interact with the dashboard herein or you can click here to open in a new page."
  },
  {
    "objectID": "portfolio2/posts/python/dashboards.html#contact",
    "href": "portfolio2/posts/python/dashboards.html#contact",
    "title": "Creating Dashboards with Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/polynomial_regression.html",
    "href": "portfolio2/posts/python/polynomial_regression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Oftentimes we‚Äôll encounter data where the relationship between the feature(s) and the response variable can‚Äôt be best described with a straight line. In these cases, we should use polynomial regression.\nAn example of a polynomial, coud be:\n3x4‚Äì7x3+2x2+113x^4 ‚Äì 7x^3 + 2x^2 + 11\nTerminology\n\nDegree of a polynomial: The highest power in polynomial. In our example, 4\nCoefficient: Each constant (3, 7, 2, 11) in polynomial is a coefficient. In polynomial regression, these coefficients will be estimated\nLeading term: The term with the highest power (3x43x^4). It determines the polynomial‚Äôs graph behavior\nLeading coefficient: The coefficient of the leading term (3)\nConstant term: The y intercept, it never changes: no matter what the value of x is, the constant term remains the same\n\n\n\nLet‚Äôs return to $ 3x4 - 7x3 + 2x2 + 11 $, if we write a polynomial‚Äôs terms from the highest degree term to the lowest degree term, it‚Äôs called a polynomial‚Äôs standard form.\nIn the context of machine learning, you‚Äôll often see it reversed:\ny=Œ≤0+Œ≤1x+Œ≤2x2+‚Ä¶+Œ≤nxny = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_nx^n\nwhere:\n\ny is the response variable we want to predict\nx is the feature\nŒ≤0\\beta_0 is the y intercept\n\nThe other √üs are the coefficients/parameters we‚Äôd like to find when we train our model on the available x and y values\n\nn is the degree of the polynomial (the higher n is, the more complex curved lines you can create)\nThe above polynomial regression formula is very similar to the multiple linear regression formula:\n\ny=Œ≤0+Œ≤1x+Œ≤2x+‚Ä¶+Œ≤nxy = \\beta_0 + \\beta_1x + \\beta_2x + \\dots + \\beta_nx\nIt‚Äôs not a coincidence: polynomial regression is a linear model used for describing non-linear relationships\nHow is this possible? The magic lies in creating new features by raising the original features to a power\nLinear regression is just a first-degree polynomial. Polynomial regression uses higher-degree polynomials. Both of them are linear models, but the first results in a straight line, the latter gives you a curved line.\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\n\nCode\n# create dummy dataset\nx = np.arange(0, 30)\ny = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75,\n     88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]\n\n\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create an polynomial instance\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n\nDegree = 2 means that we want to work with a 2nd degree polynomial,\ny=Œ≤0+Œ≤1x+Œ≤2x2y = \\beta_0 + \\beta_1x + \\beta_2x^2\n\n\nCode\npoly_features = poly.fit_transform(x.reshape(-1, 1))\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n\nIt may seem confusing, why are we importing LinearRegression module? üòÆ\nWe just have to remind that polynomial regression is a linear model, that‚Äôs why we import LinearRegression. üôÇ\n\n\nCode\n# Create a LinearRegression() instance\npoly_reg_model = LinearRegression()\n\n\n\n\nCode\n# Fit model to data\npoly_reg_model.fit(poly_features, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Predict responses\ny_predicted = poly_reg_model.predict(poly_features)\n\n\n\n\nCode\ny_predicted\n\n\narray([  1.70806452,   3.04187987,   4.70292388,   6.69119657,\n         9.00669792,  11.64942794,  14.61938662,  17.91657397,\n        21.54098999,  25.49263467,  29.77150802,  34.37761004,\n        39.31094073,  44.57150008,  50.1592881 ,  56.07430478,\n        62.31655014,  68.88602415,  75.78272684,  83.00665819,\n        90.55781821,  98.4362069 , 106.64182425, 115.17467027,\n       124.03474495, 133.22204831, 142.73658033, 152.57834101,\n       162.74733037, 173.24354839])\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"A Basic Polynomial Regression Example\", size=16)\nplt.scatter(x, y)\nplt.plot(x, y_predicted, c=\"green\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create data\nnp.random.seed(1)\nx_1 = np.absolute(np.random.randn(100, 1) * 10)\nx_2 = np.absolute(np.random.randn(100, 1) * 30)\ny = 2*x_1**2 + 3*x_1 + 2 + np.random.randn(100, 1)*20\n\n\n\n\nCode\n# Create visual\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes[0].scatter(x_1, y)\naxes[1].scatter(x_2, y)\naxes[0].set_title(\"x_1 plotted\")\naxes[1].set_title(\"x_2 plotted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create dataframe\ndf = pd.DataFrame(\n    {\"x_1\":x_1.reshape(100,),\n     \"x_2\":x_2.reshape(100,),\n     \"y\":y.reshape(100,)},\n        index=range(0,100))\ndf\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      y\n    \n  \n  \n    \n      0\n      16.243454\n      13.413857\n      570.412369\n    \n    \n      1\n      6.117564\n      36.735231\n      111.681987\n    \n    \n      2\n      5.281718\n      12.104749\n      62.392124\n    \n    \n      3\n      10.729686\n      17.807356\n      303.538953\n    \n    \n      4\n      8.654076\n      32.847355\n      151.109269\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.773401\n      48.823150\n      -0.430738\n    \n    \n      96\n      3.438537\n      18.069578\n      44.308720\n    \n    \n      97\n      0.435969\n      12.608466\n      19.383456\n    \n    \n      98\n      6.200008\n      24.328550\n      78.371729\n    \n    \n      99\n      6.980320\n      31.333263\n      132.108914\n    \n  \n\n100 rows √ó 3 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Define train and test sets\nX, y = df[[\"x_1\", \"x_2\"]], df[\"y\"]\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Create polynomial regression\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Test model\npoly_reg_y_predicted = poly_reg_model.predict(X_test)\n\n\nThe formula for Root Mean Square Error is:\nRMSE=1n‚àëi=1n(yi‚àíyÃÇi)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\nThe smaller the RMSE metric the better the model\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\npoly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))\nprint(f'Polynomial regression\\nRMSE: {poly_reg_rmse:.2f}')\n\n\nPolynomial regression\nRMSE: 20.94\n\n\n\n\n\n\nCode\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Create linear regression instance\nlin_reg_model = LinearRegression()\n# Fit regression model\nlin_reg_model.fit(X_train, y_train)\n# Create predictions\nlin_reg_y_predicted = lin_reg_model.predict(X_test)\n# Calculate RMSE\nlin_reg_rmse = np.sqrt(mean_squared_error(y_test, lin_reg_y_predicted))\n# Print results\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:,.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nIn the train_test_split method we use X instead of poly_features, and it‚Äôs for a good reason.\nX contains our two original features (x_1 and x_2), so our linear regression model takes the form of:\ny=Œ≤0+Œ≤1x1+Œ≤2x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\n\nCode\nlin_reg_model.coef_\n\n\narray([43.73176255, -0.53140809])\n\n\n\n\nCode\nlin_reg_model.intercept_\n\n\nnp.float64(-117.07280081594811)\n\n\nOn the other hand, poly_features contains new features as well, created out of x_1 and x_2, so our polynomial regression model (based on a 2nd degree polynomial with two features) looks like this:\ny=Œ≤0+Œ≤1x1+Œ≤2x2+Œ≤3x12+Œ≤4x22+Œ≤5x1x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_2^2 + \\beta_5x_1x_2\nThis is because poly.fit_transform(X) added three new features to the original two (x1 (x1x_1) and x2 (x2x_2)): x12x_1^2, x22x_2^2 and x1x2x_1x_2\nx12x_1^2 and x22x_2^2 need no explanation, as we‚Äôve already covered how they are created in the ‚ÄúCoding a polynomial regression model with scikit-learn‚Äù section.\nWhat‚Äôs more interesting is x1x2x_1x_2 ‚Äì when two features are multiplied by each other, it‚Äôs called an interaction term. An interaction term accounts for the fact that one variable‚Äôs value may depend on another variable‚Äôs value (more on this here). poly.fit_transform() automatically created this interaction term for us, isn‚Äôt that cool? üôÇ\n\n\nCode\npoly_reg_model.coef_\n\n\narray([ 3.61945509, -1.0859955 ,  1.89905813,  0.0207338 ,  0.01300394])\n\n\n\n\nCode\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nThe RMSE for the polynomial regression model is 20.94, while the RMSE for the linear regression model is 62.3. The polynomial regression model performs almost 3 times better than the linear regression model!\n\n\n\n\nIn this notebook, we have been able to expose that a basic understanding of polynomial regression. And we have shown RMSE metric for comparing the performance among different ML models.\nWe used a 2nd degree polynomial for ourthis example. Naturally, we should always test and trial before deploying a model to find what degree of polynomial performs best.\n\n\n\n\nUjhelyi T. Polynomial regression I mainly based on.\nHow to become a Data Scientist\n\n\n\n\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/polynomial_regression.html#environment-settings",
    "href": "portfolio2/posts/python/polynomial_regression.html#environment-settings",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\n\nCode\n# create dummy dataset\nx = np.arange(0, 30)\ny = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75,\n     88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]\n\n\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create an polynomial instance\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n\nDegree = 2 means that we want to work with a 2nd degree polynomial,\ny=Œ≤0+Œ≤1x+Œ≤2x2y = \\beta_0 + \\beta_1x + \\beta_2x^2\n\n\nCode\npoly_features = poly.fit_transform(x.reshape(-1, 1))\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n\nIt may seem confusing, why are we importing LinearRegression module? üòÆ\nWe just have to remind that polynomial regression is a linear model, that‚Äôs why we import LinearRegression. üôÇ\n\n\nCode\n# Create a LinearRegression() instance\npoly_reg_model = LinearRegression()\n\n\n\n\nCode\n# Fit model to data\npoly_reg_model.fit(poly_features, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Predict responses\ny_predicted = poly_reg_model.predict(poly_features)\n\n\n\n\nCode\ny_predicted\n\n\narray([  1.70806452,   3.04187987,   4.70292388,   6.69119657,\n         9.00669792,  11.64942794,  14.61938662,  17.91657397,\n        21.54098999,  25.49263467,  29.77150802,  34.37761004,\n        39.31094073,  44.57150008,  50.1592881 ,  56.07430478,\n        62.31655014,  68.88602415,  75.78272684,  83.00665819,\n        90.55781821,  98.4362069 , 106.64182425, 115.17467027,\n       124.03474495, 133.22204831, 142.73658033, 152.57834101,\n       162.74733037, 173.24354839])\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"A Basic Polynomial Regression Example\", size=16)\nplt.scatter(x, y)\nplt.plot(x, y_predicted, c=\"green\")\nplt.show()"
  },
  {
    "objectID": "portfolio2/posts/python/polynomial_regression.html#polynomial-regression-with-multiple-features",
    "href": "portfolio2/posts/python/polynomial_regression.html#polynomial-regression-with-multiple-features",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Code\n# Create data\nnp.random.seed(1)\nx_1 = np.absolute(np.random.randn(100, 1) * 10)\nx_2 = np.absolute(np.random.randn(100, 1) * 30)\ny = 2*x_1**2 + 3*x_1 + 2 + np.random.randn(100, 1)*20\n\n\n\n\nCode\n# Create visual\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes[0].scatter(x_1, y)\naxes[1].scatter(x_2, y)\naxes[0].set_title(\"x_1 plotted\")\naxes[1].set_title(\"x_2 plotted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create dataframe\ndf = pd.DataFrame(\n    {\"x_1\":x_1.reshape(100,),\n     \"x_2\":x_2.reshape(100,),\n     \"y\":y.reshape(100,)},\n        index=range(0,100))\ndf\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      y\n    \n  \n  \n    \n      0\n      16.243454\n      13.413857\n      570.412369\n    \n    \n      1\n      6.117564\n      36.735231\n      111.681987\n    \n    \n      2\n      5.281718\n      12.104749\n      62.392124\n    \n    \n      3\n      10.729686\n      17.807356\n      303.538953\n    \n    \n      4\n      8.654076\n      32.847355\n      151.109269\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.773401\n      48.823150\n      -0.430738\n    \n    \n      96\n      3.438537\n      18.069578\n      44.308720\n    \n    \n      97\n      0.435969\n      12.608466\n      19.383456\n    \n    \n      98\n      6.200008\n      24.328550\n      78.371729\n    \n    \n      99\n      6.980320\n      31.333263\n      132.108914\n    \n  \n\n100 rows √ó 3 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Define train and test sets\nX, y = df[[\"x_1\", \"x_2\"]], df[\"y\"]\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Create polynomial regression\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Test model\npoly_reg_y_predicted = poly_reg_model.predict(X_test)\n\n\nThe formula for Root Mean Square Error is:\nRMSE=1n‚àëi=1n(yi‚àíyÃÇi)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\nThe smaller the RMSE metric the better the model\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\npoly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))\nprint(f'Polynomial regression\\nRMSE: {poly_reg_rmse:.2f}')\n\n\nPolynomial regression\nRMSE: 20.94\n\n\n\n\n\n\nCode\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Create linear regression instance\nlin_reg_model = LinearRegression()\n# Fit regression model\nlin_reg_model.fit(X_train, y_train)\n# Create predictions\nlin_reg_y_predicted = lin_reg_model.predict(X_test)\n# Calculate RMSE\nlin_reg_rmse = np.sqrt(mean_squared_error(y_test, lin_reg_y_predicted))\n# Print results\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:,.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nIn the train_test_split method we use X instead of poly_features, and it‚Äôs for a good reason.\nX contains our two original features (x_1 and x_2), so our linear regression model takes the form of:\ny=Œ≤0+Œ≤1x1+Œ≤2x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\n\nCode\nlin_reg_model.coef_\n\n\narray([43.73176255, -0.53140809])\n\n\n\n\nCode\nlin_reg_model.intercept_\n\n\nnp.float64(-117.07280081594811)\n\n\nOn the other hand, poly_features contains new features as well, created out of x_1 and x_2, so our polynomial regression model (based on a 2nd degree polynomial with two features) looks like this:\ny=Œ≤0+Œ≤1x1+Œ≤2x2+Œ≤3x12+Œ≤4x22+Œ≤5x1x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_2^2 + \\beta_5x_1x_2\nThis is because poly.fit_transform(X) added three new features to the original two (x1 (x1x_1) and x2 (x2x_2)): x12x_1^2, x22x_2^2 and x1x2x_1x_2\nx12x_1^2 and x22x_2^2 need no explanation, as we‚Äôve already covered how they are created in the ‚ÄúCoding a polynomial regression model with scikit-learn‚Äù section.\nWhat‚Äôs more interesting is x1x2x_1x_2 ‚Äì when two features are multiplied by each other, it‚Äôs called an interaction term. An interaction term accounts for the fact that one variable‚Äôs value may depend on another variable‚Äôs value (more on this here). poly.fit_transform() automatically created this interaction term for us, isn‚Äôt that cool? üôÇ\n\n\nCode\npoly_reg_model.coef_\n\n\narray([ 3.61945509, -1.0859955 ,  1.89905813,  0.0207338 ,  0.01300394])\n\n\n\n\nCode\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nThe RMSE for the polynomial regression model is 20.94, while the RMSE for the linear regression model is 62.3. The polynomial regression model performs almost 3 times better than the linear regression model!"
  },
  {
    "objectID": "portfolio2/posts/python/polynomial_regression.html#conclusion",
    "href": "portfolio2/posts/python/polynomial_regression.html#conclusion",
    "title": "Polynomial Regression",
    "section": "",
    "text": "In this notebook, we have been able to expose that a basic understanding of polynomial regression. And we have shown RMSE metric for comparing the performance among different ML models.\nWe used a 2nd degree polynomial for ourthis example. Naturally, we should always test and trial before deploying a model to find what degree of polynomial performs best."
  },
  {
    "objectID": "portfolio2/posts/python/polynomial_regression.html#references",
    "href": "portfolio2/posts/python/polynomial_regression.html#references",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Ujhelyi T. Polynomial regression I mainly based on.\nHow to become a Data Scientist"
  },
  {
    "objectID": "portfolio2/posts/python/polynomial_regression.html#contact",
    "href": "portfolio2/posts/python/polynomial_regression.html#contact",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Jesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html",
    "href": "portfolio2/posts/python/mermaid-diagrams.html",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "",
    "text": "Figure¬†1: Mermaid for diagramming"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Data Science Cycle Diagram using Mermaid",
    "text": "Data Science Cycle Diagram using Mermaid\n\n\nCode\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D\n\n\n\n\n\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#flow-process",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Flow Process",
    "text": "Flow Process\n\n\nCode\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\n\n\nAdding colors\n\n\nCode\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#complex-flow-process",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#complex-flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Complex Flow Process",
    "text": "Complex Flow Process\ngraph\n    A[ItemA]--&gt; ItemB\n    A--&gt; ItemC\n    subgraph ItemC\n        D[ItemD]--&gt;E[ItemE]\n        E--&gt;D\n    end\n\n    X[ItemX]==&gt;Decision\n    click X \"https://mermaid.js.org/\"\n    Decision{Item Y?}==&gt;|Yes| Y[ItemY]\n    Decision==&gt;|No| Z[ItemZ]"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#pie-charts",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#pie-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Pie charts",
    "text": "Pie charts\npie title Pie chart 2\n    \"Category A\" : 2000\n    \"Category B\" : 500\n    \"Category C\" : 1000"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#gantt-chart",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#gantt-chart",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Gantt Chart",
    "text": "Gantt Chart\ngantt \n    title Gantt chart 1\n    dateFormat YYYY-MM-DD\n    axisFormat %b %d\n    section Section A\n        Task A1: a1, 2024-04-15, 7d\n        Task A2: after a1, 5d\n    section Section B\n        Task B1: 2024-04-22, 2024-05-02\n        Task B2: 2024-04-29, 5d"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#journey-charts",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#journey-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Journey Charts",
    "text": "Journey Charts\njourney\n    title Journey 1\n    section Section A\n        Activity 1A: 5: Person1\n        Activity 2A: 3: Person2\n        Activity 3A: 2: Person1, Person2\n    section Section B\n        Activity 1B: 4: Person2\n        Activity 1B: 5: Person1"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#state-diagram",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#state-diagram",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "State Diagram",
    "text": "State Diagram\nstateDiagram\n    [*] --&gt; StateA\n    StateA --&gt; [*]\n    StateA --&gt; StateB\n    StateB --&gt; StateA\n    StateB --&gt; StateD\n    StateD --&gt; [*]"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#items-diagrams",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#items-diagrams",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Items diagrams",
    "text": "Items diagrams\nerDiagram\n    Item1 ||--o{ Item2: text1\n    Item2 ||--|{ Item3: text2\n    Item1 }|..|{ Item4: text3\n    Item4 }o--o{ Item4: text3"
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#conclusions",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#conclusions",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Conclusions",
    "text": "Conclusions\nMermaid is a popular markdown-like syntax for generating diagrams and charts using Markdown. Think of it as a way to describe your diagrams in a human-readable format, which Mermaid then renders into a visual representation.\nMermaid Python acts as a bridge, allowing you to generate these Mermaid diagrams within your Python environment and even embed them in your Jupyter notebooks or web applications.\nMermaid Python also allows you to customize the appearance of your diagrams, add styling, and even integrate with other Python libraries. You can dynamically generate diagrams based on your data, making it a powerful tool for data visualization and exploration."
  },
  {
    "objectID": "portfolio2/posts/python/mermaid-diagrams.html#contact",
    "href": "portfolio2/posts/python/mermaid-diagrams.html#contact",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html",
    "href": "portfolio2/posts/python/restaurants_sql.html",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "",
    "text": "Figure¬†1: Restaurants Report"
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#project-phases",
    "href": "portfolio2/posts/python/restaurants_sql.html#project-phases",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Project phases",
    "text": "Project phases\n\nData Loading\n\nDuckDB: Load large datasets directly into DuckDB for efficient in-memory operations and query execution.\nPolars: Utilize Polars‚Äô fast data loading capabilities, especially for CSV and Parquet files, to quickly ingest data into its DataFrame structure.\n\nData Transformation\n\nPolars: Perform data cleaning, filtering, aggregation, and other transformations within the Polars DataFrame efficiently.\nDuckDB: Execute complex SQL queries directly on the in-memory data within DuckDB for advanced data manipulation.\n\nData Visualization\n\nPolars: Use Polars‚Äô built-in plotting capabilities for quick exploratory visualizations.\nPlotly: Leverage Plotly‚Äôs extensive library of interactive and customizable plots for in-depth analysis and presentation.\n\nPerformance Optimization\n\nMinimize Data Transfers: Avoid unnecessary data transfers between tools. For example, if possible, perform data transformations within DuckDB and then directly visualize results using Plotly.\nUtilize Parallel Processing: Leverage the parallel processing capabilities of both DuckDB and Polars to speed up data processing tasks.\nOptimize Queries: Write efficient SQL queries and use appropriate data types to maximize DuckDB‚Äôs performance.\nCaching: Cache intermediate results to avoid redundant computations."
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#environment-settings",
    "href": "portfolio2/posts/python/restaurants_sql.html#environment-settings",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nCode\nimport numpy as np\nimport polars as pl\nimport random\nimport duckdb as db\nimport plotly.express as px"
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#create-dummy-dataset",
    "href": "portfolio2/posts/python/restaurants_sql.html#create-dummy-dataset",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Create dummy dataset",
    "text": "Create dummy dataset\n\n\nCode\ndef generate_dummy_data(num_rows=10_000):\n  \"\"\"\n  Generates dummy data for restaurants with name, rating_count, cost, city, and cuisine.\n  Args:\n    num_rows: Number of rows to generate.\n  Returns:\n    A list of dictionaries, where each dictionary represents a restaurant with the specified fields.\n  \"\"\"\n\n  data = []\n  names = [\"The Cozy Nook\", \"Spice & Bloom\", \"The Golden Wok\", \"Midnight Diner\", \"Ocean Breeze\", \n           \"Cafe Delight\", \"The Burger Joint\", \"Pizza Palace\", \"Taste of Italy\", \"French Delights\",\n           \"The Curry House\", \"Sushi Corner\", \"Greek Gyros\", \"Taco Town\", \"The BBQ Shack\"]\n  cities = [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Rome\", \n            \"Berlin\", \"Sydney\", \"Madrid\", \"Amsterdam\", \"Lisbon\"]\n  cuisines = [\"Italian\", \"Mexican\", \"Indian\", \"Chinese\", \"Japanese\", \n              \"American\", \"French\", \"Thai\", \"Greek\", \"Spanish\"]\n\n  for _ in range(num_rows):\n    restaurant = {\n        'name': random.choice(names),\n        'rating_count': random.randint(100, 5000),\n        'cost': random.uniform(10.0, 100.0),\n        'city': random.choice(cities),\n        'cuisine': random.choice(cuisines),\n        'rating': random.randint(0, 5)\n    }\n    data.append(restaurant)\n\n  return data\n\n\n\n\nCode\n# Generate rows of dummy data\ndummy_restaurants = generate_dummy_data()"
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#create-dataframe",
    "href": "portfolio2/posts/python/restaurants_sql.html#create-dataframe",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Create dataframe",
    "text": "Create dataframe\n\n\nCode\n# Create dataframe\nrestaurants = pl.DataFrame(dummy_restaurants)\n\n\n\n\nCode\n# Show dataframe\n(\n    restaurants\n        .to_pandas()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n3,408\n$37.75\nLisbon\nFrench\n2\n\n\nGreek Gyros\n1,484\n$93.44\nParis\nItalian\n0\n\n\nOcean Breeze\n2,792\n$32.26\nBerlin\nMexican\n5\n\n\nGreek Gyros\n2,038\n$20.78\nLondon\nSpanish\n5\n\n\nThe Curry House\n2,321\n$94.79\nLisbon\nFrench\n1\n\n\n\n\n\n\nTable¬†1: Restaurants dataset"
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#connect-to-database",
    "href": "portfolio2/posts/python/restaurants_sql.html#connect-to-database",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Connect to database",
    "text": "Connect to database\n\n\nCode\n# connect to database\nconn = db.connect('my_database.db')\n#conn = db.connect('restaurants.db')\n\n\n\n\nCode\n# retrieve data from table\nres = conn.sql('select * from restaurants limit 5')\n\n\n\n\nCode\n# Show table\n(\n    res.df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nGreek Gyros\n770\n$68.39\nNew York\nFrench\n1\n\n\nTaste of Italy\n4,420\n$88.23\nAmsterdam\nChinese\n0\n\n\nMidnight Diner\n2,155\n$12.97\nLisbon\nMexican\n1\n\n\nTaste of Italy\n3,375\n$52.79\nSydney\nChinese\n1\n\n\n\n\n\n\nTable¬†2: Restaurants table from database"
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#queries",
    "href": "portfolio2/posts/python/restaurants_sql.html#queries",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Queries",
    "text": "Queries\n\nWhich restaurant of London is visited by the least number of people?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where city = 'London' and rating_count = (select min(rating_count)\n                                            from restaurants \n                                            where city = 'London')\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nGreek Gyros\n101\n$13.82\nLondon\nIndian\n2\n\n\n\n\n\n\nTable¬†3: Query 1 result table\n\n\n\n\n\n\nWhich restaurant has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where cost*rating_count = (select max(cost*rating_count)\n                                from restaurants)\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nOcean Breeze\n4,969\n$99.92\nMadrid\nChinese\n4\n\n\n\n\n\n\nTable¬†4: Query 2 result table\n\n\n\n\n\n\nHow many restaurants are having a rating more than the average rating?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where rating &gt; (select avg(rating)\n                    from restaurants)\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nThe Burger Joint\n3,631\n$25.36\nLondon\nAmerican\n4\n\n\nThe Curry House\n3,033\n$56.05\nAmsterdam\nItalian\n3\n\n\nThe Burger Joint\n4,656\n$19.38\nAmsterdam\nAmerican\n3\n\n\nPizza Palace\n1,862\n$71.95\nParis\nJapanese\n5\n\n\n\n\n\n\nTable¬†5: Query 3 result table\n\n\n\n\n\n\nWhich restaurant of New York has generated the most revenue?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where city = 'New York' and cost*rating_count = (select max(cost*rating_count)\n                                                    from restaurants \n                                                    where city = 'New York')\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Curry House\n4,955\n$97.92\nNew York\nJapanese\n5\n\n\n\n\n\n\nTable¬†6: Query 4 result table\n\n\n\n\n\n\nWhich restaurant chain has the maximum number of restaurants?\n\n\nCode\n(\n    conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nno_of_chains\n\n\n\n\nThe Burger Joint\n721\n\n\nPizza Palace\n703\n\n\nGreek Gyros\n696\n\n\nCafe Delight\n692\n\n\nFrench Delights\n681\n\n\nThe BBQ Shack\n671\n\n\nThe Golden Wok\n667\n\n\nOcean Breeze\n665\n\n\nSpice & Bloom\n665\n\n\nMidnight Diner\n657\n\n\n\n\n\n\nTable¬†7: Query 5 result table\n\n\n\n\n\n\nCode\nres_chains = conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n''').pl()\n\nfig = px.bar(res_chains, \n             x='no_of_chains',\n             y='name',\n             orientation='h',\n             hover_data=['no_of_chains', 'name',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Chains by Restaurant',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='outside',\n                  marker_color='rgb(55, 83, 109)',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,)\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Chains', yaxis_title='Restaurant')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†2: Restaurant Chains\n\n\n\n\n\n\nWhich restaurant chain has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select name, sum(rating_count * cost) as revenue\n    from restaurants\n    group by name\n    order by revenue DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'revenue': '{:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrevenue\n\n\n\n\nThe Burger Joint\n108,820,424.64\n\n\nPizza Palace\n100,382,853.92\n\n\nCafe Delight\n98,037,063.93\n\n\nGreek Gyros\n96,403,445.25\n\n\nThe BBQ Shack\n96,286,414.02\n\n\nOcean Breeze\n95,664,612.77\n\n\nThe Golden Wok\n94,860,125.75\n\n\nSpice & Bloom\n91,824,854.56\n\n\nFrench Delights\n91,236,701.38\n\n\nMidnight Diner\n91,170,162.30\n\n\n\n\n\n\nTable¬†8: Query 6 result table\n\n\n\n\n\n\nCode\nrev = conn.sql('''\n            select name, sum(rating_count * cost) as revenue\n            from restaurants\n            group by name\n            order by revenue DESC\n            limit 10\n            ''').pl()\n\nfig = px.bar(rev, \n             x='revenue',\n             y='name',\n             orientation='h',\n             hover_data=['revenue', 'name',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Revenue by Restaurant',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='inside',\n                  marker_color='#004700',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,\n                  hovertemplate='&lt;br&gt;'.join([\n                      'Restaurant: %{y}',\n                      'Revenue: %{x}',\n                  ]),\n                 )\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Revenue', yaxis_title='Restaurant',\n                 xaxis_tickprefix='$', xaxis_tickformat=',.2f')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†3: Revenue by Restaurant\n\n\n\n\n\n\nWhich city has the maximum number of restaurants?\n\n\nCode\n(\n    conn.sql('''\n    select city, count(*) as no_of_restaurants\n    from restaurants\n    group by city\n    order by no_of_restaurants DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'no_of_restaurants': '{:,.0f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nno_of_restaurants\n\n\n\n\nTokyo\n1,071\n\n\nAmsterdam\n1,038\n\n\nLisbon\n1,005\n\n\nMadrid\n1,003\n\n\nLondon\n993\n\n\nParis\n992\n\n\nRome\n979\n\n\nBerlin\n977\n\n\nNew York\n971\n\n\nSydney\n971\n\n\n\n\n\n\nTable¬†9: Query 7 result table\n\n\n\n\n\n\nWhich city has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select city, sum(rating_count * cost) as revenue\n    from restaurants\n    group by city\n    order by revenue DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'revenue': '${:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nrevenue\n\n\n\n\nAmsterdam\n$148,839,878.62\n\n\nTokyo\n$148,035,421.32\n\n\nMadrid\n$141,487,618.97\n\n\nParis\n$141,219,374.56\n\n\nLondon\n$140,876,613.54\n\n\nRome\n$139,622,129.63\n\n\nNew York\n$138,621,609.28\n\n\nLisbon\n$136,814,247.27\n\n\nBerlin\n$136,434,163.25\n\n\nSydney\n$131,656,513.97\n\n\n\n\n\n\nTable¬†10: Query 8 result table\n\n\n\n\n\n\nCode\ncities = conn.sql('''\n                select city, sum(rating_count * cost) as revenue\n                from restaurants\n                group by city\n                order by revenue DESC\n                limit 10\n                ''').pl()\n\nfig = px.bar(cities, \n             x='revenue',\n             y='city',\n             orientation='h',\n             hover_data=['revenue', 'city',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Revenue by City',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='inside',\n                  marker_color='#880808',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,\n                  hovertemplate='&lt;br&gt;'.join([\n                      'City: %{y}',\n                      'Revenue: %{x}',\n                  ]),\n                 )\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Revenue', yaxis_title='City',\n                 xaxis_tickprefix='$', xaxis_tickformat=',.2f')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†4: Revenue by City\n\n\n\n\n\n\nList 10 least expensive cuisines\n\n\nCode\n(\n    conn.sql('''\n    select cuisine, avg(cost) as avg_cost\n    from restaurants\n    group by cuisine\n    order by avg_cost asc\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncuisine\navg_cost\n\n\n\n\nIndian\n$53.92\n\n\nAmerican\n$53.96\n\n\nItalian\n$54.61\n\n\nThai\n$54.64\n\n\nFrench\n$54.65\n\n\nMexican\n$55.47\n\n\nSpanish\n$55.54\n\n\nJapanese\n$55.55\n\n\nGreek\n$55.77\n\n\nChinese\n$56.26\n\n\n\n\n\n\nTable¬†11: Query 9 result table\n\n\n\n\n\n\nList 10 most expensive cuisines\n\n\nCode\n(\n    conn.sql('''\n    select cuisine, avg(cost) as avg_cost\n    from restaurants\n    group by cuisine\n    order by avg_cost desc\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncuisine\navg_cost\n\n\n\n\nChinese\n$56.26\n\n\nGreek\n$55.77\n\n\nJapanese\n$55.55\n\n\nSpanish\n$55.54\n\n\nMexican\n$55.47\n\n\nFrench\n$54.65\n\n\nThai\n$54.64\n\n\nItalian\n$54.61\n\n\nAmerican\n$53.96\n\n\nIndian\n$53.92\n\n\n\n\n\n\nTable¬†12: Query 10 result table\n\n\n\n\n\n\nWhat city has Mexican food as the most popular cuisine?\n\n\nCode\n(\n    conn.sql('''\n    select city, avg(cost) avg_cost, count(*) as restaurants\n    from restaurants\n    where cuisine = 'Mexican'\n    group by city\n    order by restaurants desc\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\navg_cost\nrestaurants\n\n\n\n\nLondon\n$52.30\n112\n\n\nTokyo\n$48.83\n111\n\n\nRome\n$57.11\n108\n\n\nAmsterdam\n$60.67\n108\n\n\nLisbon\n$56.35\n101\n\n\n\n\n\n\nTable¬†13: Query 11 result table\n\n\n\n\n\n\nCode\n# close connection\nconn.close()"
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#conclusions",
    "href": "portfolio2/posts/python/restaurants_sql.html#conclusions",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Conclusions",
    "text": "Conclusions\nThis project highlights the value of combining the strengths of SQL and Python for data-intensive tasks, providing a robust and efficient solution for a wide range of data analysis challenges.\nBy carefully considering the interaction between DuckDB, Polars, and Plotly, you can create a powerful and efficient data analysis and visualization pipeline.\nThis approach can lead organizations to unlock valuable insights from their data more quickly and effectively, driving data-driven decision-making."
  },
  {
    "objectID": "portfolio2/posts/python/restaurants_sql.html#contact",
    "href": "portfolio2/posts/python/restaurants_sql.html#contact",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html",
    "href": "portfolio2/posts/python/spatial_analysis.html",
    "title": "Geospatial Analysis in Python",
    "section": "",
    "text": "Figure¬†1: Image by Henrikki Tenkanen, Vuokko Heikinheimo, David Whipp"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#environment-setting",
    "href": "portfolio2/posts/python/spatial_analysis.html#environment-setting",
    "title": "Geospatial Analysis in Python",
    "section": "Environment setting",
    "text": "Environment setting\n\n\nCode\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport folium\nfrom great_tables import GT, md\nfrom warnings import filterwarnings\nfilterwarnings('ignore')"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#data-collection",
    "href": "portfolio2/posts/python/spatial_analysis.html#data-collection",
    "title": "Geospatial Analysis in Python",
    "section": "Data collection",
    "text": "Data collection\n\n\nCode\nconn = db.connect('datasets/geospatial.db')\n\n\n\n\nCode\nconn.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  name   ‚îÇ\n‚îÇ varchar ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ zomato  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\ndata = conn.sql('select * from zomato').pl()\n\n\n\n\nCode\ndata.columns\n\n\n['url',\n 'address',\n 'name',\n 'online_order',\n 'book_table',\n 'rate',\n 'votes',\n 'phone',\n 'location',\n 'rest_type',\n 'dish_liked',\n 'cuisines',\n 'approx_cost(for two people)',\n 'reviews_list',\n 'menu_item',\n 'listed_in(type)',\n 'listed_in(city)']\n\n\n\n\nCode\ndata.shape\n\n\n(51717, 17)"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#data-preprocessing",
    "href": "portfolio2/posts/python/spatial_analysis.html#data-preprocessing",
    "title": "Geospatial Analysis in Python",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\n\nCode\ndata.is_duplicated().sum()\n\n\n0\n\n\n\n\nCode\ndata.select(pl.all().is_null().sum()).to_dicts()\n\n\n[{'url': 0,\n  'address': 0,\n  'name': 0,\n  'online_order': 0,\n  'book_table': 0,\n  'rate': 7775,\n  'votes': 0,\n  'phone': 1208,\n  'location': 21,\n  'rest_type': 227,\n  'dish_liked': 28078,\n  'cuisines': 45,\n  'approx_cost(for two people)': 346,\n  'reviews_list': 0,\n  'menu_item': 0,\n  'listed_in(type)': 0,\n  'listed_in(city)': 0}]\n\n\n\n\nCode\n# As we have few missing values in location feature ,then we can drop the null\ndata = data.drop_nulls(subset=pl.col('location'))\n\n\n\n\nCode\n(\n    GT(data.select('address','name','rate','votes','location','rest_type','dish_liked','cuisines').head(3))\n    .tab_header(\n        title=md('Zomato Restaurants')\n    )\n    .cols_width(\n        cases={'rate':'50px',\n              }\n               )\n    .tab_source_note(source_note=md('&lt;br&gt; *Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants\n\n\naddress\nname\nrate\nvotes\nlocation\nrest_type\ndish_liked\ncuisines\n\n\n\n\n942, 21st Main Road, 2nd Stage, Banashankari, Bangalore\nJalsa\n4.1/5\n775\nBanashankari\nCasual Dining\nPasta, Lunch Buffet, Masala Papad, Paneer Lajawab, Tomato Shorba, Dum Biryani, Sweet Corn Soup\nNorth Indian, Mughlai, Chinese\n\n\n2nd Floor, 80 Feet Road, Near Big Bazaar, 6th Block, Kathriguppe, 3rd Stage, Banashankari, Bangalore\nSpice Elephant\n4.1/5\n787\nBanashankari\nCasual Dining\nMomos, Lunch Buffet, Chocolate Nirvana, Thai Green Curry, Paneer Tikka, Dum Biryani, Chicken Biryani\nChinese, North Indian, Thai\n\n\n1112, Next to KIMS Medical College, 17th Cross, 2nd Stage, Banashankari, Bangalore\nSan Churro Cafe\n3.8/5\n918\nBanashankari\nCafe, Casual Dining\nChurros, Cannelloni, Minestrone Soup, Hot Chocolate, Pink Sauce Pasta, Salsa, Veg Supreme Pizza\nCafe, Mexican, Italian\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†1: Zomato Restaurants from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\n\n\nCode\n# create a copy\ndf = data.clone()\n\n\nLets make every place more readible so that u will get more more accurate geographical co-ordinates..\n\n\nCode\ndf = df.with_columns(\n    location=(pl.col('location') + ', Bangalore, Karnataka, India')\n)\n\n\n\n\nCode\ndf.select('location').sample(5).to_dicts()\n\n\n[{'location': 'HSR, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'}]\n\n\n\n\nCode\ndf.schema\n\n\nSchema([('url', String),\n        ('address', String),\n        ('name', String),\n        ('online_order', Boolean),\n        ('book_table', Boolean),\n        ('rate', String),\n        ('votes', Int64),\n        ('phone', String),\n        ('location', String),\n        ('rest_type', String),\n        ('dish_liked', String),\n        ('cuisines', String),\n        ('approx_cost(for two people)', String),\n        ('reviews_list', String),\n        ('menu_item', String),\n        ('listed_in(type)', String),\n        ('listed_in(city)', String)])"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#extract-coordinates-from-data",
    "href": "portfolio2/posts/python/spatial_analysis.html#extract-coordinates-from-data",
    "title": "Geospatial Analysis in Python",
    "section": "Extract coordinates from data",
    "text": "Extract coordinates from data\nfirst we will learn how to extract Latitudes & longitudes using ‚Äòlocation‚Äô feature\n\n\nCode\nrest_loc = pl.DataFrame()\n\n\n\n\nCode\nrest_loc = pl.DataFrame({'name': df.select('location').unique()})\n\n\n\n\nCode\nrest_loc.sample(5).to_dicts()\n\n\n[{'name': 'Jakkur, Bangalore, Karnataka, India'},\n {'name': 'Kalyan Nagar, Bangalore, Karnataka, India'},\n {'name': 'RT Nagar, Bangalore, Karnataka, India'},\n {'name': 'Koramangala 7th Block, Bangalore, Karnataka, India'},\n {'name': 'Kaggadasapura, Bangalore, Karnataka, India'}]\n\n\n\n\nCode\n# Nominatim is a tool to search OpenStreetMap data by address or location\nfrom geopy.geocoders import Nominatim\n\n\n\n\nCode\ngeolocator = Nominatim(user_agent='app', timeout=None)\n\n\n\n\nCode\nlat = [] # define lat list to store all the latitudes\nlon = [] # define lon list to store all the longitudes\n\nfor name in pl.Series(rest_loc.select('name')):\n    location = geolocator.geocode(name)\n    \n    if location is None:\n        lat.append(np.nan)\n        lon.append(np.nan)\n        \n    else:\n        lat.append(location.latitude)\n        lon.append(location.longitude)\n\n\n\n\nCode\nlat[:10]\n\n\n[13.0621474,\n 12.9846713,\n 12.981015523680384,\n 12.985098650000001,\n 12.9096941,\n nan,\n 12.9067683,\n 12.938455602031697,\n 12.9176571,\n 12.9489339]\n\n\n\n\nCode\nrest_loc = rest_loc.with_columns(\n    lat=pl.Series(lat), # For python lists, construct a Series\n    lon=pl.Series(lon),\n)\n\n\n\n\nCode\n(\n    GT(rest_loc.head(5), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Coordinates')\n    )\n    .fmt_number(columns=['lat','lon'], decimals=4, use_seps=False)\n    .cols_width(\n        cases={'name':'200%',\n               'lat':'90%',\n               'lon':'90%',\n              }\n               )\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Coordinates\n\n\nname\nlat\nlon\n\n\n\n\nSahakara Nagar, Bangalore, Karnataka, India\n13.0621\n77.5801\n\n\nKaggadasapura, Bangalore, Karnataka, India\n12.9847\n77.6791\n\n\nInfantry Road, Bangalore, Karnataka, India\n12.9810\n77.6021\n\n\nCV Raman Nagar, Bangalore, Karnataka, India\n12.9851\n77.6631\n\n\nJP Nagar, Bangalore, Karnataka, India\n12.9097\n77.5866\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†2: Zomato restaurants coordinates from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nWe have found out latitude and longitude of each location listed in the dataset using geopy This is used to plot maps.\n\n\nCode\npl.Series(rest_loc.select('lat')).is_null().sum()\n\n\n0\n\n\n\n\nCode\npl.Series(rest_loc.select('lat')).is_nan().sum()\n\n\n2\n\n\n\n\nCode\nrest_loc.filter(pl.col('lat').is_nan())\n\n\n\nshape: (2, 3)\n\n\n\nname\nlat\nlon\n\n\nstr\nf64\nf64\n\n\n\n\n\"Sadashiv Nagar, Bangalore, Kar‚Ä¶\nNaN\nNaN\n\n\n\"Rammurthy Nagar, Bangalore, Ka‚Ä¶\nNaN\nNaN\n\n\n\n\n\n\n\n\nCode\nrest_loc = rest_loc.drop_nans()"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#where-are-most-number-of-restaurants-located-in-bengalore",
    "href": "portfolio2/posts/python/spatial_analysis.html#where-are-most-number-of-restaurants-located-in-bengalore",
    "title": "Geospatial Analysis in Python",
    "section": "Where are most number of restaurants located in Bengalore?",
    "text": "Where are most number of restaurants located in Bengalore?\n\n\nCode\nrest_locations = pl.Series(df.select('location')).value_counts(sort=True, name='total')\n\n\n\n\nCode\nrest_locations = rest_locations.rename({'location':'name', 'total':'count'})\n\n\n\n\nCode\n(\n    GT(rest_locations.head(), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Count')\n    )\n    .cols_width(cases={'name': '200%',})\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Count\n\n\nname\ncount\n\n\n\n\nBTM, Bangalore, Karnataka, India\n5124\n\n\nHSR, Bangalore, Karnataka, India\n2523\n\n\nKoramangala 5th Block, Bangalore, Karnataka, India\n2504\n\n\nJP Nagar, Bangalore, Karnataka, India\n2235\n\n\nWhitefield, Bangalore, Karnataka, India\n2144\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†3: Zomato restaurants count from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nNow we can say that these are locations where most of restaurants are located.\nLets create Heatmap of this results so that it becomes more user-friendly.\nNow, in order to perform spatial analysis, we need latitudes & longitudes of every location, so lets merge both dataframes in order to get geographical co-ordinates.\n\n\nCode\nbeng_rest_locations = rest_locations.join(rest_loc, on='name')\n\n\n\n\nCode\n(\n    GT(beng_rest_locations.head(), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Count & coordinates')\n    )\n    .cols_width(cases={'name': '200%',})\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Count & coordinates\n\n\nname\ncount\nlat\nlon\n\n\n\n\nBTM, Bangalore, Karnataka, India\n5124\n12.9163603\n77.604733\n\n\nHSR, Bangalore, Karnataka, India\n2523\n12.90056335\n77.64947470503677\n\n\nKoramangala 5th Block, Bangalore, Karnataka, India\n2504\n12.9348429\n77.6189768\n\n\nJP Nagar, Bangalore, Karnataka, India\n2235\n12.9096941\n77.5866067\n\n\nWhitefield, Bangalore, Karnataka, India\n2144\n12.9696365\n77.7497448\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†4: Zomato restaurants count and coordinates from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nnow in order to show-case it via Map(Heatmap) ,first we need to create BaseMap so that I can map our Heatmap on top of BaseMap !\n\n\nCode\ndef Generate_basemap():\n    basemap = folium.Map(location=[12.97 , 77.59], zoom_start=11)\n    return basemap\n\n\n\n\nCode\n# Geographic heat maps are used to identify where something occurs, and demonstrate areas of high and low density...\nfrom folium.plugins import HeatMap\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nbeng_rest_locations = beng_rest_locations.to_pandas()\n\n\n\n\nCode\nHeatMap(beng_rest_locations[['lat', 'lon' , 'count']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x3058e0da0&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†2: Zomato Restaurants Heatmap\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out.\n\n\nMajority of the Restaurants are avaiable in the city centre area."
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#performing-marker-cluster-analysis",
    "href": "portfolio2/posts/python/spatial_analysis.html#performing-marker-cluster-analysis",
    "title": "Geospatial Analysis in Python",
    "section": "Performing Marker Cluster Analysis",
    "text": "Performing Marker Cluster Analysis\n\n\nCode\nfrom folium.plugins import FastMarkerCluster\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nFastMarkerCluster(beng_rest_locations[['lat', 'lon' , 'count']]).add_to(basemap)\n\n\n&lt;folium.plugins.fast_marker_cluster.FastMarkerCluster at 0x30a2cd280&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†3: Zomato Marker Cluster Map\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out."
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#mapping-all-the-markers-of-places-of-bangalore",
    "href": "portfolio2/posts/python/spatial_analysis.html#mapping-all-the-markers-of-places-of-bangalore",
    "title": "Geospatial Analysis in Python",
    "section": "Mapping all the markers of places of Bangalore",
    "text": "Mapping all the markers of places of Bangalore\nPlotting Markers on the Map :\nFolium gives a folium.Marker() class for plotting markers on a map\nJust pass the latitude and longitude of the location, mention the popup and tooltip and add it to the map.\nPlotting markers is a two-step process.\n\nyou need to create a base map on which your markers will be placed\nand then add your markers to it:\n\n\n\nCode\nm = Generate_basemap()\n\n\n\n\nCode\n# Add points to the map\nfor index, row in beng_rest_locations.iterrows():\n    folium.Marker(location=[row['lat'], row['lon']], popup=row['count']).add_to(m)\n\n\n\n\nCode\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†4: Zomato Restaurants Marker Map\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out.\n\n\nRate field cleaning\nIn order to Analyse where are the restaurants situated with high average rate, first we need to clean ‚Äòrate‚Äô feature\n\n\nCode\n(\n    df.filter(\n        pl.col('rate').str.contains('^([^0-9]*)$')\n    )\n    .select('rate')\n    .unique()\n    .to_dicts()\n)\n\n\n[{'rate': '-'}, {'rate': 'NEW'}]\n\n\n\n\nCode\npl.Series(df.select('rate')).is_null().sum()\n\n\n7754\n\n\n\n\nCode\n# approximately 15% of your rating belongs to missing values\npl.Series(df.select('rate')).is_null().sum()/pl.Series(df.select('rate')).len()*100\n\n\n14.999226245744351\n\n\n\n\nCode\ndf = (\n    df.drop_nulls(subset='rate')\n        .with_columns(\n            pl.col('rate').replace(['NEW', '-',], ['0', '0'])\n        )\n        .with_columns(\n            rating=pl.col('rate').str.replace('/5', '')\n        )\n        .with_columns(\n            pl.col('rating').str.strip_chars()\n        )\n        .cast({'rating': pl.Float32})\n)\n\n\n\n\nCode\ndf.select('rating').unique().to_dicts()\n\n\n[{'rating': 2.4000000953674316},\n {'rating': 2.299999952316284},\n {'rating': 0.0},\n {'rating': 3.5},\n {'rating': 4.099999904632568},\n {'rating': 4.400000095367432},\n {'rating': 4.699999809265137},\n {'rating': 4.900000095367432},\n {'rating': 2.700000047683716},\n {'rating': 3.9000000953674316},\n {'rating': 3.799999952316284},\n {'rating': 3.4000000953674316},\n {'rating': 3.0},\n {'rating': 2.5999999046325684},\n {'rating': 3.299999952316284},\n {'rating': 4.199999809265137},\n {'rating': 2.200000047683716},\n {'rating': 4.0},\n {'rating': 4.5},\n {'rating': 2.5},\n {'rating': 3.5999999046325684},\n {'rating': 3.700000047683716},\n {'rating': 2.0999999046325684},\n {'rating': 4.800000190734863},\n {'rating': 3.200000047683716},\n {'rating': 2.799999952316284},\n {'rating': 4.300000190734863},\n {'rating': 2.9000000953674316},\n {'rating': 2.0},\n {'rating': 4.599999904632568},\n {'rating': 3.0999999046325684},\n {'rating': 1.7999999523162842}]"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#most-highest-rated-restaurants",
    "href": "portfolio2/posts/python/spatial_analysis.html#most-highest-rated-restaurants",
    "title": "Geospatial Analysis in Python",
    "section": "Most highest rated restaurants",
    "text": "Most highest rated restaurants\n\n\nCode\ndf.select('name','rate','votes','location','dish_liked','rating').sort('rating', descending=True).head()\n\n\n\nshape: (5, 6)\n\n\n\nname\nrate\nvotes\nlocation\ndish_liked\nrating\n\n\nstr\nstr\ni64\nstr\nstr\nf32\n\n\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn‚Ä¶\n\"Cocktails, Dahi Kebab, Rajma C‚Ä¶\n4.9\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn‚Ä¶\n\"Cocktails, Dahi Kebab, Rajma C‚Ä¶\n4.9\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn‚Ä¶\n\"Cocktails, Dahi Kebab, Rajma C‚Ä¶\n4.9\n\n\n\"Belgian Waffle Factory\"\n\"4.9/5\"\n1746\n\"Brigade Road, Bangalore, Karna‚Ä¶\n\"Coffee, Berryblast, Nachos, Ch‚Ä¶\n4.9\n\n\n\"Belgian Waffle Factory\"\n\"4.9/5\"\n1746\n\"Brigade Road, Bangalore, Karna‚Ä¶\n\"Coffee, Berryblast, Nachos, Ch‚Ä¶\n4.9\n\n\n\n\n\n\n\n\nCode\ngrp_df = (\n    df.group_by('location').agg(pl.col('rating').mean(), pl.col('name').count())\n        .rename({'location':'name', 'rating':'avg_rating', 'name':'count'})\n)\n\n\n\n\nCode\ngrp_df\n\n\n\nshape: (92, 3)\n\n\n\nname\navg_rating\ncount\n\n\nstr\nf32\nu32\n\n\n\n\n\"Brookefield, Bangalore, Karnat‚Ä¶\n3.374697\n581\n\n\n\"Thippasandra, Bangalore, Karna‚Ä¶\n3.095396\n152\n\n\n\"Electronic City, Bangalore, Ka‚Ä¶\n3.04191\n964\n\n\n\"Koramangala 1st Block, Bangalo‚Ä¶\n3.263946\n965\n\n\n\"Koramangala 3rd Block, Bangalo‚Ä¶\n3.978755\n193\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"RT Nagar, Bangalore, Karnataka‚Ä¶\n3.278125\n64\n\n\n\"Jalahalli, Bangalore, Karnatak‚Ä¶\n3.486956\n23\n\n\n\"Commercial Street, Bangalore, ‚Ä¶\n3.109709\n309\n\n\n\"Banaswadi, Bangalore, Karnatak‚Ä¶\n3.362927\n499\n\n\n\"Koramangala 5th Block, Bangalo‚Ä¶\n3.901511\n2381\n\n\n\n\n\n\nlets consider only those restaurants who have send atleast 400 orders\n\n\nCode\ntemp_df = grp_df.filter(pl.col('count')&gt;400)\n\n\n\n\nCode\ntemp_df.shape\n\n\n(35, 3)\n\n\n\n\nCode\ntemp_df\n\n\n\nshape: (35, 3)\n\n\n\nname\navg_rating\ncount\n\n\nstr\nf32\nu32\n\n\n\n\n\"Brookefield, Bangalore, Karnat‚Ä¶\n3.374697\n581\n\n\n\"Electronic City, Bangalore, Ka‚Ä¶\n3.04191\n964\n\n\n\"Koramangala 1st Block, Bangalo‚Ä¶\n3.263946\n965\n\n\n\"Bannerghatta Road, Bangalore, ‚Ä¶\n3.271675\n1324\n\n\n\"HSR, Bangalore, Karnataka, Ind‚Ä¶\n3.484063\n2128\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Richmond Road, Bangalore, Karn‚Ä¶\n3.688013\n634\n\n\n\"Koramangala 7th Block, Bangalo‚Ä¶\n3.747846\n1089\n\n\n\"Frazer Town, Bangalore, Karnat‚Ä¶\n3.56488\n578\n\n\n\"Banaswadi, Bangalore, Karnatak‚Ä¶\n3.362927\n499\n\n\n\"Koramangala 5th Block, Bangalo‚Ä¶\n3.901511\n2381\n\n\n\n\n\n\n\n\nCode\nrest_loc\n\n\n\nshape: (91, 3)\n\n\n\nname\nlat\nlon\n\n\nstr\nf64\nf64\n\n\n\n\n\"Sahakara Nagar, Bangalore, Kar‚Ä¶\n13.062147\n77.580061\n\n\n\"Kaggadasapura, Bangalore, Karn‚Ä¶\n12.984671\n77.679091\n\n\n\"Infantry Road, Bangalore, Karn‚Ä¶\n12.981016\n77.602133\n\n\n\"CV Raman Nagar, Bangalore, Kar‚Ä¶\n12.985099\n77.663117\n\n\n\"JP Nagar, Bangalore, Karnataka‚Ä¶\n12.909694\n77.586607\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Seshadripuram, Bangalore, Karn‚Ä¶\n12.993188\n77.575342\n\n\n\"Jakkur, Bangalore, Karnataka, ‚Ä¶\n13.078474\n77.606894\n\n\n\"Bommanahalli, Bangalore, Karna‚Ä¶\n12.908945\n77.623904\n\n\n\"Kammanahalli, Bangalore, Karna‚Ä¶\n13.009346\n77.637709\n\n\n\"Nagawara, Bangalore, Karnataka‚Ä¶\n13.042279\n77.624858\n\n\n\n\n\n\nlets merge both the dataframe so that we can get coordinates as well\n\n\nCode\nratings_locations = temp_df.join(rest_loc, on='name')\n\n\n\n\nCode\nratings_locations\n\n\n\nshape: (35, 5)\n\n\n\nname\navg_rating\ncount\nlat\nlon\n\n\nstr\nf32\nu32\nf64\nf64\n\n\n\n\n\"JP Nagar, Bangalore, Karnataka‚Ä¶\n3.412929\n1849\n12.909694\n77.586607\n\n\n\"Koramangala 4th Block, Bangalo‚Ä¶\n3.814351\n864\n12.932778\n77.629405\n\n\n\"Whitefield, Bangalore, Karnata‚Ä¶\n3.384171\n1693\n12.969637\n77.749745\n\n\n\"Bannerghatta Road, Bangalore, ‚Ä¶\n3.271675\n1324\n12.951856\n77.604011\n\n\n\"Jayanagar, Bangalore, Karnatak‚Ä¶\n3.61525\n1718\n12.939904\n77.582638\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Ulsoor, Bangalore, Karnataka, ‚Ä¶\n3.541396\n901\n12.977879\n77.62467\n\n\n\"Frazer Town, Bangalore, Karnat‚Ä¶\n3.56488\n578\n12.998683\n77.615525\n\n\n\"Indiranagar, Bangalore, Karnat‚Ä¶\n3.652168\n1936\n12.996298\n77.545278\n\n\n\"Koramangala 6th Block, Bangalo‚Ä¶\n3.662465\n1111\n12.939025\n77.623848\n\n\n\"Kammanahalli, Bangalore, Karna‚Ä¶\n3.499809\n525\n13.009346\n77.637709\n\n\n\n\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nratings_locations = ratings_locations.to_pandas()\n\n\n\n\nCode\nHeatMap(ratings_locations[['lat', 'lon' , 'avg_rating']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x30a39bcb0&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†5: Highest-rated Zomato Restaurants Heatmap\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out."
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#conclusions",
    "href": "portfolio2/posts/python/spatial_analysis.html#conclusions",
    "title": "Geospatial Analysis in Python",
    "section": "Conclusions",
    "text": "Conclusions\nPython, with its powerful libraries and ease of use, has become an indispensable tool for geospatial analysis. By leveraging the capabilities of libraries like GeoPandas, Shapely, and folium, data scientists can effectively explore and analyze geospatial data, gain valuable insights, and make informed decisions.\nIn this article, we have shown a brief overview of geospatial analysis in Python."
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#references",
    "href": "portfolio2/posts/python/spatial_analysis.html#references",
    "title": "Geospatial Analysis in Python",
    "section": "References",
    "text": "References\n\nSingh, S (2024) Spatial Analysis & Geospatial Data Science in Python\nTenkanen, H et al (2022) Introduction to Python for Geographic Data Analysis"
  },
  {
    "objectID": "portfolio2/posts/python/spatial_analysis.html#contact",
    "href": "portfolio2/posts/python/spatial_analysis.html#contact",
    "title": "Geospatial Analysis in Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/slides.html",
    "href": "portfolio2/posts/python/slides.html",
    "title": "Creating Slides with Python and Quarto",
    "section": "",
    "text": "Quarto offers a powerful and versatile tool for data scientists to create presentations with code that are informative, transparent, reproducible, and visually appealing."
  },
  {
    "objectID": "portfolio2/posts/python/slides.html#contact",
    "href": "portfolio2/posts/python/slides.html#contact",
    "title": "Creating Slides with Python and Quarto",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/plotting-with-holoviews.html#overview",
    "href": "portfolio2/posts/python/plotting-with-holoviews.html#overview",
    "title": "Creating Charts with HoloViews",
    "section": "Overview",
    "text": "Overview\nFor data analytics, the magic lies in transforming raw numbers into visuals that reveal hidden patterns and trends. But the process of creating these visualizations can often be cumbersome, requiring extensive coding and customization. Here‚Äôs where HoloViews steps in, offering a refreshing approach to data visualization in Python."
  },
  {
    "objectID": "portfolio2/posts/python/plotting-with-holoviews.html#what-is-holoviews",
    "href": "portfolio2/posts/python/plotting-with-holoviews.html#what-is-holoviews",
    "title": "Creating Charts with HoloViews",
    "section": "What is HoloViews?",
    "text": "What is HoloViews?\nHoloViews is an open-source Python library designed to streamline data analysis and visualization. It departs from the traditional method of meticulously crafting plots line by line. Instead, HoloViews focuses on a declarative approach, where you describe your data and desired visualization, and it takes care of the intricate details. This allows you to express your ideas with concise code, freeing you to delve deeper into your story-telling."
  },
  {
    "objectID": "portfolio2/posts/python/plotting-with-holoviews.html#why-choose-holoviews",
    "href": "portfolio2/posts/python/plotting-with-holoviews.html#why-choose-holoviews",
    "title": "Creating Charts with HoloViews",
    "section": "Why Choose HoloViews?",
    "text": "Why Choose HoloViews?\nSeveral factors make HoloViews an attractive option for data visualization:\n\nSimplicity\n\nHoloViews boasts a user-friendly syntax, enabling you to create complex visualizations with minimal code. This focus on brevity empowers you to iterate quickly and explore your data efficiently.\n\nFlexibility\n\nHoloViews integrates seamlessly with popular data structures like NumPy and Pandas, effortlessly handling your data. Additionally, it plays well with different plotting backends like Bokeh, Plotly and Matplotlib, giving you control over the final look and feel of your visualizations.\n\nInteractivity\n\nHoloViews visualizations are not static images. They can be interactive, allowing users to zoom, pan, and explore the data from various angles. This interactivity fosters deeper engagement and a richer understanding of the information.\n\nComprehensiveness\n\nThe HoloViews ecosystem extends beyond the core library. It encompasses projects like hvPlot for quick visualizations and GeoViews for crafting geographical visualizations. This suite of tools caters to a wide range of data exploration needs."
  },
  {
    "objectID": "portfolio2/posts/python/plotting-with-holoviews.html#environment-settings",
    "href": "portfolio2/posts/python/plotting-with-holoviews.html#environment-settings",
    "title": "Creating Charts with HoloViews",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nCode\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom itables import init_notebook_mode\ninit_notebook_mode(all_interactive=True)\nimport holoviews as hv\nhv.extension('bokeh')\n\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nThis is the init_notebook_mode cell from ITables v2.2.3\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\nCode\n# Get data\nurl = 'https://raw.githubusercontent.com/shoukewei/data/main/data-pydm/gdp_china_outlier_treated.csv'\ndf = pd.read_csv(url)\n\n\n\n\nCode\n# Dataset preview\nfrom itables import show\n\nshow(df, lengthMenu=[10, 25, 50, 100,])\n\n\n\n\n    \n      \n      prov\n      gdpr\n      year\n      gdp\n      pop\n      finv\n      trade\n      fexpen\n      uinc\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.3 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n\n\nCode\n# Change column name\ndf = df.rename(columns={'prov':'Province',})\n\n\n\n\nCode\n# Convert to holviews dataset\nhd = hv.Dataset(df)"
  },
  {
    "objectID": "portfolio2/posts/python/plotting-with-holoviews.html#creating-charts",
    "href": "portfolio2/posts/python/plotting-with-holoviews.html#creating-charts",
    "title": "Creating Charts with HoloViews",
    "section": "Creating Charts",
    "text": "Creating Charts\n\n\nCode\n# Customization specs\ngrid_style = {'grid_line_color': 'white',\n              'grid_line_width': 2.,\n             }\n\n# Create chart\ncurves_app = hd.to(hv.Bars, kdims=['year'], vdims=['gdp'], groupby='Province',)\n\n# Chart options\ncurves_app.opts(height=500,\n                width=650,\n                xlabel='Year',\n                tools=['hover'],\n                ylabel='GDP',\n                xrotation=45,\n                toolbar=None, #above, below, left, right\n                fill_color='#1c2841',\n                line_color='black',\n                bgcolor='#f6f6f6',\n                show_grid=True,\n                gridstyle=grid_style,\n               )\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can select a Province from the filter to get its correspondent chart."
  },
  {
    "objectID": "portfolio2/posts/python/plotting-with-holoviews.html#conclusions",
    "href": "portfolio2/posts/python/plotting-with-holoviews.html#conclusions",
    "title": "Creating Charts with HoloViews",
    "section": "Conclusions",
    "text": "Conclusions\nAs you gain experience with HoloViews, you can delve into its more advanced features.\nHoloViews allows for extensive customization, enabling you to tailor your visualizations to perfectly suit your needs and branding.\nFurthermore, HoloViews integrates well with other data science libraries within the Python ecosystem, fostering a powerful and cohesive environment for data exploration and analysis."
  },
  {
    "objectID": "portfolio2/posts/python/plotting-with-holoviews.html#contact",
    "href": "portfolio2/posts/python/plotting-with-holoviews.html#contact",
    "title": "Creating Charts with HoloViews",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/datawrapper_api.html",
    "href": "portfolio2/posts/python/datawrapper_api.html",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "",
    "text": "Figure¬†1: Datawrapper Charts Catalogue"
  },
  {
    "objectID": "portfolio2/posts/python/datawrapper_api.html#key-features-of-datawrapper",
    "href": "portfolio2/posts/python/datawrapper_api.html#key-features-of-datawrapper",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Key Features of Datawrapper",
    "text": "Key Features of Datawrapper\n\nEasy-to-use interface\n\nDatawrapper‚Äôs intuitive design makes it accessible to users of all skill levels. Even those without a strong background in data visualization can create professional-looking charts and maps.\n\nDiverse chart¬†types\n\nFrom simple bar charts and line graphs to more complex maps and scatter plots, Datawrapper offers a variety of chart types to suit different data sets and storytelling needs.\n\nCustomization\n\nUsers can customize their visualizations to match their brand or personal style. This includes options for changing colors, fonts, and layouts.\n\nInteractivity\n\nDatawrapper allows users to create interactive visualizations that respond to user input. For example, users can hover over data points to see more detailed information or filter data based on specific criteria.\n\nEmbedding and¬†sharing\n\nOnce a visualization is created, it can be easily embedded into websites, blogs, or social media posts. Users can also share their visualizations directly with others.\n\nCollaboration\n\nDatawrapper supports collaboration, allowing multiple users to work on the same visualization simultaneously. This is particularly useful for teams or organizations that need to create data visualizations together."
  },
  {
    "objectID": "portfolio2/posts/python/datawrapper_api.html#use-cases-for-datawrapper",
    "href": "portfolio2/posts/python/datawrapper_api.html#use-cases-for-datawrapper",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Use Cases for Datawrapper",
    "text": "Use Cases for Datawrapper\n\nJournalism\n\nDatawrapper is a popular tool among journalists who want to present complex data in a clear and visually engaging way. It can be used to create interactive charts, maps, and infographics that enhance storytelling.\n\nResearch\n\nResearchers can use Datawrapper to visualize their findings and make them more accessible to a wider audience. By presenting data in a visual format, researchers can communicate their ideas more effectively and increase the impact of their work.\n\nBusiness\n\nBusinesses can use Datawrapper to create dashboards, reports, and presentations that help them make data-driven decisions. By visualizing key metrics and trends, businesses can gain valuable insights into their performance.\n\nEducation\n\nTeachers and students can use Datawrapper to create interactive visualizations that help them understand and learn from data. It can be used to teach data analysis, statistics, and other subjects.\n\nVisualization Types\nCollection of available Datawrapper chart types and their IDs\n\n\n\nTable¬†1: Datawrapper Chart Types\n\n\n\n\n\nChart\nTypeID\n\n\n\n\nBar Chart\nd3-bars\n\n\nSplit Bars\nd3-bars-split\n\n\nStacked Bars\nd3-bars-stacked\n\n\nBullet Bars\nd3-bars-bullet\n\n\nDot Plot\nd3-dot-plot\n\n\nRange Plot\nd3-range-plot\n\n\nArrow Plot\nd3-arrow-plot\n\n\nColumn Chart\ncolumn-chart\n\n\nGrouped Column Chart\ngrouped-column-chart\n\n\nStacked Column Chart\nstacked-column-chart\n\n\nArea Chart\nd3-area\n\n\nLine Chart\nd3-lines\n\n\nMultiple Lines Chart\nmultiple-lines\n\n\nPie Chart\nd3-pies\n\n\nDonut Chart\nd3-donuts\n\n\nMultiple Pies\nd3-multiple-pies\n\n\nMultiple Donuts\nd3-multiple-donuts\n\n\nScatter Plot\nd3-scatter-plot\n\n\nElection Donut\nelection-donut-chart\n\n\nTable\ntables\n\n\nChoropleth Map\nd3-maps-choropleth\n\n\nSymbol Map\nd3-maps-symbols\n\n\nLocator Map\nlocator-map\n\n\n\n\n\n\nMore information\n\n\nEnvironment settings\n\n\nShow code\n# Import libraries\nfrom datawrapper import Datawrapper\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow code\n# Token gotten from datawrapper\nfilename = 'credentials.json'\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n# read credentials\ntoken = keys['datawrapper_api']\n\n\n\n\nShow code\n# Access datawrapper api\ndw = Datawrapper(access_token = token)\n\n\n\n\nDatawrapper account\n\n\nShow code\n# Check your account details\ndw.get_my_account()\n\n\n\n\nDataset\n\n\nShow code\n# Read dataset courtesy of Sergio Sanchez\ndf = pd.read_csv(\n    'https://raw.githubusercontent.com/chekos/datasets/master/data/datawrapper_example.csv',\n    sep=\";\",\n)\n\n\n\n\nShow code\n# Select columns\ndf.columns = ['country','%pop in the capital','%pop in urban areas','%pop in rural areas']\n\n\n\n\nShow code\n# Show dataframe\ndf\n\n\n\n\n\n\n\n\n\ncountry\n%pop in the capital\n%pop in urban areas\n%pop in rural areas\n\n\n\n\n0\nIceland (Reykjav√≠k)\n56.02\n38.0\n6.0\n\n\n1\nArgentina (Buenos Aires)\n34.95\n56.6\n8.4\n\n\n2\nJapan (Tokyo)\n29.52\n63.5\n7.0\n\n\n3\nUK (London)\n22.70\n59.6\n17.7\n\n\n4\nDenmark (Copenhagen)\n22.16\n65.3\n12.5\n\n\n5\nFrance (Paris)\n16.77\n62.5\n20.7\n\n\n6\nRussia (Moscow)\n8.39\n65.5\n26.1\n\n\n7\nNiger (Niamey)\n5.53\n12.9\n81.5\n\n\n8\nGermany (Berlin)\n4.35\n70.7\n24.9\n\n\n9\nIndia (Delhi)\n1.93\n30.4\n67.6\n\n\n10\nUSA (Washington, D.C.)\n1.54\n79.9\n18.6\n\n\n11\nChina (Beijing)\n1.40\n53.0\n45.6\n\n\n\n\n\n\n\n\n\nCreate stackbar chart\n\n\nShow code\n# Create Datawrapper bar chart\npop = dw.create_chart(\n    title='Where do people live?', chart_type='d3-bars-stacked', data=df\n)\n\n\n\n\nUpdate chart description\n\n\nShow code\ndw.update_description(\n    pop['id'],\n    source_name = 'UN Population Division',\n    source_url = 'https://population.un.org/wup/',\n    byline = 'Jesus L. Monroy&lt;br&gt;Economist & Data Scientist&lt;br&gt;&lt;br&gt;',\n    intro = 'Population percentage living in the capital by Country'\n)\n\n\n\n\nPublish chart\n\n\nShow code\ndw.publish_chart(chart_id = pop['id'])\n\n\n\n\nCustomize metadata\n\n\nShow code\ndw.update_chart(\n    chart_id = pop['id'],\n    metadata = {\n        'visualize': {\n            'sharing': {'enabled': True},\n            'thick': True,\n            'custom-colors': {\n                '%pop in rural areas': '#dadada',\n                '%pop in urban areas': '#1d81a2',\n                '%pop in the capital': '#15607a',\n               },\n           },\n        'publish': {\n            'blocks': {'get-the-data': False},\n           },\n    }\n)\n\n\n\n\nRepublish chart\n\n\nShow code\ndw.publish_chart(pop['id'])"
  },
  {
    "objectID": "portfolio2/posts/python/datawrapper_api.html#display-interactive-chart",
    "href": "portfolio2/posts/python/datawrapper_api.html#display-interactive-chart",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure¬†2: Datawrapper Interactive Chart\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can hover over the bars on the above chart to get dynamic results."
  },
  {
    "objectID": "portfolio2/posts/python/datawrapper_api.html#conclusions",
    "href": "portfolio2/posts/python/datawrapper_api.html#conclusions",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Conclusions",
    "text": "Conclusions\nTo start using Datawrapper, simply create a free account on their website. Once you‚Äôre logged in, you can begin creating your first visualization.\nDatawrapper offers a variety of tutorials and resources to help you get started and make the most of the platform.\nBy leveraging the power of Datawrapper, you can create compelling data visualizations that help you tell your story more effectively.\nWhether you‚Äôre a seasoned data analyst or just starting out, Datawrapper is a valuable tool that can help you bring your data to life."
  },
  {
    "objectID": "portfolio2/posts/python/datawrapper_api.html#references",
    "href": "portfolio2/posts/python/datawrapper_api.html#references",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "References",
    "text": "References\n\nDatawrapper (2024) Barcharts in Datawrapper Academy\nKhanchandani E. (2020) How to use Datawrapper for journalists in Interhacktives\nSanchez, S. (2023) A lightweight Python wrapper for the Datawrapper API in Datawrapper API"
  },
  {
    "objectID": "portfolio2/posts/python/datawrapper_api.html#contact",
    "href": "portfolio2/posts/python/datawrapper_api.html#contact",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/python/airports.html",
    "href": "portfolio2/posts/python/airports.html",
    "title": "Flying into Mexico",
    "section": "",
    "text": "Figure¬†1: Mexico City‚Äôs International Airport"
  },
  {
    "objectID": "portfolio2/posts/python/airports.html#contact",
    "href": "portfolio2/posts/python/airports.html#contact",
    "title": "Flying into Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/tableau/covid-cases.html",
    "href": "portfolio2/posts/tableau/covid-cases.html",
    "title": "Covid Cases in Americas",
    "section": "",
    "text": "This dashboard was designed to track cases and deaths by covid in Americas in 2024 by region and country.\n\nData Sources: The data comes from Worldometer information about cases and deaths by countries.\nKey Metrics: The dashboard shows cases and deaths KPIs.\nVisualizations: The types of charts used are bar and map charts by country.\nTarget Audience: The dashboard was designed for health follow-up."
  },
  {
    "objectID": "portfolio2/posts/tableau/covid-cases.html#overview",
    "href": "portfolio2/posts/tableau/covid-cases.html#overview",
    "title": "Covid Cases in Americas",
    "section": "",
    "text": "This dashboard was designed to track cases and deaths by covid in Americas in 2024 by region and country.\n\nData Sources: The data comes from Worldometer information about cases and deaths by countries.\nKey Metrics: The dashboard shows cases and deaths KPIs.\nVisualizations: The types of charts used are bar and map charts by country.\nTarget Audience: The dashboard was designed for health follow-up."
  },
  {
    "objectID": "portfolio2/posts/tableau/covid-cases.html#contact",
    "href": "portfolio2/posts/tableau/covid-cases.html#contact",
    "title": "Covid Cases in Americas",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/tableau/missing-people.html",
    "href": "portfolio2/posts/tableau/missing-people.html",
    "title": "Missing People in Mexico",
    "section": "",
    "text": "This dashboard was designed to track missing people by location, last place seen and place type.\n\nData Sources: The data mainly come from government agency dedicated to attend requests about missing people.\nKey Metrics: The dashboard shows locations where missing people was seen for last time, location origin and missing people by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for government agency for public statistics and operational performance."
  },
  {
    "objectID": "portfolio2/posts/tableau/missing-people.html#overview",
    "href": "portfolio2/posts/tableau/missing-people.html#overview",
    "title": "Missing People in Mexico",
    "section": "",
    "text": "This dashboard was designed to track missing people by location, last place seen and place type.\n\nData Sources: The data mainly come from government agency dedicated to attend requests about missing people.\nKey Metrics: The dashboard shows locations where missing people was seen for last time, location origin and missing people by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for government agency for public statistics and operational performance."
  },
  {
    "objectID": "portfolio2/posts/tableau/missing-people.html#contact",
    "href": "portfolio2/posts/tableau/missing-people.html#contact",
    "title": "Missing People in Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/posts/tableau/mexico-city-crimes.html",
    "href": "portfolio2/posts/tableau/mexico-city-crimes.html",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "",
    "text": "This dashboard was designed to track crime incidence in Mexico City during 2019-2024 by time, gender, age, mayorship and neighborhood.\n\nData Sources: The data comes from Mexico City Open Data Webpage.\nKey Metrics: The dashboard shows crime KPIs.\nVisualizations: The types of charts used are bar and map charts.\nTarget Audience: The dashboard was designed for crime incidence follow-up."
  },
  {
    "objectID": "portfolio2/posts/tableau/mexico-city-crimes.html#overview",
    "href": "portfolio2/posts/tableau/mexico-city-crimes.html#overview",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "",
    "text": "This dashboard was designed to track crime incidence in Mexico City during 2019-2024 by time, gender, age, mayorship and neighborhood.\n\nData Sources: The data comes from Mexico City Open Data Webpage.\nKey Metrics: The dashboard shows crime KPIs.\nVisualizations: The types of charts used are bar and map charts.\nTarget Audience: The dashboard was designed for crime incidence follow-up."
  },
  {
    "objectID": "portfolio2/posts/tableau/mexico-city-crimes.html#contact",
    "href": "portfolio2/posts/tableau/mexico-city-crimes.html#contact",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio2/about.html",
    "href": "portfolio2/about.html",
    "title": "About me",
    "section": "",
    "text": "Economist & Data Scientist\n\nI‚Äôve applied data analysis and visualization to drive strategic value for 10+ years across industries like public security, e-commerce, and healthcare.\nI use Python1, SQL, and Tableau to transform raw data into clear, actionable data products (webpages, slideshows, interactive reports) that support decision-making, occasionally incorporating regression and classification analyses.\nCommitted to fostering data literacy, I share data posts on my Blog and also on Learning Data and T3CH via Medium.\nBackground in Economics (BA), Information Technology (MA) and Data Science and Machine Learning (Certification).\nVegan.\n¬© 2025"
  },
  {
    "objectID": "portfolio2/about.html#footnotes",
    "href": "portfolio2/about.html#footnotes",
    "title": "About me",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIncluding libraries such as numpy, pandas, polars, duckdb, matplotlib, seaborn, plotly, folium, sckitlearn.‚Ü©Ô∏é"
  },
  {
    "objectID": "portfolio/index.html",
    "href": "portfolio/index.html",
    "title": "Projects",
    "section": "",
    "text": "Mexico City Crime According to the Statistics\n\n\nIssues and Figures, 2019-2024\n\n\n\npython\n\n\n\nCrime in Mexico City presents a complex and evolving challenge. The city, a sprawling metropolis, grapples with a range of criminal activities, from petty theft and street-level drug offenses to organized crime and violent acts. Factors contributing to this multifaceted issue include socioeconomic disparities, corruption, and the influence of transnational criminal organizations.\n\n\n\n\n\nApr 11, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City Crime Incidence 2019-2024\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nStop Building Dashboards\n\n\nStart Understanding the White Collar Workflow\n\n\n\npython\n\nsql\n\n\n\nIn today‚Äôs data-driven world, Business Intelligence (BI) tools promise powerful insights and streamlined reporting. Yet, the humble spreadsheet and slideshow persist in the white-collar world. While BI tools manages complex analysis and visualization, spreadsheets and slideshows offer unique advantages that keep them firmly entrenched in our workflows.\n\n\n\n\n\nFeb 24, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGasoline Prices in Mexico\n\n\nFuel Price Fluctuations, A Constant Concern\n\n\n\npython\n\n\n\nGasoline prices in Mexico are a complex issue influenced by various factors. In this article we examine the gasoline prices in the 32 States that constitute Mexico and also their differences in cost, sale price and profits. Challenges remain in stabilizing prices and mitigating the impact on consumers and the economy.\n\n\n\n\n\nFeb 3, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nThis project explores how to leverage the strengths of DuckDB and MotherDuck to build a robust data processing and storage solution. DuckDB excels at fast in-memory analytics, while MotherDuck provides a scalable and cost-effective cloud data warehouse. By combining these technologies, you can achieve optimal performance for both local and cloud-based data operations.\n\n\n\n\n\nJan 13, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nData Alchemy, SQL Analysis within Python\n\n\nA Case Study using Duckdb, Polars and Plotly\n\n\n\npython\n\nsql\n\n\n\nThis project focuses on leveraging the strengths of DuckDB, Polars, and Plotly for efficient data analysis and visualization. DuckDB is used for fast in-memory data processing, Polars provides a user-friendly and high-performance DataFrame library, and Plotly offers interactive and customizable visualizations.\n\n\n\n\n\nJan 6, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Analysis in Python\n\n\nA Case Study using Duckdb, Polars and Folium\n\n\n\npython\n\n\n\nGeospatial analysis involves the application of spatial concepts and techniques to data that has geographic coordinates. With the rise of big data and the increasing availability of geospatial information, the demand for effective geospatial analysis tools has grown significantly. Python, with its rich ecosystem of libraries, has emerged as a powerful and popular choice for geospatial data scientists.\n\n\n\n\n\nJan 2, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy BI Tools Fall Short, A Failure to Capture the Office Workflow\n\n\nSlideshows and Spreadsheets Still Rule the Business World\n\n\n\npython\n\n\n\nDespite BI solutions, spreadsheets & slideshows persist in data-driven decision making.\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSports Commerce\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge Your SQL Analysis with Python and DuckDB\n\n\nRethinking Data Analysis: A New Paradigm\n\n\n\npython\n\nsql\n\n\n\nThis article explores the synergy between Python and DuckDB, a powerful in-memory database, to revolutionize SQL-based data analysis. By leveraging Python‚Äôs extensive data science ecosystem and DuckDB‚Äôs lightning-fast query execution, data professionals can significantly accelerate their workflows.\n\n\n\n\n\nNov 23, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid Cases in Americas\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSpotipy, A Python Library for Spotify Music Lovers\n\n\nDiscover your Music Habits with Python\n\n\n\npython\n\n\n\nEver wondered about the intricacies of your listening habits on Spotify? Or perhaps you‚Äôre a developer looking to build a music-related application? Enter Spotipy, a fantastic Python library that acts as a bridge between your code and the Spotify Web API.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFlying into Mexico\n\n\nA Bird‚Äôs-Eye View of Major Airports\n\n\n\npython\n\n\n\nAs a popular tourist destination, Mexico offers visitors a diverse range of experiences, from ancient ruins to pristine beaches. The country‚Äôs well-connected airport system ensures seamless travel to these captivating destinations.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Stunning Charts with Datawrapper and Python\n\n\nAn Overview\n\n\n\npython\n\n\n\nDatawrapper is a powerful online tool designed to help you create engaging and informative data visualizations. Whether you‚Äôre a journalist, researcher, or simply someone who wants to present data in a more visually appealing way, Datawrapper can make the process quick and easy.\n\n\n\n\n\nSep 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City‚Äôs Underground\n\n\nA History of Challenges and Triumphs\n\n\n\npython\n\n\n\nMexico City‚Äôs metro is the largest and busiest in Latin America, serving more than 5.5 million passengers daily in its 195 stations and 12 lines\n\n\n\n\n\nSep 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico‚Äôs Peace index 2024\n\n\n\n\n\n\npython\n\n\n\nThe Mexico Peace Index (MPI), created by the Institute for Economics and Peace (IEP), is a valuable tool for understanding peacefulness in Mexico. According to IEP, Mexico‚Äôs peacefulness has improved for 04 years in a row. However, the situation isn‚Äôt uniform. While 15 states showed improvement, 17 got worse. Drug cartel activity and political violence remain challenges.\n\n\n\n\n\nMay 17, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Data Pipelines with BigQuery and Python\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn the realm of data analytics, Extract, Transform, Load (ETL) processes play a pivotal role. They streamline the integration of data from various sources, enabling its cleaning, manipulation, and loading into target systems like BigQuery, Google‚Äôs cloud-based data warehouse. By leveraging Python‚Äôs versatility and BigQuery‚Äôs scalability, you can construct powerful ETL pipelines to prepare your data for insightful analysis.\n\n\n\n\n\nMay 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nNational Guard‚Äôs Deployed Personnel in Mexico\n\n\n\n\n\n\npython\n\n\n\nMexico‚Äôs National Guard is a relatively new security force established in 2019. It was created to address the country‚Äôs high crime rates and complement traditional law enforcement.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Charts with HoloViews\n\n\nUnveiling Insights with Less Effort by Using HoloViews\n\n\n\npython\n\n\n\nHoloViews offers a compelling alternative for data visualization in Python. With its emphasis on simplicity, flexibility, and interactivity, HoloViews empowers you to create insightful visualizations that effectively communicate your data‚Äôs story.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Dashboards with Python\n\n\nUnleash the Power of Python\n\n\n\npython\n\n\n\nPython, a versatile programming language, empowers you to create interactive dashboards that unlock the hidden potential of your data.\n\n\n\n\n\nApr 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Use Mermaid to Create Diagrams as a Data Scientist\n\n\nUse Python code as your Secret Weapon\n\n\n\npython\n\n\n\nIn this article, we will show how you can create diagrams with code within Jupyter and stop using external diagramming graphical user interfaces (GUIs) like draw.io or Lucidchart.\n\n\n\n\n\nMar 29, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico‚Äôs Political Landscape\n\n\n\n\n\n\npython\n\n\n\nIn this article, we will show an overview of Mexico‚Äôs Political Scenario and its correlation with Mexico City‚Äôs.\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Slides with Python and Quarto\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDuckDB: A Compelling Solution for Data Analysis within Python Environment\n\n\nA valuable tool for data scientists working with Python due to its speed, ease of use, and tight integration.\n\n\n\npython\n\nsql\n\n\n\nDuckDB is a fast, embedded analytical database that outstands in in-memory operations. It provides a SQL interface, making it easy for users with database querying experience.\n\n\n\n\n\nMar 18, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWater Collection System in Mexico City - 2022\n\n\nRainwater harvesting systems are a prominent water collection method in Mexico\n\n\n\npython\n\n\n\nRainwater harvesting is a sustainable and effective way to manage water resources in Mexico. By promoting water conservation and increasing water security, these systems offer a path towards a more water-resilient future.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico: A Populous Nation with a Dynamic Demographic Landscape\n\n\nAn Overview of Mexico‚Äôs Population Dynamics since 1950\n\n\n\npython\n\n\n\nMexico boasts a population of nearly 130 million, ranking it as the 10th most populous country globally. Mexico is a highly urbanized nation, with 88% of the population residing in cities. This trend is expected to continue, driven by economic opportunities and infrastructure development.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating JSON Files with Python\n\n\nAn Overview Using Pandas\n\n\n\npython\n\n\n\nThis article presents a straightforward and efficient method for reading JSON files using Python Pandas library. We will show how to import JSON data into DataFrames, enabling comprehensive analysis and exploration. This approach significantly simplifies the process of working with JSON data, making it accessible for data analysis.\n\n\n\n\n\nMar 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Lego Toys with Polars\n\n\nA quick summary about Lego bricks\n\n\n\npython\n\n\n\nThe Lego brick was invented in 1949 by Ole Kirk Christiansen, and the company has since grown to become one of the world‚Äôs leading toy manufacturers. Lego products are sold in over 140 countries, and the company has over 40,000 employees worldwide.\n\n\n\n\n\nMar 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFederal Prisons in Mexico\n\n\nA Geographical Overview of Mexico‚Äôs Federal Penal System\n\n\n\npython\n\n\n\nMexico‚Äôs federal prisons, known as Ceferesos (Centros Federales de Readaptaci√≥n Social), are a complex system facing challenges. While intended for rehabilitation, reports often highlight overcrowding, violence, and inadequate living conditions. In this article we will show the location of federal prisons in Mexico, including the famous Islas Mar√≠as, which closed in 2019. We wil use Folium, a powerful Python library to enhance data analysis and geographic visualization.\n\n\n\n\n\nMar 6, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression\n\n\nAn Extension for Linear Regression Models\n\n\n\npython\n\n\n\nIn this article, we shall show where linear regression falls short and we should use polynomial regression instead.\n\n\n\n\n\nMar 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Analysis Overview\n\n\nA quick summary about linear and nonlinear regression\n\n\n\npython\n\n\n\nWhether you want to do statistics, machine learning, or economic analysis, it‚Äôs likely that you will have to use regression analysis. Regression analysis is one of the most important fields in statistics, economics and machine learning. We shall briefly explore the different techniques about regression.\n\n\n\n\n\nFeb 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCohort Analysis\n\n\nUnderstanding Customer Behavior\n\n\n\npython\n\n\n\nCohort analysis is an extremely helpful tool that can be used to improve business practices and can effectively increase user retention if businesses implement necessary changes according to the test results. In today‚Äôs world where data is everywhere, cohort analysis is effective in extracting useful information by analyzing the behavioral patterns of customers in order to predict the future of the business. Observing cohorts over time gives insight into user experience and helps in developing better tactics.\n\n\n\n\n\nFeb 9, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the Power of Big Data with PySpark\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn today‚Äôs data-driven world, we‚Äôre constantly bombarded with massive amounts of information. Analyzing this data efficiently and effectively is crucial for businesses and researchers alike. That‚Äôs where PySpark comes in. It‚Äôs a powerful tool that brings the distributed computing capabilities of Apache Spark to the familiar and versatile Python ecosystem.\n\n\n\n\n\nJan 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMissing People in Mexico\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUS Sales Dashboard\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nLooker Studio\n\n\n\n\n\n\nlooker-studio\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nTableau Charts example\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDiving into the World of SQL\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nJesus L. Monroy\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "portfolio/posts/tableau/looker-studio.html",
    "href": "portfolio/posts/tableau/looker-studio.html",
    "title": "Looker Studio",
    "section": "",
    "text": "We used Looker Studio to present results about customer behavior and online sales and conversions from Google Analytics.\nGoogle Analytics is a powerful tool for any business, but it becomes especially crucial for e-commerce businesses. It provides a wealth of data and insights into how customers interact with your online store, allowing you to optimize your website and marketing efforts to drive sales."
  },
  {
    "objectID": "portfolio/posts/tableau/looker-studio.html#summary",
    "href": "portfolio/posts/tableau/looker-studio.html#summary",
    "title": "Looker Studio",
    "section": "",
    "text": "We used Looker Studio to present results about customer behavior and online sales and conversions from Google Analytics.\nGoogle Analytics is a powerful tool for any business, but it becomes especially crucial for e-commerce businesses. It provides a wealth of data and insights into how customers interact with your online store, allowing you to optimize your website and marketing efforts to drive sales."
  },
  {
    "objectID": "portfolio/posts/tableau/looker-studio.html#e-commerce-weekly-dashboard",
    "href": "portfolio/posts/tableau/looker-studio.html#e-commerce-weekly-dashboard",
    "title": "Looker Studio",
    "section": "E-Commerce Weekly Dashboard",
    "text": "E-Commerce Weekly Dashboard\n\nProject Goal: This dashboard was designed to track the principal indicators about e-commerce from websites and mobile apps.\nData Sources: The data mainly come from Google Analytics tracking websites and mobile apps.\nKey Metrics: The dashboard shows users, new users, pageviews, sessions, orders, revenue and conversion rates.\nVisualizations: The types of charts used are time series for behavior, KPI cards, bar charts for categories and tables for detailed information.\nTarget Audience: The dashboard was designed for marketing managers and their teams."
  },
  {
    "objectID": "portfolio/posts/tableau/looker-studio.html#e-commerce-weekly-report",
    "href": "portfolio/posts/tableau/looker-studio.html#e-commerce-weekly-report",
    "title": "Looker Studio",
    "section": "E-Commerce Weekly Report",
    "text": "E-Commerce Weekly Report\n\nProject Goal: This report was designed to track the principal indicators about e-commerce from websites and mobile apps.\nData Sources: The data mainly come from Google Analytics tracking websites and mobile apps.\nKey Metrics: The report shows users, new users, pageviews, sessions, orders, revenue and conversion rates by year and month.\nVisualizations: The types of charts used are sparklines for behavior of the different metrics.\nTarget Audience: The report was designed for the marketing team for operational monitoring."
  },
  {
    "objectID": "portfolio/posts/tableau/looker-studio.html#references",
    "href": "portfolio/posts/tableau/looker-studio.html#references",
    "title": "Looker Studio",
    "section": "References",
    "text": "References\n\nAdvantages of Looker Studio"
  },
  {
    "objectID": "portfolio/posts/tableau/looker-studio.html#contact",
    "href": "portfolio/posts/tableau/looker-studio.html#contact",
    "title": "Looker Studio",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/tableau/sports.html",
    "href": "portfolio/posts/tableau/sports.html",
    "title": "Sports Commerce",
    "section": "",
    "text": "This dashboard was designed to show sales by store and time intervals.\n\nData Sources: The data mainly come from sales department.\nKey Metrics: The dashboard shows sales by stores, quantities of items sold and sales by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories.\nTarget Audience: The dashboard was designed for sales department team with a sales target parameter to analyze which stores exceeds the target and which keeps below."
  },
  {
    "objectID": "portfolio/posts/tableau/sports.html#overview",
    "href": "portfolio/posts/tableau/sports.html#overview",
    "title": "Sports Commerce",
    "section": "",
    "text": "This dashboard was designed to show sales by store and time intervals.\n\nData Sources: The data mainly come from sales department.\nKey Metrics: The dashboard shows sales by stores, quantities of items sold and sales by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories.\nTarget Audience: The dashboard was designed for sales department team with a sales target parameter to analyze which stores exceeds the target and which keeps below."
  },
  {
    "objectID": "portfolio/posts/tableau/sports.html#contact",
    "href": "portfolio/posts/tableau/sports.html#contact",
    "title": "Sports Commerce",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html",
    "href": "portfolio/posts/tableau/sundries.html",
    "title": "Tableau Charts example",
    "section": "",
    "text": "In this section we present some examples of charts created with Tableau.\nKeep in mind that dashboards are instrinsically ‚Äúexploratory‚Äù, i.e.¬†you need to¬†actively explore and interact by yourself with dashboards. There is no guided path."
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#summary",
    "href": "portfolio/posts/tableau/sundries.html#summary",
    "title": "Tableau Charts example",
    "section": "",
    "text": "In this section we present some examples of charts created with Tableau.\nKeep in mind that dashboards are instrinsically ‚Äúexploratory‚Äù, i.e.¬†you need to¬†actively explore and interact by yourself with dashboards. There is no guided path."
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#dynamics-of-personnel-deployed-in-mexico",
    "href": "portfolio/posts/tableau/sundries.html#dynamics-of-personnel-deployed-in-mexico",
    "title": "Tableau Charts example",
    "section": "Dynamics of Personnel Deployed in Mexico",
    "text": "Dynamics of Personnel Deployed in Mexico\n\n\n\n\n\n\n\nNote\n\n\n\nClick below to wath the video"
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#example-of-cloud-chart",
    "href": "portfolio/posts/tableau/sundries.html#example-of-cloud-chart",
    "title": "Tableau Charts example",
    "section": "Example of cloud chart",
    "text": "Example of cloud chart"
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#profits-by-year-and-category",
    "href": "portfolio/posts/tableau/sundries.html#profits-by-year-and-category",
    "title": "Tableau Charts example",
    "section": "Profits by year and category",
    "text": "Profits by year and category"
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#profitloss-by-sub-category",
    "href": "portfolio/posts/tableau/sundries.html#profitloss-by-sub-category",
    "title": "Tableau Charts example",
    "section": "Profit/Loss by sub-category",
    "text": "Profit/Loss by sub-category"
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#profitloss-in-function-of-minimum-expected-level-by-category",
    "href": "portfolio/posts/tableau/sundries.html#profitloss-in-function-of-minimum-expected-level-by-category",
    "title": "Tableau Charts example",
    "section": "Profit/Loss in function of minimum expected level by category",
    "text": "Profit/Loss in function of minimum expected level by category"
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#identification-of-products-by-profit-or-loss-contribution",
    "href": "portfolio/posts/tableau/sundries.html#identification-of-products-by-profit-or-loss-contribution",
    "title": "Tableau Charts example",
    "section": "Identification of products by profit or loss contribution",
    "text": "Identification of products by profit or loss contribution"
  },
  {
    "objectID": "portfolio/posts/tableau/sundries.html#contact",
    "href": "portfolio/posts/tableau/sundries.html#contact",
    "title": "Tableau Charts example",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/tableau/us-sales.html",
    "href": "portfolio/posts/tableau/us-sales.html",
    "title": "US Sales Dashboard",
    "section": "",
    "text": "This dashboard was designed to track sales, profits and customers by categories, months and locations.\n\nData Sources: The data mainly come from the sales department with a detailed information about sales, customers and profits categories.\nKey Metrics: The dashboard shows sales, profit and customers KPIs.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for sales managers and their teams."
  },
  {
    "objectID": "portfolio/posts/tableau/us-sales.html#overview",
    "href": "portfolio/posts/tableau/us-sales.html#overview",
    "title": "US Sales Dashboard",
    "section": "",
    "text": "This dashboard was designed to track sales, profits and customers by categories, months and locations.\n\nData Sources: The data mainly come from the sales department with a detailed information about sales, customers and profits categories.\nKey Metrics: The dashboard shows sales, profit and customers KPIs.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for sales managers and their teams."
  },
  {
    "objectID": "portfolio/posts/tableau/us-sales.html#contact",
    "href": "portfolio/posts/tableau/us-sales.html#contact",
    "title": "US Sales Dashboard",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/big-query.html#extract-phase",
    "href": "portfolio/posts/python/big-query.html#extract-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Extract Phase",
    "text": "Extract Phase\nThe extraction phase entails retrieving data from your source. This may involve interacting with:\n\nFlat files\nDatabases\nXML files\nAPIs\nOther\n\n\nFlat files\n\n\nCode\nusers = (\n    pl.read_csv('users.csv', dtypes={'phone': pl.Utf8, 'id_atg':pl.Utf8})\n    .with_columns(\n        pl.col('entry_data').str.strptime(pl.Datetime, strict=False)\n    )\n)\n\n\n\n\nCode\nusers_profiling = (\n    # read csv file\n    pl.read_csv('profiles.csv', dtypes={'contact_phone': pl.Utf8,'post_code':pl.Utf8})\n        # change column dtypes\n    .with_columns(\n        pl.col('entry_data','entry_data_gep','update_date').str.strptime(pl.Datetime, strict=False),\n        pl.col('contact_phone').cast(pl.Utf8),\n    )\n)\n\n\n\n\nCode\norders = (\n    pl.read_csv('orders.csv').with_columns(\n        pl.col('entry_date','delivery_date').str.strptime(pl.Datetime, strict=False)\n    )\n)\n\n\n\n\nCode\norder_details = (\n    pl.read_csv('order-details.csv')\n)\n\n\n\n\nCode\npromotions = (\n    pl.read_csv('promotions.csv', dtypes={'short_description':pl.Utf8})\n    .with_columns(pl.col('key').cast(pl.Utf8))\n).unique(subset='key')\n\n\n\n\nCode\norder_status = pl.read_csv('order-status.csv')\n\n\n\n\nCode\nsocial_networks = (\n    pl.read_csv('social_networks.csv').select(\n        pl.col('id_social_network','social_network','description')\n    )\n)\n\n\n\n\nParquet files\n\n\nCode\ntypes = pl.read_parquet('types.parquet')\n\n\n\n\nCode\ngenre = pl.read_parquet('genre.parquet')\n\n\n\n\njson files\n\n\nCode\nwarehouses = pl.read_json('warehuse_catalog.json')\n\n\n\n\nDatabases\n\n\nCode\n# Connection to MS Access\nconn = pyodbc.connect(r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'r'DBQ=C:\\Users\\user\\folder\\file.accdb;')\n# Create cursor\ncursor = conn.cursor()\n\n\n\n\nCode\n# Write query\nquery = 'select * from sales where year=2022'\n# Convert to pandas dataframe\ndf = pd.read_sql(query, con=conn)\ndf.head()"
  },
  {
    "objectID": "portfolio/posts/python/big-query.html#transform-phase",
    "href": "portfolio/posts/python/big-query.html#transform-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Transform Phase",
    "text": "Transform Phase\nCleanse, validate, and manipulate the extracted data based on your analysis requirements. This might include:\n\nData Cleaning\n\nHandle missing values, inconsistent formatting, or errors.\n\nData Type Conversion\n\nEnsure consistent data types for columns based on their intended use in BigQuery.\n\nFiltering/Aggregation\n\nSelect or aggregate specific data subsets for targeted analysis.\n\nEnrichment\n\nMerge extracted data with additional sources to enhance its value.\n\n\nCode\n# join types, orders, order_details, promotions and warehouses\nsheet = (\n    types.join(orders, on='id_type', how='left')\n    .join(order_details, on='id_order', how='left')\n    .join(promotions, on='key', how='left')\n    .join(warehouses, on='id_warehouse', how='left')\n).rename({'id_warehouse':'id_warehouse_promo', 'active':'promo_active'})"
  },
  {
    "objectID": "portfolio/posts/python/big-query.html#load-phase",
    "href": "portfolio/posts/python/big-query.html#load-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Load Phase",
    "text": "Load Phase\nThere are two primary options for loading data:\n\nStaging Table\n\nCreate a staging table and load the transformed data into it for temporary storage before validating and potentially modifying it:\n\nDirect Load\n\nLoad the data directly into your target table, bypassing the staging step. However, this approach can be less flexible for complex transformations:\n\n\nCode\n# create dataset\nclient.create_dataset('database')\n\n\nDataset(DatasetReference('gepp-538', 'database'))\n\n\n\n\nCode\n# convert to pandas\nsheet = sheet.to_pandas()\n# upload to big query\nsheet.to_gbq('dw.transformation.catalog',\n                    project_id='repository-538',\n                    if_exists='replace',\n                    credentials=bq_credentials)\n\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00&lt;00:00, 7626.01it/s]"
  },
  {
    "objectID": "portfolio/posts/python/big-query.html#execute-queries-from-big-query",
    "href": "portfolio/posts/python/big-query.html#execute-queries-from-big-query",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Execute queries from Big Query",
    "text": "Execute queries from Big Query\n\n\nCode\n# create sql query\nquery = '''\n    SELECT *\n    FROM `dw.transformation.catalog`\n'''\n# convert query to pandas dataframe\ncatalog = pd.read_gbq(query, credentials=bq_credentials)"
  },
  {
    "objectID": "portfolio/posts/python/big-query.html#contact",
    "href": "portfolio/posts/python/big-query.html#contact",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/ceferesos.html",
    "href": "portfolio/posts/python/ceferesos.html",
    "title": "Federal Prisons in Mexico",
    "section": "",
    "text": "Figure¬†1: Cefereso No.¬†12 Vehicule Gate"
  },
  {
    "objectID": "portfolio/posts/python/ceferesos.html#main-functions",
    "href": "portfolio/posts/python/ceferesos.html#main-functions",
    "title": "Federal Prisons in Mexico",
    "section": "Main Functions",
    "text": "Main Functions\n\nAdministration of the Federal Penitentiary System\nResponsible for the management and operation of federal social rehabilitation centers (CEFERESOS) throughout the country.\nImplementation of social reintegration policies\nDevelops and implements programs and strategies to facilitate the reintegration of inmates into society through educational, employment, sports, and health activities.\nGuarantee of Human Rights\nEnsures respect for the human rights of persons deprived of their liberty, ensuring decent living conditions in prisons.\nCrime Prevention\nContributes to crime prevention through the implementation of effective social reintegration programs and collaboration with other public security institutions."
  },
  {
    "objectID": "portfolio/posts/python/ceferesos.html#importance",
    "href": "portfolio/posts/python/ceferesos.html#importance",
    "title": "Federal Prisons in Mexico",
    "section": "Importance",
    "text": "Importance\nThe OADPRS plays a fundamental role in the criminal justice system, as social rehabilitation is a key element in reducing recidivism and strengthening public security. Its work is essential to ensuring that persons deprived of their liberty have the opportunity to reintegrate into society in a productive and law-abiding manner. The work of the OADPRS is monitored by the National Human Rights Commission to ensure that inmates‚Äô rights are respected."
  },
  {
    "objectID": "portfolio/posts/python/ceferesos.html#challenges",
    "href": "portfolio/posts/python/ceferesos.html#challenges",
    "title": "Federal Prisons in Mexico",
    "section": "Challenges",
    "text": "Challenges\nThe OADPRS faces significant challenges, such as overcrowding in some prisons, the need to improve inmates‚Äô living conditions, and the fight against corruption. In addition, the CNDH has issued various recommendations stemming from the lack of timely medical care within penitentiary centers.\nIn short, the OADPRS is a crucial institution for public safety and criminal justice in Mexico, responsible for administering the federal penitentiary system and promoting the social reintegration of inmates."
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html",
    "href": "portfolio/posts/python/gdrive_project.html",
    "title": "Stop Building Dashboards",
    "section": "",
    "text": "Figure¬†1: ETL Phases"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#overview",
    "href": "portfolio/posts/python/gdrive_project.html#overview",
    "title": "Stop Building Dashboards",
    "section": "Overview",
    "text": "Overview\nSome reasons why spreadsheets and slideshows persist in the office workflow, include:\n\nFamiliarity and Ease of Use\nFlexibility and Control\nStorytelling and Communication\nCollaboration and Sharing\nAd-hoc Analysis and Exploration\nCost and Accessibility\n\nWhile BI tools may be transforming the way we analyze data, it‚Äôs clear that spreadsheets and slideshows aren‚Äôt going anywhere anytime soon. They serve a different purpose, filling a gap that BI tools often miss.\nBuilding a Python-Powered Data Pipeline\nBusiness Intelligence (BI) tools are powerful, but they can also be expensive and complex. What if you could build a custom, flexible, and potentially more cost-effective solution using Python?\nThis post explores how you can leverage Python to collect, transform, and deliver data to Spreadsheets and Slides for compelling presentations. Why Python?\nPython has become a powerhouse in data science and automation. Its rich ecosystem of libraries makes it ideal for data manipulation, and seamless slides interaction. This combination offers a powerful alternative to traditional BI tools for certain use cases.\nIn this post we shall propose the use of Python (to collect, cleanse and transform data), Google Spreadsheets (to store transformed data) and Google Slides (to showcase visualizations). Proposed Workflow\nImagine you need to generate a weekly sales report and all you have to do is to run the next command:\n%%bash\njupyter-execute ./projects/weekly-report.ipynb\nAnd, voila! you have your weekly report updated and ready to present in Google Slides."
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#environment-settings",
    "href": "portfolio/posts/python/gdrive_project.html#environment-settings",
    "title": "Stop Building Dashboards",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nShow code\n# Import authenticator and gspread to manage g-sheets\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\n\n# Import other libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow code\n# get token\nfilename = 'credentials.json'\n\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n\n# read credentials\ntoken = keys['md_token']"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#extract-phase",
    "href": "portfolio/posts/python/gdrive_project.html#extract-phase",
    "title": "Stop Building Dashboards",
    "section": "Extract Phase",
    "text": "Extract Phase\n\n\nShow code\n# connect to motherduck cloud\nconn = db.connect(f'md:?motherduck_token={token}')\n\n\n\n\nShow code\nconn.sql('show databases')\n\n\n\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     database_name     ‚îÇ\n‚îÇ        varchar        ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ md_information_schema ‚îÇ\n‚îÇ my_db                 ‚îÇ\n‚îÇ my_portfolio          ‚îÇ\n‚îÇ sample_data           ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\nTable¬†1: Databases\n\n\n\n\n\n\nShow code\n# select specific database\nconn.sql('use my_portfolio')\n\n\n\n\nShow code\n# show tables in database\nconn.sql('show tables')\n\n\n\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       name       ‚îÇ\n‚îÇ     varchar      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ airports         ‚îÇ\n‚îÇ appl_stock       ‚îÇ\n‚îÇ cdmx_subway      ‚îÇ\n‚îÇ colors           ‚îÇ\n‚îÇ contains_null    ‚îÇ\n‚îÇ houses           ‚îÇ\n‚îÇ people           ‚îÇ\n‚îÇ prevalencia      ‚îÇ\n‚îÇ restaurants      ‚îÇ\n‚îÇ retail_sales     ‚îÇ\n‚îÇ sales            ‚îÇ\n‚îÇ sales_info       ‚îÇ\n‚îÇ sets             ‚îÇ\n‚îÇ water_collection ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     14 rows      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\nTable¬†2: Tables in database\n\n\n\n\n\n\nShow code\n# dataset\ndataset = conn.sql('select * from restaurants').df()\n\n(\n    dataset.head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nGreek Gyros\n770\n$68.39\nNew York\nFrench\n1\n\n\nTaste of Italy\n4,420\n$88.23\nAmsterdam\nChinese\n0\n\n\nMidnight Diner\n2,155\n$12.97\nLisbon\nMexican\n1\n\n\nTaste of Italy\n3,375\n$52.79\nSydney\nChinese\n1\n\n\n\n\n\n\nTable¬†3: Dataset Preview"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#transform-phase",
    "href": "portfolio/posts/python/gdrive_project.html#transform-phase",
    "title": "Stop Building Dashboards",
    "section": "Transform Phase",
    "text": "Transform Phase\n\nWhich restaurant chain has the maximum number of restaurants?\n\n\nShow code\nchains = (\n    conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n    ''').df()\n)\nchains\n\n\n\n\n\n\n\n\n\n\n\n\nname\nno_of_chains\n\n\n\n\n0\nThe Burger Joint\n721\n\n\n1\nPizza Palace\n703\n\n\n2\nGreek Gyros\n696\n\n\n3\nCafe Delight\n692\n\n\n4\nFrench Delights\n681\n\n\n5\nThe BBQ Shack\n671\n\n\n6\nThe Golden Wok\n667\n\n\n7\nOcean Breeze\n665\n\n\n8\nSpice & Bloom\n665\n\n\n9\nMidnight Diner\n657\n\n\n\n\n\n\n\n\nTable¬†4: Data grouped by restaurant chains\n\n\n\n\n\n\nWhich restaurant chain has generated maximum revenue?\n\n\nShow code\nrevenue = (\n    conn.sql('''\n    select name, sum(rating_count * cost) as revenue\n    from restaurants\n    group by name\n    order by revenue DESC\n    limit 10\n    ''').df()\n)\n\n(\n    revenue\n        .style\n        .hide()    \n        .format({'revenue': '{:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrevenue\n\n\n\n\nThe Burger Joint\n108,820,424.64\n\n\nPizza Palace\n100,382,853.92\n\n\nCafe Delight\n98,037,063.93\n\n\nGreek Gyros\n96,403,445.25\n\n\nThe BBQ Shack\n96,286,414.02\n\n\nOcean Breeze\n95,664,612.77\n\n\nThe Golden Wok\n94,860,125.75\n\n\nSpice & Bloom\n91,824,854.56\n\n\nFrench Delights\n91,236,701.38\n\n\nMidnight Diner\n91,170,162.30\n\n\n\n\n\n\nTable¬†5: Data grouped by restaurant and revenue\n\n\n\n\n\n\nWhich city has generated maximum revenue?\n\n\nShow code\ncities = (\n    conn.sql('''\n    select city, sum(rating_count * cost) as revenue\n    from restaurants\n    group by city\n    order by revenue DESC\n    limit 10\n    ''').df()\n)\n\n(\n    cities\n        .style\n        .hide()    \n        .format({'revenue': '${:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nrevenue\n\n\n\n\nAmsterdam\n$148,839,878.62\n\n\nTokyo\n$148,035,421.32\n\n\nMadrid\n$141,487,618.97\n\n\nParis\n$141,219,374.56\n\n\nLondon\n$140,876,613.54\n\n\nRome\n$139,622,129.63\n\n\nNew York\n$138,621,609.28\n\n\nLisbon\n$136,814,247.27\n\n\nBerlin\n$136,434,163.25\n\n\nSydney\n$131,656,513.97\n\n\n\n\n\n\nTable¬†6: Data grouped by city and revenue"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#load-phase",
    "href": "portfolio/posts/python/gdrive_project.html#load-phase",
    "title": "Stop Building Dashboards",
    "section": "Load Phase",
    "text": "Load Phase\n\n\nShow code\n# Create scope to authenticate\nSCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n\n# Read credentials\nGOOGLE_SHEETS_KEY_FILE = 'arkham-538.json'\ncredentials = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_SHEETS_KEY_FILE, SCOPES)\ngc = gspread.authorize(credentials)\n\n\n\n\nShow code\nimport pytz\nimport datetime\n\ntz = pytz.timezone('America/Mexico_City')\nupdate = datetime.datetime.now(tz).strftime('%b %d, %Y')\nperiod = update\n\n\n\n\nShow code\ndef save_to_gsheets(df, sheet_name, worksheet_name, period):\n    creds = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_SHEETS_KEY_FILE, SCOPES)\n    client = gspread.authorize(creds)\n    sheet = client.open(sheet_name)\n    worksheet = sheet.worksheet(worksheet_name)\n\n    # Convert datetimes to strings in advance\n    for column in df.columns[df.dtypes == 'datetime64[ns]']:\n        df[column] = df[column].astype(str)\n\n    # Prepare data for batch update\n    data = [df.columns.values.tolist()] + df.fillna('').values.tolist()\n\n    # Freeze rows and update cell values with a single batch update\n    worksheet.freeze(4)\n    worksheet.update('A4:M', data)\n\n    #fija fecha de consulta o actualizacion\n    update_data = {\n    'Last update': [\n        period,]\n    }\n\n    # convert to dataframe\n    update_data = pd.DataFrame(update_data, columns=['Last update'])\n\n    worksheet.update([update_data.columns.values.tolist()] + update_data.fillna('').values.tolist(),'A1:A2',)\n\n    print(f'DataFrame uploaded to: workbook: {sheet_name}, sheet: {worksheet_name}')\n\n\n\n\nShow code\nsave_to_gsheets(dataset, 'restaurants', 'data', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: data\n\n\n\n\nShow code\nsave_to_gsheets(chains, 'restaurants', 'chains', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: chains\n\n\n\n\nShow code\nsave_to_gsheets(revenue, 'restaurants', 'revenue', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: revenue\n\n\n\n\nShow code\nsave_to_gsheets(cities, 'restaurants', 'cities', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: cities"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#close-connection",
    "href": "portfolio/posts/python/gdrive_project.html#close-connection",
    "title": "Stop Building Dashboards",
    "section": "Close connection",
    "text": "Close connection\n\n\nShow code\n# close connection\nconn.close()"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#retrieve-data-from-gsheets",
    "href": "portfolio/posts/python/gdrive_project.html#retrieve-data-from-gsheets",
    "title": "Stop Building Dashboards",
    "section": "Retrieve data from gsheets",
    "text": "Retrieve data from gsheets\n\n\nShow code\n# Access worksheet id\ndf_id = '1JNAWb2QkFwh61v7QwEEVZnNhTPS0csbdMdll9y1csEg'\ndf_workbook = gc.open_by_key(df_id)\n# Access data by worksheet sheet\ndf = df_workbook.worksheet('data')\n# Save data to table\ndf = df.get_all_values()\n# Save accessed data from google sheets to dataframe\ndf = pd.DataFrame(df[1:], columns=df[0])\n\n\n\n\nShow code\ndf.head()\n\n\n\n\n\n\n\n\n\n\n\n\nLast update\n\n\n\n\n\n\n\n\n\n0\nFeb 24, 2025\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n3\nThe Golden Wok\n1477\n33.62048759\nBerlin\nAmerican\n5\n\n\n4\nGreek Gyros\n770\n68.38887409\nNew York\nFrench\n1\n\n\n\n\n\n\n\n\nTable¬†7: Data Saved on Gogle Sheets"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#google-sheets-report-data",
    "href": "portfolio/posts/python/gdrive_project.html#google-sheets-report-data",
    "title": "Stop Building Dashboards",
    "section": "Google Sheets Report Data",
    "text": "Google Sheets Report Data\n\n\n\n\n\n\nFigure¬†2: Google Sheets Data for Presentation Report"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#sync-between-google-sheets-and-google-slides",
    "href": "portfolio/posts/python/gdrive_project.html#sync-between-google-sheets-and-google-slides",
    "title": "Stop Building Dashboards",
    "section": "Sync between Google Sheets and Google Slides",
    "text": "Sync between Google Sheets and Google Slides\nSimply we copy and paste with sync for each table and chart and customize our slides.\n\n\n\n\n\n\nFigure¬†3: Synchronization between Google Sheets and Slides\n\n\n\n\n\n\n\n\n\nGoogle Slides\n\n\n\nYou can see the final report on Google Slides"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#conclusions",
    "href": "portfolio/posts/python/gdrive_project.html#conclusions",
    "title": "Stop Building Dashboards",
    "section": "Conclusions",
    "text": "Conclusions\nWhile BI tools are valuable, Python offers a compelling alternative for building custom data pipelines. By leveraging the power of Python using polars and duckdb libraries for data collection and transformation, and libraries like plotly for visualization you can create a flexible, cost-effective, and automated solution for delivering data to Google Spreadsheets, using gspread, and Google Slides for impactful presentations, by sync between these Google apps.\nThis approach empowers you to take control of your data and create highly tailored reporting solutions by replacing BI license costs."
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#references",
    "href": "portfolio/posts/python/gdrive_project.html#references",
    "title": "Stop Building Dashboards",
    "section": "References",
    "text": "References\n\nBusiness (2023) How to Design a Dashboard Presentation: A Step-by-Step Guide in slidemodel.com\nKarlson, P. (2022) Are Spreadsheets Secretly Running Your Business? in Forbes\nMonroy, Jesus (2024) Why BI Tools Fall Short: PowerPoint and Excel Still Rule the Business World in Medium\nMoore J. (2024) But, Can I Export it to Excel? in Do Mo(o)re with Data\nSchwab, P. (2021) Excel dominates the business world‚Ä¶ and that‚Äôs not about to change in Into the Minds"
  },
  {
    "objectID": "portfolio/posts/python/gdrive_project.html#contact",
    "href": "portfolio/posts/python/gdrive_project.html#contact",
    "title": "Stop Building Dashboards",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html",
    "href": "portfolio/posts/python/crimes-web.html",
    "title": "Mexico City Crime According to the Statistics",
    "section": "",
    "text": "Figure¬†1: Mexico City Crimes"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#import-libraries",
    "href": "portfolio/posts/python/crimes-web.html#import-libraries",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Import libraries",
    "text": "Import libraries\n\n\nShow code\n#import libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport datetime as datetime\nimport json\nimport plotly.express as px\nimport folium\nfrom folium.plugins import Fullscreen"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#database-connection",
    "href": "portfolio/posts/python/crimes-web.html#database-connection",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Database connection",
    "text": "Database connection\n\n\nShow code\n# get token\nfilename = 'credentials.json'\n\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n\n# read credentials\ntoken = keys['md_token']\n\n\nDuckdb is a powerful tool for data analysts and developers who need to perform fast and efficient analytical queries on large datasets, especially in environments where simplicity and portability are crucial.\n\n\nShow code\n# connect to motherduck cloud\nconn = db.connect(f'md:?motherduck_token={token}')\n\n\n\n\nShow code\n# retrieve dataframe\ndf = conn.execute('select * from projects.cdmx_fgj_uncleaned').pl()\n\n\n\n\nShow code\n# close connection\nconn.close()\n\n\n\n\nShow code\nprint(f'The dataset contains {df.shape[0]:,.0f} rows')\n\n\nThe dataset contains 1,415,763 rows"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#missing-values",
    "href": "portfolio/posts/python/crimes-web.html#missing-values",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Missing values",
    "text": "Missing values\n\n\nShow code\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1415763 entries, 0 to 1415762\nData columns (total 22 columns):\n #   Column             Non-Null Count    Dtype  \n---  ------             --------------    -----  \n 0   anio_inicio        1415748 non-null  float64\n 1   mes_inicio         1415748 non-null  object \n 2   fecha_inicio       1415748 non-null  object \n 3   hora_inicio        1415748 non-null  object \n 4   anio_hecho         1415343 non-null  float64\n 5   mes_hecho          1415343 non-null  object \n 6   fecha_hecho        1415342 non-null  object \n 7   hora_hecho         1415353 non-null  object \n 8   delito             1415749 non-null  object \n 9   categoria_delito   1415749 non-null  object \n 10  sexo               1168059 non-null  object \n 11  edad               931053 non-null   float64\n 12  tipo_persona       1408196 non-null  object \n 13  calidad_juridica   1415748 non-null  object \n 14  competencia        1415749 non-null  object \n 15  colonia_hecho      1340796 non-null  object \n 16  colonia_catalogo   1323317 non-null  object \n 17  alcaldia_hecho     1413284 non-null  object \n 18  alcaldia_catalogo  1372214 non-null  object \n 19  municipio_hecho    1413284 non-null  object \n 20  latitud            1340994 non-null  float64\n 21  longitud           1340994 non-null  float64\ndtypes: float64(5), object(17)\nmemory usage: 237.6+ MB\n\n\n\n\nShow code\ndf = df.dropna(subset=['latitud'])\n\n\n\n\nShow code\nprint(f'The dataset now has {df.shape[0]:,.0f} rows')\n\n\nThe dataset now has 1,340,994 rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the original dataset includes 1,415,763 rows.\nHowever, there are many null values in different fields for considerable gaps.\nAs the dataset can be considered large, we decided to drop all null latitude rows, obtaining 1,340,994 rows.\n\n\n\n\nDue to the missing rows in many fields, we kept around with 94.72% of the original dataset"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#polars-dataframe",
    "href": "portfolio/posts/python/crimes-web.html#polars-dataframe",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Polars dataframe",
    "text": "Polars dataframe\nPolars is a modern DataFrame library that prioritizes performance and efficiency. Its Rust-based architecture, combined with features like lazy evaluation and parallel processing, makes it a powerful tool for data professionals.\n\n\nShow code\n# convert to polars dataframe\ndf = pl.from_pandas(df)\n\n\n\n\nShow code\ndf.schema\n\n\nSchema([('anio_inicio', Float64),\n        ('mes_inicio', String),\n        ('fecha_inicio', String),\n        ('hora_inicio', String),\n        ('anio_hecho', Float64),\n        ('mes_hecho', String),\n        ('fecha_hecho', String),\n        ('hora_hecho', String),\n        ('delito', String),\n        ('categoria_delito', String),\n        ('sexo', String),\n        ('edad', Float64),\n        ('tipo_persona', String),\n        ('calidad_juridica', String),\n        ('competencia', String),\n        ('colonia_hecho', String),\n        ('colonia_catalogo', String),\n        ('alcaldia_hecho', String),\n        ('alcaldia_catalogo', String),\n        ('municipio_hecho', String),\n        ('latitud', Float64),\n        ('longitud', Float64)])"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#data-cleansing",
    "href": "portfolio/posts/python/crimes-web.html#data-cleansing",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Data cleansing",
    "text": "Data cleansing\n\n\nShow code\ndf = (\n    df.filter(\n    # drop null and 2018 years\n        (pl.col('anio_inicio')!=2018)\n    ).with_columns(\n    # create datetime field\n        (pl.col('fecha_inicio').cast(pl.String) + ' ' + pl.col('hora_inicio')\n             .cast(pl.String)).alias('fecha_inicio')\n    ).select(\n    # exclude columns\n        pl.exclude('anio_inicio','mes_inicio','hora_inicio',\n                   'anio_hecho','mes_hecho','fecha_hecho','hora_hecho')\n    ).with_columns(\n        fecha_inicio=pl.col('fecha_inicio').str.to_datetime()\n    ).drop_nulls(subset='fecha_inicio')\n)\n\n\n\n\nShow code\ndf.select(pl.all().is_null().sum()).to_dicts()\n\n\n[{'fecha_inicio': 0,\n  'delito': 0,\n  'categoria_delito': 0,\n  'sexo': 237070,\n  'edad': 449780,\n  'tipo_persona': 6858,\n  'calidad_juridica': 1,\n  'competencia': 0,\n  'colonia_hecho': 382,\n  'colonia_catalogo': 17677,\n  'alcaldia_hecho': 3,\n  'alcaldia_catalogo': 953,\n  'municipio_hecho': 3,\n  'latitud': 0,\n  'longitud': 0}]\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that even if we dropped around 75,000 rows, there continue to be many fields with empty rows, such as sex, age, neighborhood, mayorship and municipality."
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#data-cleaning-for-age",
    "href": "portfolio/posts/python/crimes-web.html#data-cleaning-for-age",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Data cleaning for age",
    "text": "Data cleaning for age\n\n\nAge goes from 0.0 up to 369.0 years old!\n\n\n\n\nShow code\n# case for age\ndf = df.with_columns(\n    pl.when(pl.col('edad') &lt; 18).then(18)\n    .when(pl.col('edad')&gt;99).then(99)\n    .otherwise('edad')\n    .alias('edad')\n)\n\n\nWe have cleaned age values by setting age less than 18 to 18, and age values gretar than 99 to 99."
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#accrued-crimes-by-year",
    "href": "portfolio/posts/python/crimes-web.html#accrued-crimes-by-year",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Accrued crimes by year",
    "text": "Accrued crimes by year\n\n\nShow code\nyears = (\n    df.sort('fecha_inicio')\n        .group_by_dynamic('fecha_inicio', every='1y')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n(\n    years\n        .to_pandas()\n        .style.format(\n            {\n            'fecha_inicio':'{:%Y}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nfecha_inicio\ncrimes\n\n\n\n\n2019\n256,827\n\n\n2020\n204,659\n\n\n2021\n228,627\n\n\n2022\n237,659\n\n\n2023\n239,402\n\n\n2024\n173,803\n\n\n\n\n\n\nTable¬†1: Accrued Crimes in Mexico City by Year\n\n\n\n\n\n\nShow code\nfig = px.bar(years, \n             x='fecha_inicio',\n             y='crimes',\n             orientation='v',\n             hover_data=['fecha_inicio','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Year',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Crimes')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†2: Crimes in Mexico City by Year"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#time-behavior-of-crimes",
    "href": "portfolio/posts/python/crimes-web.html#time-behavior-of-crimes",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Time behavior of crimes",
    "text": "Time behavior of crimes\n\n\nShow code\nmonths = (\n    df\n        .sort('fecha_inicio', descending=False)\n        .group_by_dynamic('fecha_inicio', every='1mo')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n(\n    months\n        .to_pandas()\n        .tail(10)\n        .style.format(\n            {\n            'fecha_inicio':'{:%b, %Y}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nfecha_inicio\ncrimes\n\n\n\n\nDec, 2023\n17,358\n\n\nJan, 2024\n18,354\n\n\nFeb, 2024\n18,750\n\n\nMar, 2024\n19,797\n\n\nApr, 2024\n19,923\n\n\nMay, 2024\n20,879\n\n\nJun, 2024\n19,209\n\n\nJul, 2024\n19,429\n\n\nAug, 2024\n19,149\n\n\nSep, 2024\n18,313\n\n\n\n\n\n\nTable¬†2: Accrued Crimes in Mexico City by Month\n\n\n\n\n\n\nShow code\nfig = px.line(months, \n             x='fecha_inicio',\n             y='crimes',\n             hover_data=['fecha_inicio','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City 2019-2024',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Crimes')),\n    )\n\nfig.update_traces(line_color='#7f0000',\n                  line={'width':3},\n                  )\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†3: Time Behavior of Crimes in Mexico City"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#crimes-by-sex",
    "href": "portfolio/posts/python/crimes-web.html#crimes-by-sex",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by sex",
    "text": "Crimes by sex\n\n\nShow code\ndf_sex = (\n    df.group_by('sexo')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n# convert to pandas\ndf_sex = df_sex.to_pandas()\n# rename row\ndf_sex['sexo'] = df_sex['sexo'].replace({None:'NA'})\n\n\n\n\nShow code\n(\n    df_sex\n        .sort_values('crimes')\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nsexo\ncrimes\n\n\n\n\nNA\n237,070\n\n\nFemenino\n535,960\n\n\nMasculino\n567,947\n\n\n\n\n\n\nTable¬†3: Accrued Crimes in Mexico City by Sex\n\n\n\n\n\n\nShow code\nfig = px.bar(df_sex.sort_values('crimes'),\n             y='sexo',\n             x='crimes',\n             orientation='h',\n             hover_data=['sexo','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Sex',\n             template='ggplot2',\n             text='crimes',\n             )\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Sex')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†4: Accrued Crimes in Mexico City by Sex"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#crimes-by-age",
    "href": "portfolio/posts/python/crimes-web.html#crimes-by-age",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by age",
    "text": "Crimes by age\n\n\nShow code\ndf_edad = (\n    df.group_by('edad')\n        .agg(pl.len().alias('crimes'))\n        .sort('edad')\n        .drop_nulls()\n)\n\n\n\n\nShow code\n(\n    df_edad\n        .to_pandas()\n        .sample(10)\n        .style.format(\n            {\n            'edad':'{:,.0f}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nedad\ncrimes\n\n\n\n\n56\n10,789\n\n\n47\n16,414\n\n\n44\n16,818\n\n\n90\n313\n\n\n85\n789\n\n\n37\n21,022\n\n\n82\n1,181\n\n\n88\n493\n\n\n50\n16,098\n\n\n73\n3,296\n\n\n\n\n\n\nTable¬†4: Accrued Crimes in Mexico City by Age\n\n\n\n\n\n\nShow code\nfig = px.bar(df_edad,\n             x='edad',\n             y='crimes',\n             orientation='v',\n             hover_data=['edad','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Age',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Age')),\n    )\n\nfig.update_traces(marker_color='#7f0000',)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†5: Crimes in Mexico City by Age Distribution"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#crimes-by-neighborhood",
    "href": "portfolio/posts/python/crimes-web.html#crimes-by-neighborhood",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by neighborhood",
    "text": "Crimes by neighborhood\n\n\nShow code\ndf_colonia = (\n    df.group_by('colonia_hecho')\n        .agg(pl.len().alias('crimes'))\n        .top_k(10, by='crimes')\n        .sort('crimes', descending=False)\n)\n\n\n\n\nShow code\n(\n    df_colonia\n        .to_pandas()\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\ncolonia_hecho\ncrimes\n\n\n\n\nPEDREGAL DE SANTO DOMINGO\n10,073\n\n\nJU√ÅREZ\n11,423\n\n\nBUENAVISTA\n11,749\n\n\nNARVARTE\n11,906\n\n\nAGR√çCOLA ORIENTAL\n12,024\n\n\nMORELOS\n12,518\n\n\nROMA NORTE\n14,878\n\n\nDEL VALLE CENTRO\n17,178\n\n\nDOCTORES\n24,711\n\n\nCENTRO\n40,066\n\n\n\n\n\n\nTable¬†5: Accrued Crimes in Mexico City by Neighborhood\n\n\n\n\n\n\nShow code\nfig = px.bar(df_colonia,\n             y='colonia_hecho',\n             x='crimes',\n             orientation='h',\n             hover_data=['colonia_hecho','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City - Top 10 Neighborhoods',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Alcaldia')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†6: Crimes in Mexico City by Neighborhood"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#crimes-by-mayorship",
    "href": "portfolio/posts/python/crimes-web.html#crimes-by-mayorship",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by mayorship",
    "text": "Crimes by mayorship\n\n\nShow code\ndf_alcaldia = (\n    df.group_by('alcaldia_hecho')\n        .agg(pl.len().alias('crimes'))\n        .sort('crimes')\n        .filter(pl.col('alcaldia_hecho')!='FUERA DE CDMX')\n)\n\n\n\n\nShow code\n(\n    df_alcaldia\n        .to_pandas()\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nalcaldia_hecho\ncrimes\n\n\n\n\nMILPA ALTA\n13,184\n\n\nCUAJIMALPA DE MORELOS\n23,329\n\n\nLA MAGDALENA CONTRERAS\n27,285\n\n\nTLAHUAC\n41,547\n\n\nXOCHIMILCO\n45,924\n\n\nIZTACALCO\n60,427\n\n\nAZCAPOTZALCO\n64,561\n\n\nVENUSTIANO CARRANZA\n78,446\n\n\nMIGUEL HIDALGO\n84,242\n\n\nTLALPAN\n84,382\n\n\nCOYOACAN\n93,953\n\n\nALVARO OBREGON\n94,083\n\n\nBENITO JUAREZ\n101,759\n\n\nGUSTAVO A. MADERO\n139,294\n\n\nCUAUHTEMOC\n188,964\n\n\nIZTAPALAPA\n199,591\n\n\n\n\n\n\nTable¬†6: Accrued Crimes in Mexico City by Mayorship\n\n\n\n\n\n\nShow code\nfig = px.bar(df_alcaldia,\n             y='alcaldia_hecho',\n             x='crimes',\n             orientation='h',\n             hover_data=['alcaldia_hecho','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Mayorship',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Alcaldia')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†7: Crimes in Mexico City by Mayorship"
  },
  {
    "objectID": "portfolio/posts/python/crimes-web.html#heat-map-of-crimes",
    "href": "portfolio/posts/python/crimes-web.html#heat-map-of-crimes",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Heat map of crimes",
    "text": "Heat map of crimes\n\n\nShow code\ndf_map = (\n    df.with_columns(\n        (pl.col('colonia_hecho') + ', ' + pl.col('alcaldia_hecho')).alias('neighborhood')\n    )\n        .filter(pl.col('alcaldia_hecho')!='FUERA DE CDMX')\n        .group_by('neighborhood', maintain_order=True)\n        .agg(latitude=pl.col('latitud').mean(),\n             longitude=pl.col('longitud').mean(),\n             crimes=pl.col('delito').len()\n            )\n)\n\n\n\n\nShow code\nheat_map = df_map.to_pandas()\n\n\n\n\nShow code\n(\n    heat_map.sort_values('crimes', ascending=False)\n        .head(10)\n        .style.format(\n        {\n            'latitude':'{:,.4f}',\n            'longitude':'{:,.4f}',\n            'crimes':'{:,.0f}',\n        }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nneighborhood\nlatitude\nlongitude\ncrimes\n\n\n\n\nCENTRO, CUAUHTEMOC\n19.4327\n-99.1375\n40,042\n\n\nDOCTORES, CUAUHTEMOC\n19.4200\n-99.1486\n24,711\n\n\nDEL VALLE CENTRO, BENITO JUAREZ\n19.3831\n-99.1682\n17,178\n\n\nROMA NORTE, CUAUHTEMOC\n19.4184\n-99.1627\n14,878\n\n\nAGR√çCOLA ORIENTAL, IZTACALCO\n19.3947\n-99.0708\n12,008\n\n\nNARVARTE, BENITO JUAREZ\n19.3930\n-99.1542\n11,906\n\n\nJU√ÅREZ, CUAUHTEMOC\n19.4268\n-99.1628\n11,408\n\n\nPEDREGAL DE SANTO DOMINGO, COYOACAN\n19.3275\n-99.1677\n10,073\n\n\nPOLANCO, MIGUEL HIDALGO\n19.4335\n-99.1956\n10,019\n\n\nAGR√çCOLA PANTITLAN, IZTACALCO\n19.4104\n-99.0649\n9,662\n\n\n\n\n\n\nTable¬†7: Crimes in Mexico City Heatmap\n\n\n\n\n\n\nShow code\nfrom folium.plugins import HeatMap\n\nbasemap = folium.Map(location=[19.35, -99.12], zoom_start=10.4)\n\n\n\n\nShow code\nHeatMap(heat_map[['latitude', 'longitude', 'crimes']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x310ecc530&gt;\n\n\n\n\nShow code\nfolium.plugins.Fullscreen().add_to(basemap)\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†8: Mexico City Crime Heatmap by Mayorship\n\n\n\n\n\n\nShow code\n# mexico city crime map\nm = folium.Map(\n    location=[19.35, -99.12],\n    zoom_start=10,\n    control_scale=False,\n)\n# Layers\nCrime = folium.FeatureGroup(name='&lt;u&gt;&lt;b&gt;Place&lt;/b&gt;&lt;/u&gt;', show=True)\nm.add_child(Crime)\n#draw marker with symbol you want at base\nmy_symbol_css_class= \"\"\" &lt;style&gt;\n.fa-mysymbol3:before {\n    font-family: Gill Sans; \n    font-weight: bold;\n    font-size: 11px;\n    color: white;\n    background-color:'';\n    border-radius: 10px; \n    white-space: pre;\n    content: 'P';\n    }\n&lt;/style&gt;    \n        \"\"\"\n# the below is just add above  CSS class to folium root map      \nm.get_root().html.add_child(folium.Element(my_symbol_css_class))\n# then we just create marker and specific your css class in icon like below\nfor i in heat_map.index:\n   html=f\"\"\"\n        &lt;p style=\"font-size: 14px;\"&gt;{heat_map.iloc[i]['neighborhood']}&lt;/font&gt;&lt;/p&gt;\n        &lt;p style=\"font-size: 14px;\"&gt;Total crimes: {heat_map.iloc[i]['crimes']}&lt;/font&gt;&lt;/p&gt;\n        \"\"\"\n   iframe = folium.IFrame(html=html, width=220, height=90)\n   popup = folium.Popup(iframe, max_width=250)\n   folium.Marker(\n        location = [heat_map.iloc[i]['latitude'], heat_map.iloc[i]['longitude']],\n        icon = folium.Icon(color='darkred', prefix='fa', icon='fa-mysymbol3'),\n        popup = popup,\n        tooltip = heat_map.iloc[i]['neighborhood']\n    ).add_to(Crime)\nfolium.plugins.Fullscreen().add_to(m)\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†9: Mexico City Crime Hubs by Neighborhood"
  },
  {
    "objectID": "portfolio/posts/python/mexico_pop1950-2023.html",
    "href": "portfolio/posts/python/mexico_pop1950-2023.html",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "",
    "text": "In this article we shall see Mexico‚Äôs Population evolution over years.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n\n\nCode\n# Read tsv file\nmex = pd.read_csv('datasets/mexico_pop.tsv', sep='\\t')\n# Drop spaces\nmex = mex.rename(columns={'Year ':'Year','Population ':'Population','Growth Rate':'Growth rate'})\n\n\n\n\nCode\nmex.sample(5)\n\n\n\n\n\n\n\n\n\nYear\nPopulation\nGrowth rate\n\n\n\n\n41\n1982\n70,656,783\n2.06%\n\n\n54\n1969\n48,714,394\n3.27%\n\n\n69\n1954\n30,616,896\n2.67%\n\n\n42\n1981\n69,233,769\n2.26%\n\n\n47\n1976\n60,452,543\n3.00%\n\n\n\n\n\n\n\n\n\nCode\nmex['Population'] = pd.to_numeric(mex['Population'].str.replace(',',''))\n\n\n\n\nCode\nmex['Growth rate'] = pd.to_numeric(mex['Growth rate'].str.replace('%','', regex=True))\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Population'))\n    + geom_line(color='#b30000', size=1.5, linetype='solid')\n    + labs(x='Year', y='Population', \n           title='Total Population in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 150_000_000))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw() # Removes \"&lt;Figure Size: (640 x 480)&gt;\" below chart\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Growth rate'))\n    + geom_line(color='#767600', size=1.5, linetype='solid')\n    + labs(x='Year', y='Grwoth rate %', \n           title='Population Growth in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 5))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw()"
  },
  {
    "objectID": "portfolio/posts/python/mexico_pop1950-2023.html#introduction",
    "href": "portfolio/posts/python/mexico_pop1950-2023.html#introduction",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "",
    "text": "In this article we shall see Mexico‚Äôs Population evolution over years.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n\n\nCode\n# Read tsv file\nmex = pd.read_csv('datasets/mexico_pop.tsv', sep='\\t')\n# Drop spaces\nmex = mex.rename(columns={'Year ':'Year','Population ':'Population','Growth Rate':'Growth rate'})\n\n\n\n\nCode\nmex.sample(5)\n\n\n\n\n\n\n\n\n\nYear\nPopulation\nGrowth rate\n\n\n\n\n41\n1982\n70,656,783\n2.06%\n\n\n54\n1969\n48,714,394\n3.27%\n\n\n69\n1954\n30,616,896\n2.67%\n\n\n42\n1981\n69,233,769\n2.26%\n\n\n47\n1976\n60,452,543\n3.00%\n\n\n\n\n\n\n\n\n\nCode\nmex['Population'] = pd.to_numeric(mex['Population'].str.replace(',',''))\n\n\n\n\nCode\nmex['Growth rate'] = pd.to_numeric(mex['Growth rate'].str.replace('%','', regex=True))\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Population'))\n    + geom_line(color='#b30000', size=1.5, linetype='solid')\n    + labs(x='Year', y='Population', \n           title='Total Population in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 150_000_000))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw() # Removes \"&lt;Figure Size: (640 x 480)&gt;\" below chart\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Growth rate'))\n    + geom_line(color='#767600', size=1.5, linetype='solid')\n    + labs(x='Year', y='Grwoth rate %', \n           title='Population Growth in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 5))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw()"
  },
  {
    "objectID": "portfolio/posts/python/mexico_pop1950-2023.html#conclusions",
    "href": "portfolio/posts/python/mexico_pop1950-2023.html#conclusions",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "Conclusions",
    "text": "Conclusions\nMexico boasts a population of nearly 130 million, ranking it as the 10th most populous country globally.\nThe population is steadily growing, with estimates suggesting an annual increase of around 1.2%.\nMexico is a highly urbanized nation, with 88% of the population residing in cities. This trend is expected to continue, driven by economic opportunities and infrastructure development.\nThe overall population density is moderate, at around 66 individuals per square kilometer, but varies significantly across regions.\nThe population is relatively young, with a median age of 29.8 years. This indicates a large segment in their working years, potentially driving economic growth.\nHowever, an aging population is expected in the coming decades, requiring adjustments to social security and healthcare systems.\nMexico is a multicultural nation with a rich indigenous heritage. While mestizos (mixed Indigenous and European ancestry) form the majority, Indigenous groups still comprise a significant portion of the population, contributing to cultural diversity.\nRapid urbanization brings challenges like managing infrastructure, providing essential services, and tackling inequality.\nInvesting in education, healthcare, and job creation can harness the demographic dividend presented by the young population.\nAddressing the potential impact of an aging population and promoting sustainable development are crucial for long-term prosperity."
  },
  {
    "objectID": "portfolio/posts/python/mexico_pop1950-2023.html#references",
    "href": "portfolio/posts/python/mexico_pop1950-2023.html#references",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "References",
    "text": "References\n\nWorldometer\nINEGI - National Institute of Statistics and Geography\nCIA World Factbook"
  },
  {
    "objectID": "portfolio/posts/python/mexico_pop1950-2023.html#contact",
    "href": "portfolio/posts/python/mexico_pop1950-2023.html#contact",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/spotipy.html#what-is-spotipy",
    "href": "portfolio/posts/python/spotipy.html#what-is-spotipy",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "What is Spotipy?",
    "text": "What is Spotipy?\nSpotipy is a lightweight, easy-to-use Python library that simplifies interacting with the Spotify API. It handles the nitty-gritty details of authentication, making requests, and parsing responses, letting you focus on the fun part: exploring and manipulating music data."
  },
  {
    "objectID": "portfolio/posts/python/spotipy.html#why-use-spotipy",
    "href": "portfolio/posts/python/spotipy.html#why-use-spotipy",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Why Use Spotipy?",
    "text": "Why Use Spotipy?\n\nAccess a Universe of Music Data\nRetrieve information about artists, albums, tracks, playlists, audio features (like danceability and energy), and much more.\nAutomate Tasks\nCreate scripts to manage your playlists, discover new music based on your taste, analyze your listening habits, or even build your own music recommendation system.\nIntegrate with Other Tools\nCombine Spotipy with other Python libraries like Pandas for data analysis, Matplotlib for visualization, or Flask for building web applications.\nBuild Music-Focused Apps\nDevelop custom applications that leverage Spotify‚Äôs vast music catalog and user data. Think of creating personalized radio stations, music visualizations, or tools for music discovery.\nEasy to Learn and Use\nSpotipy‚Äôs well-documented API and intuitive design make it accessible to both beginners and experienced Python developers."
  },
  {
    "objectID": "portfolio/posts/python/spotipy.html#key-features-and-capabilities",
    "href": "portfolio/posts/python/spotipy.html#key-features-and-capabilities",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Key Features and Capabilities",
    "text": "Key Features and Capabilities\n\nAuthentication\nHandles the OAuth 2.0 flow for authenticating users and obtaining access tokens, allowing you to access both public and private data.\nSearching\nSearch for artists, tracks, albums, and playlists using keywords.\nRetrieving Information\nFetch detailed information about specific artists, tracks, albums, and playlists, including metadata, audio features, and related artists.\nPlaylist Management\nCreate, modify, and manage playlists, including adding and removing tracks.\nUser Profile Access\nAccess user profile information, including listening history, followed artists, and saved tracks.\nAudio Features Analysis\nRetrieve audio features for tracks, such as danceability, energy, tempo, valence, and more. This data can be used to analyze music and build interesting applications.\n\n\nEnvironment settings\n\n\nShow code\nimport json\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\n\n\n\n\nShow code\n# Token gotten from spotify api\nfilename = '../APIs/credentials.json'\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n# read credentials\nclientID = keys['spotipy_client_id']\nclientSecret = keys['spotipy_client_secret']\n\n\n\n\nShow code\nclient_credential_manager = SpotifyClientCredentials(client_id=clientID, client_secret=clientSecret)\nsp = spotipy.Spotify(client_credentials_manager=client_credential_manager)\n\n\n\n\nArtist\n\n\nShow code\nresults = sp.artist_albums('spotify:artist:06HL4z0CvFAxyc27GXpf02', album_type='album')\nalbums = results['items']\nwhile results['next']:\n    results = sp.next(results)\n    albums.extend(results['items'])\n\nfor album in albums:\n    print(album['name'])\n\n\n1989 (Taylor's Version) [Deluxe]\n1989 (Taylor's Version)\nSpeak Now (Taylor's Version)\nMidnights (The Til Dawn Edition)\nMidnights (3am Edition)\nMidnights\nRed (Taylor's Version)\nFearless (Taylor's Version)\nevermore (deluxe version)\nevermore\nfolklore: the long pond studio sessions (from the Disney+ special) [deluxe edition]\nfolklore (deluxe version)\nfolklore\nLover\nreputation\nreputation Stadium Tour Surprise Song Playlist\n1989 (Deluxe)\n1989\nRed (Deluxe Edition)\nSpeak Now World Tour Live\nSpeak Now\nSpeak Now (Deluxe Package)\nFearless (Platinum Edition)\nFearless (International Version)\nLive From Clear Channel Stripped 2008\nTaylor Swift\n\n\n\n\nAlbum\n\n\nShow code\nalbum = sp.album_tracks('spotify:album:6MeLjaERUK6fJ58YZpPlyC')\nlista_canciones = album['items']\n\nfor cancion in lista_canciones:\n    print(cancion['name'])\n\n\nY Nos Dieron las Diez\nConductores Suicidas\nYo Quiero Ser una Chica Almodovar\nA la Orilla de la Chimenea\nTodos Menos T√∫\nLa del Pirata Cojo\nLa Canci√≥n de las Noches Perdidas\nLos Cuentos Que Yo Cuento\nPeor para el Sol\nAmor Se Llama el Juego\nPastillas para No So√±ar"
  },
  {
    "objectID": "portfolio/posts/python/spotipy.html#search",
    "href": "portfolio/posts/python/spotipy.html#search",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Search",
    "text": "Search\n\n\nShow code\nartist_name = []\ntrack_name = []\npopularity = []\ntrack_id = []\n\nfor i in range(0,1_000,50):\n    track_results = sp.search(q='year:2020', type='track', limit=50, offset=i)\n    for i, t in enumerate(track_results['tracks']['items']):\n        artist_name.append(t['artists'][0]['name'])\n        track_name.append(t['name'])\n        track_id.append(t['id'])\n        popularity.append(t['popularity'])\n\n\n\n\nShow code\ntrack_dataframe = pd.DataFrame(\n        {'artist_name':artist_name,\n         'track_name':track_name,\n         'track_id':track_id,\n         'popularity':popularity}\n)\n\n\n\n\nShow code\ntrack_dataframe\n\n\n\n\n\n\n\n\n\nartist_name\ntrack_name\ntrack_id\npopularity\n\n\n\n\n0\nDream Supplier\nClean Baby Sleep White Noise (Loopable)\n0zirWZTcXBBwGsevrsIpvT\n94\n\n\n1\nHotel Ugly\nShut up My Moms Calling\n3hxIUxnT27p5WcmjGUXNwx\n90\n\n\n2\nBrent Faiyaz\nClouded\n2J6OF7CkpdQGSfm1wdclqn\n86\n\n\n3\n21 Savage\nGlock In My Lap\n6pcywuOeGGWeOQzdUyti6k\n87\n\n\n4\nSteve Lacy\nInfrunami\n0f8eRy9A0n6zXpKSHSCAEp\n86\n\n\n...\n...\n...\n...\n...\n\n\n995\nEdith Whiskers\nHome\n18V1UiYRvWYwn01CRDbbuR\n73\n\n\n996\nDuke Dumont\nOcean Drive\n4b93D55xv3YCH5mT4p6HPn\n74\n\n\n997\nBad Bunny\nTE DESEO LO MEJOR\n23XjN1s3DZC8Q9ZwuorYY4\n73\n\n\n998\nJunior H\nNo Me Pesa\n4YU704KDCv4tyE6qQxliY3\n69\n\n\n999\nDestroy Lonely\nIn The Air\n2eJBpNlTTPatjec4SDQvuo\n64\n\n\n\n\n1000 rows √ó 4 columns"
  },
  {
    "objectID": "portfolio/posts/python/spotipy.html#download-music-songs-from-spotify",
    "href": "portfolio/posts/python/spotipy.html#download-music-songs-from-spotify",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Download music songs from Spotify",
    "text": "Download music songs from Spotify\n\n\nShow code\nimport spotdl\nimport spotify_dl\n\n\n\n\nShow code\n%%bash\ncd ~/Downloads;\nspotify_dl -l 'https://open.spotify.com/track/3D0bXrSv7O73vOaGOG8J9c?si=2eb4c15e2d7c4adf' \\\n&gt; /dev/null;\n\n\n\n\nShow code\n%%bash\ncd ~/Downloads;\nspotdl 'https://open.spotify.com/track/3D0bXrSv7O73vOaGOG8J9c?si=2eb4c15e2d7c4adf' \\\n&gt; /dev/null;"
  },
  {
    "objectID": "portfolio/posts/python/spotipy.html#conclusions",
    "href": "portfolio/posts/python/spotipy.html#conclusions",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Conclusions",
    "text": "Conclusions\nSpotipy is a valuable tool for anyone interested in working with Spotify‚Äôs music data. It bridges the gap between the Spotify API and the Python programming language, enabling developers to create innovative and data-driven music experiences. While there are considerations related to API limitations and authentication, the benefits of using Spotipy generally outweigh the challenges.\nConsiderations and Limitations\n\nAPI Rate Limits: Spotify‚Äôs API has rate limits, which can restrict the number of requests you can make within a given time period. This necessitates careful planning and optimization of API calls, especially for large-scale data retrieval.\nAuthentication Complexity: While Spotipy simplifies authentication, understanding OAuth 2.0 and managing access tokens can still be a hurdle for some users.\nData Structure Awareness: Effective use of Spotipy requires a good understanding of the structure of Spotify‚Äôs data, including the various object types (artists, tracks, playlists) and their attributes.\nDependence on Spotify API: Spotipy‚Äôs functionality is entirely dependent on the Spotify Web API. Any changes or limitations to the API will directly affect the library‚Äôs capabilities.\nMaintaining Token Refreshing: Applications that use spotipy and run for long periods of time, need to implement robust token refreshing, or the application will cease to function."
  },
  {
    "objectID": "portfolio/posts/python/spotipy.html#contact",
    "href": "portfolio/posts/python/spotipy.html#contact",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/mexico_gn.html",
    "href": "portfolio/posts/python/mexico_gn.html",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "",
    "text": "GN personnel"
  },
  {
    "objectID": "portfolio/posts/python/mexico_gn.html#national-guard",
    "href": "portfolio/posts/python/mexico_gn.html#national-guard",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "National Guard",
    "text": "National Guard\nMexico‚Äôs National Guard is a relatively new security force established in 2019. It was created to address the country‚Äôs high crime rates and complement traditional law enforcement.\n\nFormation\n\nIn 2019, it emerged by merging elements of the Federal Police, Military Police, and Naval Police.\n\nMission\n\nThe National Guard was intended to be a gendarmerie, focusing on territorial defense and public security tasks like crime prevention and patrolling.\n\nStructure\n\nInitially envisioned under civilian control, a 2022 reform transferred command to the Ministry of National Defense, sparking controversy.\n\nCurrent Status\n\nThe National Guard‚Äôs role is evolving. It still tackles crime, but also handles tasks like border control and disaster relief."
  },
  {
    "objectID": "portfolio/posts/python/mexico_gn.html#display-interactive-chart",
    "href": "portfolio/posts/python/mexico_gn.html#display-interactive-chart",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure¬†1: Datawrapper Interactive Chart\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "portfolio/posts/python/mexico_gn.html#references",
    "href": "portfolio/posts/python/mexico_gn.html#references",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "References",
    "text": "References\n\nWhat is Guardia Nacional?\nGobierno de Mexico"
  },
  {
    "objectID": "portfolio/posts/python/mexico_gn.html#contact",
    "href": "portfolio/posts/python/mexico_gn.html#contact",
    "title": "National Guard‚Äôs Deployed Personnel in Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/mexico_pol_scenario.html#mexicos-political-scenario",
    "href": "portfolio/posts/python/mexico_pol_scenario.html#mexicos-political-scenario",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Mexico‚Äôs Political Scenario",
    "text": "Mexico‚Äôs Political Scenario\nA complex mix of democratic strides and ongoing hurdles\n\nElectoral Democracy\n\nSince 2000, Mexico has functioned as an electoral democracy with regular power transitions between parties, both nationally and regionally.\n\nMulti-Party System\n\nMexico boasts a vibrant multi-party system, with several parties competing for power, but the current dominant force is the National Regeneration Movement (MORENA).\n\nChallenges\n\nRule of Law\n\nDeficiencies in the rule of law weaken citizen confidence in political institutions. Issues like corruption, violence by criminal groups, and limited accountability create a complex environment.\n\nPublic Security\n\nDrug trafficking and organized crime pose a major threat to public safety and stability. The government‚Äôs strategies to combat these issues are a source of ongoing debate.\n\nMilitary‚Äôs Role\n\nThe expanding role of the military in public security raises concerns about potential human rights abuses and a shift away from civilian control.\n\n\n\n\n\n\nFigure¬†1: Governors of States in Mexico\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "portfolio/posts/python/mexico_pol_scenario.html#mexico-citys-political-landscape",
    "href": "portfolio/posts/python/mexico_pol_scenario.html#mexico-citys-political-landscape",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Mexico City‚Äôs Political Landscape",
    "text": "Mexico City‚Äôs Political Landscape\nMexico City‚Äôs political scene is fascinating due to its unique position.\n\nNational Influence\n\nAs the capital, Mexico City is the hub of national politics. Federal government buildings are located here, and residents elect representatives to the national Congress. This creates a tight link between the local and the national.\n\nShifting Power\n\nHistorically, the mayor was appointed by the president. Since 1997, residents directly elect their leader for a six-year term.\n\nRecent Elections (2021)\n\nThe MORENA party solidified its power in many states, including Mexico City. However, opposition parties gained control of most city boroughs, chipping away at MORENA‚Äôs local dominance.\n\nActive Citizenry\n\nMexico City residents have a strong voice due to their large numbers and ability to protest. This makes them a powerful force that politicians need to consider.\n\nChallenges\n\nLike other parts of Mexico, concerns exist about democratic processes and security during protests. Violence against women‚Äôs rights demonstrations is a recent point of tension.\nMexico City‚Äôs political situation is dynamic and reflects the national landscape. The interplay between federal and local power, recent election results, and an engaged citizenry all contribute to a complex and interesting political environment.\n\n\n\n\n\n\nFigure¬†2: Mayors of Mexico City\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "portfolio/posts/python/mexico_pol_scenario.html#conclusions",
    "href": "portfolio/posts/python/mexico_pol_scenario.html#conclusions",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Conclusions",
    "text": "Conclusions\nMexico‚Äôs political future hinges on its ability to address these challenges.\nStrengthening the rule of law, tackling corruption, and finding effective solutions for security issues will be crucial for a more stable and prosperous democracy."
  },
  {
    "objectID": "portfolio/posts/python/mexico_pol_scenario.html#contact",
    "href": "portfolio/posts/python/mexico_pol_scenario.html#contact",
    "title": "Mexico‚Äôs Political Landscape",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/working_with_json_files.html#overview",
    "href": "portfolio/posts/python/working_with_json_files.html#overview",
    "title": "Manipulating JSON Files with Python",
    "section": "Overview",
    "text": "Overview\nDiving into the world of data science and machine learning, one of the fundamental skills you‚Äôll encounter is the art of reading data.\nIf you have already some experience with it, you‚Äôre probably familiar with JSON (JavaScript Object Notation) files.\nThink of how NoSQL databases like MongoDB love to store data in JSON, or how REST APIs often respond in the same format.\nHowever, JSON, while perfect for storage and exchange, isn‚Äôt quite ready for in-depth analysis in its raw form.\nThis is where we transform it into something more analytically friendly ‚Äì a tabular format.\n\nEnvironment settings\n\n\nShow code\n# Create a json file\n\nsimple_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n}\n\nsimple_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n}\n\nsimple_json_list = [\n    simple_json, \n    simple_json_2\n\n]\n\n\n\n\nShow code\nimport pandas as pd\n\n\n\n\nManipulating json files\n\n\nShow code\npd.json_normalize(simple_json)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nDavid\nLondon\n80000\n\n\n\n\n\n\n\n\n\nShow code\npd.json_normalize(simple_json_2)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nTaylor\nChicago\n120000\n\n\n\n\n\n\n\n\n\nShow code\npd.json_normalize(simple_json_list)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nDavid\nLondon\n80000\n\n\n1\nTaylor\nChicago\n120000\n\n\n\n\n\n\n\nIn case we just want to transform some specific fields into a tabular pandas DataFrame, the json_normalize() command does not allow us to choose what fields to transform.\nTherefore, a small preprocessing of the JSON should be performed where we filter just those columns of interest.\n\n\nShow code\n# Fields to include\nfields = ['name', 'city']\n\n# Filter the JSON data\nfiltered_json_list = [{key: value for key, value in item.items() if key in fields}\n                      for item in simple_json_list]\n\npd.json_normalize(filtered_json_list)\n\n\n\n\n\n\n\n\n\nname\ncity\n\n\n\n\n0\nDavid\nLondon\n\n\n1\nTaylor\nChicago\n\n\n\n\n\n\n\nWhen dealing with multiple-leveled JSONs we find ourselves with nested JSONs within different levels.\nThe procedure is the same as before, but in this case, we can choose how many levels we want to transform.\nBy default, the command will always expand all levels and generate new columns containing the concatenated name of all the nested levels.\n\n\nShow code\n# Create a nested json file\n\nmultiple_levels_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n    'skills': {\n        'python': 'advanced',\n        'SQL': 'advanced',\n        'GCP': 'mid'\n    },\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":True,\n        \"data analyst\":False,\n        }\n    }\n\nmultiple_levels_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n    'skills': {\n        'python': 'mid',\n        'SQL': 'advanced',\n        'GCP': 'beginner'\n    },\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":False,\n        \"data analyst\":True\n        }\n    }\n\nmultiple_level_json_list = [\n    multiple_levels_json, \n    multiple_levels_json_2\n\n]\n\n\nWe would get the following table with 3 columns under the field skills:\n\nskills.python\nskills.SQL\nskills.GCP\n\nand 4 columns under the field roles\n\nroles.project manager\nroles.data engineer\nroles.data scientist\nroles.data analyst\n\nHowever, imagine we just want to transform our top level.\nWe can do so by specifically defining the parameter max_level to 0\n\n\nShow code\npd.json_normalize(multiple_level_json_list, max_level = 0)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\nskills\nroles\n\n\n\n\n0\nDavid\nLondon\n80000\n{'python': 'advanced', 'SQL': 'advanced', 'GCP...\n{'project manager': False, 'data engineer': Fa...\n\n\n1\nTaylor\nChicago\n120000\n{'python': 'mid', 'SQL': 'advanced', 'GCP': 'b...\n{'project manager': False, 'data engineer': Fa...\n\n\n\n\n\n\n\n\n\nShow code\nnested_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n    'skills': [\"python\", \"SQL\",\"GCP\"],\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":True,\n        \"data analyst\":False,\n        }\n    }\n\nnested_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n    'skills': [\"python\", \"SQL\",\"PowerBI\",\"Looker\"],\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":False,\n        \"data analyst\":True\n        }\n    }\n\nnested_json_list = [\n    nested_json, \n    nested_json_2\n\n]\n\n\n\n\nShow code\npd.json_normalize(nested_json_list, record_path=['skills'], meta=['name','city'])\n\n\n\n\n\n\n\n\n\n0\nname\ncity\n\n\n\n\n0\npython\nDavid\nLondon\n\n\n1\nSQL\nDavid\nLondon\n\n\n2\nGCP\nDavid\nLondon\n\n\n3\npython\nTaylor\nChicago\n\n\n4\nSQL\nTaylor\nChicago\n\n\n5\nPowerBI\nTaylor\nChicago\n\n\n6\nLooker\nTaylor\nChicago"
  },
  {
    "objectID": "portfolio/posts/python/working_with_json_files.html#conclusion",
    "href": "portfolio/posts/python/working_with_json_files.html#conclusion",
    "title": "Manipulating JSON Files with Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, the transformation of JSON data into CSV files using Python‚Äôs Pandas library is easy and effective.\nJSON is still the most common format in modern data storage and exchange, notably in NoSQL databases and REST APIs. However, it presents some important analytic challenges when dealing with data in its raw format."
  },
  {
    "objectID": "portfolio/posts/python/working_with_json_files.html#references",
    "href": "portfolio/posts/python/working_with_json_files.html#references",
    "title": "Manipulating JSON Files with Python",
    "section": "References",
    "text": "References\n\nFerrer, J. (2024) Converting JSONs to Pandas DataFrames: Parsing Them the Right Way in Data Science"
  },
  {
    "objectID": "portfolio/posts/python/working_with_json_files.html#contact",
    "href": "portfolio/posts/python/working_with_json_files.html#contact",
    "title": "Manipulating JSON Files with Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/cdmx-subway.html",
    "href": "portfolio/posts/python/cdmx-subway.html",
    "title": "Mexico City‚Äôs Underground",
    "section": "",
    "text": "Figure¬†1: Mexico City‚Äôs Subway Network"
  },
  {
    "objectID": "portfolio/posts/python/cdmx-subway.html#early-beginnings-and-expansion",
    "href": "portfolio/posts/python/cdmx-subway.html#early-beginnings-and-expansion",
    "title": "Mexico City‚Äôs Underground",
    "section": "Early Beginnings and Expansion",
    "text": "Early Beginnings and Expansion\nThe STC‚Äôs inception dates back to the late 1960s, a time when Mexico City was experiencing unprecedented urban growth. Recognizing the need for a modern transportation system, the government embarked on an ambitious project to construct an underground railway. The first line, Line 1, opened its doors in 1969, connecting the historic center of the city with the northern suburbs.\nOver the following decades, the STC expanded rapidly, adding new lines and stations to accommodate the increasing population. Today, the system comprises 12 lines and serves over 5 million passengers daily. It has become an integral part of the city‚Äôs infrastructure, connecting diverse neighborhoods and facilitating economic activity."
  },
  {
    "objectID": "portfolio/posts/python/cdmx-subway.html#challenges-and-overcoming-adversity",
    "href": "portfolio/posts/python/cdmx-subway.html#challenges-and-overcoming-adversity",
    "title": "Mexico City‚Äôs Underground",
    "section": "Challenges and Overcoming Adversity",
    "text": "Challenges and Overcoming Adversity\nDespite its success, the STC has faced numerous challenges throughout its history. One of the most significant issues has been overcrowding, particularly during peak hours. The system‚Äôs capacity has often been strained, leading to long wait times and uncomfortable conditions for commuters. To address this problem, the authorities have implemented various measures, including expanding the network and improving train frequency.\nAnother challenge has been the maintenance and upkeep of the system. The STC‚Äôs infrastructure is aging, and some sections require significant repairs and upgrades. Ensuring the safety and reliability of the system has been a constant priority for the authorities. In recent years, there have been efforts to modernize the infrastructure and improve maintenance practices.\n\n\n\n\n\n\nFigure¬†2: Inside the Subway"
  },
  {
    "objectID": "portfolio/posts/python/cdmx-subway.html#the-future-of-the-stc",
    "href": "portfolio/posts/python/cdmx-subway.html#the-future-of-the-stc",
    "title": "Mexico City‚Äôs Underground",
    "section": "The Future of the STC",
    "text": "The Future of the STC\nLooking ahead, the STC faces both opportunities and challenges. The government has ambitious plans to further expand the system, connecting new areas of the city and improving accessibility. However, these projects require substantial investment and careful planning. Additionally, the STC will need to adapt to changing demographics and transportation trends, such as the rise of electric vehicles and ride-sharing services.\nDespite the challenges, the STC remains a vital component of Mexico City‚Äôs urban landscape. Its history is a testament to the city‚Äôs resilience and its ability to adapt to changing circumstances. As Mexico City continues to grow and evolve, the STC will play a crucial role in shaping its future.\n\nCreating Beautiful Tables with Python and Great-Tables\n\n\nCode\n# import libraries\nimport polars as pl\nimport duckdb as db\nfrom great_tables import GT, md, html, loc, style\n\n\n\n\nCode\n# connect to dabase\nconn = db.connect('my_database.db')\n\n\n\n\nCode\n# retrieve data from table to dataframe\ndf = conn.sql('select * from cdmx_subway').pl()\n\n\n\n\nCode\n# close connection\nconn.close()\n\n\n\n\nCode\n# Change datatypes\ndf = df.select(pl.exclude('Line'))\n\n\n\n\nCode\n# Create table with great-tables\ncdmx = (\n    GT(df)\n    .tab_header(\n        title=md(\"### Mexico City's Subway Lines and Stations\"),\n        subtitle=html('''&lt;h4 align=\"left\"&gt;\n        Mexico City's metro system is a vital artery of the bustling metropolis.\n        It is the largest and busiest in Latin America, serving more than 5.5 million\n        passengers daily in its 195 stations and 12 lines.&lt;/h4&gt;''')\n    )\n    #.tab_options(table_width=\"100%\")\n    .cols_align(align='center', columns=['icon','Opening date','Stations'])\n    .cols_label(\n        icon=\"Line Sation\"\n    )\n    .fmt_date(columns='Opening date', date_style=\"m_day_year\")\n    .fmt_image(\"icon\", path=\"cdmx_metro_lines\")\n    .sub_missing(missing_text=\"\")\n    .tab_source_note(source_note=md('''**Jesus L. Monroy**&lt;br&gt;*Economist & Data Scientist*&lt;br&gt;&lt;br&gt;'''))\n    .tab_source_note(\n        source_note=md('''Source: [Wikipedia](https://en.wikipedia.org/wiki/Mexico_City_Metro&gt;Wikipedia)'''))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City's Subway Lines and Stations\n\n\nMexico City's metro system is a vital artery of the bustling metropolis. It is the largest and busiest in Latin America, serving more than 5.5 million passengers daily in its 195 stations and 12 lines.\n\n\nLine Sation\nOpening date\nStations\n\n\n\n\n\nAug 22, 1984\n20\n\n\n\nAug 22, 1984\n24\n\n\n\nDec 1, 1979\n21\n\n\n\nAug 29, 1981\n10\n\n\n\nDec 19, 1981\n13\n\n\n\nDec 21, 1983\n11\n\n\n\nNov 29, 1988\n14\n\n\n\nJul 20, 1994\n19\n\n\n\nAug 26, 1987\n12\n\n\n\nOct 30, 2012\n20\n\n\n\nAug 12, 1991\n10\n\n\n\nDec 15, 1999\n21\n\n\n\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "portfolio/posts/python/cdmx-subway.html#contact",
    "href": "portfolio/posts/python/cdmx-subway.html#contact",
    "title": "Mexico City‚Äôs Underground",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/sql_duckdb.html",
    "href": "portfolio/posts/python/sql_duckdb.html",
    "title": "Supercharge Your SQL Analysis with Python and DuckDB",
    "section": "",
    "text": "Overview\nIn this post, we‚Äôll delve into the seamless integration of DuckDB with popular Python libraries, enabling efficient data ingestion, transformation, and analysis. Through practical examples, we demonstrate how to harness the full potential of DuckDB for complex SQL queries, real-time data processing, and exploratory data analysis. By the end of this post, readers will gain the knowledge and skills to supercharge their SQL analysis projects with Python and DuckDB.\n\n\nDatabase Creation\nDatabase: retail_db\nTable: retail_sales\n\n\nShow code\n# Import libraries\nimport polars as pl\nimport duckdb as db\nimport plotly.express as px\n\n\n\n\nShow code\n# Create database\nconn = db.connect('datasets/retail_db.db')\n\n\n\n\nShow code\n# Create table\nconn.sql('''\n    create table if not exists retail_sales (\n        id INT,\n        sale_date DATE,\n        sale_time TIME,\n        customer_id INT,\n        gender VARCHAR(10),\n        age INT,\n        category VARCHAR(35),\n        quantity INT,\n        price_per_unit FLOAT,\n        cogs FLOAT,\n        total_sale FLOAT\n        )\n''')\n\n\n\n\nData Ingestion\n\n\nShow code\n# Insert data into table from csv file\nconn.sql('''\n    INSERT INTO retail_sales\n    SELECT * FROM read_csv('datasets/sales.csv')\n''')\n\n\n\n\nData Exploration and Cleaning\n\nRecord Count: Determine the total number of records in the dataset.\nCustomer Count: Find out how many unique customers are in the dataset.\nCategory Count: Identify all unique product categories in the dataset.\nNull Value Check: Check for any null values in the dataset and delete records with missing data\n\n\n\nShow code\n# Show first 10 records \nconn.sql('select * exclude(cogs) from retail_sales limit 10').pl()\n\n\n\nshape: (10, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n10\n2023-11-22\n12:30:00\n110\n\"Female\"\n37\n\"Clothing\"\n1\n99.989998\n99.989998\n\n\n\n\n\n\nRecord count\n\n\nShow code\nconn.sql('select count(*) as records from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\nrecords\n\n\ni64\n\n\n\n\n448\n\n\n\n\n\n\nCustomer count\n\n\nShow code\nconn.sql('select count(distinct customer_id) customers from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\ncustomers\n\n\ni64\n\n\n\n\n55\n\n\n\n\n\n\nCategory count\n\n\nShow code\nconn.sql('select distinct category from retail_sales').pl()\n\n\n\nshape: (9, 1)\n\n\n\ncategory\n\n\nstr\n\n\n\n\n\"Toys\"\n\n\n\"Electronics\"\n\n\n\"Books\"\n\n\n\"Home Appliances\"\n\n\n\"Groceries\"\n\n\n\"Sports Equipment\"\n\n\n\"Clothing\"\n\n\n\"Home Goods\"\n\n\n\"Beauty Products\"\n\n\n\n\n\n\nNull value check\n\n\nShow code\nconn.table('retail_sales').pl().null_count()\n\n\n\nshape: (1, 11)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ncogs\ntotal_sale\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nData Analysis\nWrite a SQL query to retrieve all columns for sales made on ‚Äò2023-11-23‚Äô\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where sale_date = '2023-11-23'\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n\n\n\n\nWrite a SQL query to retrieve all transactions where the category is ‚ÄòClothing‚Äô and the quantity sold is more than 4 in the month of Nov-2022\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where category = 'Clothing'\n        and extract('month' from sale_date) = '11'\n        and quantity &gt;= 2\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n14\n2023-11-22\n13:30:00\n114\n\"Female\"\n27\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n22\n2023-11-20\n15:30:00\n122\n\"Female\"\n28\n\"Clothing\"\n2\n49.990002\n99.980003\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n30\n2023-11-17\n17:30:00\n130\n\"Female\"\n42\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n32\n2023-11-16\n14:15:00\n456\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n40\n2023-11-14\n18:30:00\n2859\n\"Female\"\n27\n\"Clothing\"\n4\n39.990002\n159.960007\n\n\n48\n2023-11-06\n19:00:00\n5443\n\"Female\"\n35\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n\n\n\n\nWrite a SQL query to calculate the total sales for each category\n\n\nShow code\nsales = conn.sql('''\n    select\n        category\n        , round(sum(total_sale),2) as net_sale\n        , count(*) as total_orders\n    from retail_sales\n    group by 1\n    order by total_orders desc\n''').pl()\n\n\n\n\nShow code\nsales\n\n\n\nshape: (9, 3)\n\n\n\ncategory\nnet_sale\ntotal_orders\n\n\nstr\nf64\ni64\n\n\n\n\n\"Electronics\"\n68878.32\n96\n\n\n\"Clothing\"\n12557.76\n96\n\n\n\"Books\"\n4133.04\n80\n\n\n\"Home Goods\"\n8437.84\n56\n\n\n\"Toys\"\n1639.04\n24\n\n\n\"Beauty Products\"\n1359.44\n24\n\n\n\"Home Appliances\"\n9599.76\n24\n\n\n\"Groceries\"\n3069.84\n24\n\n\n\"Sports Equipment\"\n3039.68\n24\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(sales,\n             x=\"net_sale\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','net_sale',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Sales by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to find the average age of customers who purchased items from the ‚ÄòClothing‚Äô category\n\n\nShow code\nconn.sql('''\n    select\n        round(avg(age), 2) as avg_age\n    from retail_sales\n    where category = 'Clothing'\n''').pl()\n\n\n\nshape: (1, 1)\n\n\n\navg_age\n\n\nf64\n\n\n\n\n33.25\n\n\n\n\n\n\nWrite a SQL query to find all transactions where the total_sale is greater than 1,000\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where total_sale &gt; 999\n''').pl()\n\n\n\nshape: (24, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n\n\n\n\nWrite a SQL query to find the total number of transactions made by each gender in each category\n\n\nShow code\nconn.sql('''\n    select\n        category\n        , gender\n        , count(*) as total_trans\n    FROM retail_sales\n    group by category\n            , gender\n    order by 2\n''').pl()\n\n\n\nshape: (9, 3)\n\n\n\ncategory\ngender\ntotal_trans\n\n\nstr\nstr\ni64\n\n\n\n\n\"Clothing\"\n\"Female\"\n96\n\n\n\"Home Goods\"\n\"Female\"\n56\n\n\n\"Toys\"\n\"Female\"\n24\n\n\n\"Home Appliances\"\n\"Female\"\n24\n\n\n\"Beauty Products\"\n\"Female\"\n24\n\n\n\"Groceries\"\n\"Male\"\n24\n\n\n\"Electronics\"\n\"Male\"\n96\n\n\n\"Sports Equipment\"\n\"Male\"\n24\n\n\n\"Books\"\n\"Male\"\n80\n\n\n\n\n\n\nWrite a SQL query to calculate the average sale for each month\n\n\nShow code\nconn.sql('''\n    select\n        year\n        , month\n        , avg_sale\n    from\n        (\n        select\n            extract(year from sale_date) as year\n            , EXTRACT(month from sale_date) as month\n            , round(avg(total_sale),2) as avg_sale\n            , rank() over(partition by extract(year from sale_date)\n            order by avg(total_sale) desc) as rank\n        from retail_sales\n        group by 1, 2\n        ) as t1\n''').pl()\n\n\n\nshape: (2, 3)\n\n\n\nyear\nmonth\navg_sale\n\n\ni64\ni64\nf64\n\n\n\n\n2023\n11\n254.61\n\n\n2023\n10\n198.3\n\n\n\n\n\n\nWrite a SQL query to find the top 5 customers based on the highest total sales\n\n\nShow code\nconn.sql('''\n    select customer_id\n            , round(sum(total_sale),2) as total_sales\n    from retail_sales\n    group by 1\n    order by 2 desc\n    limit 5\n''').pl()\n\n\n\nshape: (5, 2)\n\n\n\ncustomer_id\ntotal_sales\n\n\ni32\nf64\n\n\n\n\n129\n7999.92\n\n\n105\n7999.92\n\n\n109\n7999.84\n\n\n113\n6399.92\n\n\n117\n6399.84\n\n\n\n\n\n\nWrite a SQL query to find the number of unique customers who purchased items from each category\n\n\nShow code\ncustomers = conn.sql('''\n    select category\n            , count(distinct customer_id) as count_unique\n    from retail_sales\n    group by category\n    order by 2 desc\n''').pl()\n\n\n\n\nShow code\ncustomers\n\n\n\nshape: (9, 2)\n\n\n\ncategory\ncount_unique\n\n\nstr\ni64\n\n\n\n\n\"Clothing\"\n12\n\n\n\"Electronics\"\n12\n\n\n\"Books\"\n10\n\n\n\"Home Goods\"\n7\n\n\n\"Beauty Products\"\n3\n\n\n\"Toys\"\n3\n\n\n\"Groceries\"\n3\n\n\n\"Home Appliances\"\n3\n\n\n\"Sports Equipment\"\n3\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(customers,\n             x=\"count_unique\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','count_unique',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Unique Customers Purchases by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to create each shift and number of orders (Example Morning &lt;12, Afternoon Between 12 & 17, Evening &gt;17)\n\n\nShow code\nconn.sql('''\n    with hourly_sale as\n        (\n        select *\n            , case \n                when extract(hour from sale_time) &lt;12 then 'Morning'\n                when extract(hour from sale_time) between 12 and 17 then 'Afternoon'\n            else 'Evening'\n            end as shift\n        from retail_sales\n        )\n    select\n        shift\n        , count(*) as total_orders\n    from hourly_sale\n    group by shift\n''').pl()\n\n\n\nshape: (3, 2)\n\n\n\nshift\ntotal_orders\n\n\nstr\ni64\n\n\n\n\n\"Evening\"\n32\n\n\n\"Morning\"\n136\n\n\n\"Afternoon\"\n280\n\n\n\n\n\n\n\n\nClose connection\n\n\nShow code\n# Close connection\nconn.close()\n\n\n\n\nConclusions\nThis project demonstrates the power of combining Python and DuckDB for efficient and insightful SQL analysis. By mastering these tools, data analysts can streamline their workflows, uncover valuable insights, and make data-driven decisions that drive business success.\nWe‚Äôve explored the fundamental concepts of SQL and its practical applications in data analysis. By leveraging the capabilities of Python and DuckDB, we‚Äôve been able to efficiently query, clean, and analyze data. This knowledge and skillset can be applied to a wide range of data-driven tasks, from simple data exploration to complex predictive modeling. As we continue to delve deeper into the world of data, the combination of Python and DuckDB promises to be a powerful tool for data analysts.\nThese insights can be used to optimize marketing strategies, improve customer satisfaction, and drive revenue growth. As data continues to proliferate, the ability to harness its power through SQL analysis will become increasingly important for businesses to stay competitive.\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/pyspark example.html",
    "href": "portfolio/posts/python/pyspark example.html",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "",
    "text": "Figure¬†1: Pyspark Logo"
  },
  {
    "objectID": "portfolio/posts/python/pyspark example.html#conclusions",
    "href": "portfolio/posts/python/pyspark example.html#conclusions",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "Conclusions",
    "text": "Conclusions\nPySpark is a powerful and versatile tool for working with big data. Its combination of Spark‚Äôs distributed computing capabilities and Python‚Äôs ease of use makes it an excellent choice for data scientists and analysts looking to tackle large-scale data processing tasks.\nWhether you‚Äôre performing complex data transformations, building machine learning models, or analyzing real-time streams, PySpark can help you unlock the potential of your data. So, dive in, explore its features, and unleash the power of big data!"
  },
  {
    "objectID": "portfolio/posts/python/pyspark example.html#references",
    "href": "portfolio/posts/python/pyspark example.html#references",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "References",
    "text": "References\n\nOfficial PySpark Documentation\nGetting Started Guide\nTutorials & Examples"
  },
  {
    "objectID": "portfolio/posts/python/pyspark example.html#contact",
    "href": "portfolio/posts/python/pyspark example.html#contact",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/lego_history.html",
    "href": "portfolio/posts/python/lego_history.html",
    "title": "Exploring Lego Toys with Polars",
    "section": "",
    "text": "Figure¬†1: Lego Toys"
  },
  {
    "objectID": "portfolio/posts/python/lego_history.html#reading-datasets",
    "href": "portfolio/posts/python/lego_history.html#reading-datasets",
    "title": "Exploring Lego Toys with Polars",
    "section": "Reading datasets",
    "text": "Reading datasets\nA comprehensive database of lego blocks is provided by Rebrickable.\nThe data is available as csv file and the schema is shown below\n\n\n\n\nDatabase schema\n\nLet us start by reading in the colors data to get a sense of the diversity of lego sets!"
  },
  {
    "objectID": "portfolio/posts/python/why_bi_tools_fall_short.html",
    "href": "portfolio/posts/python/why_bi_tools_fall_short.html",
    "title": "Why BI Tools Fall Short, A Failure to Capture the Office Workflow",
    "section": "",
    "text": "Overview\nCompanies use data warehouses or data lakes for centralized data storage, consistency, data quality, scalability and easy access. Business Intelligence (BI) solutions in conjunction with data warehouses are used to make more informed, data-driven decisions by means of dahsboards for stakeholders.\nIn this fashion, a dashboard is created using a BI application, connected to a data warehouse with the aim to be consumed by end users for their business activities.\nIt is an open secret, nonetheless, that staff steadily use spreadsheets to store information and manipulate data sets coming from data warehouses, other information systems and dashboards.\nIn a similar fashion, staff steadily use slide presentations to showcase insights and reports to managers and other stake holders. This means there are countless presentations and data analyses stored in local Excel and PowerPoint files.\n\n\nBeyond the Dashboard: Reporting with Slideshows\nDashboards can be used to gather and analyze data, while slideshows can be used to present the findings in a clear and concise manner. Besides, it enables you to use your existing data insights in the tools you‚Äôre most familiar with, without having to switch to more complex ones.\n\nYou can just do your data analysis in Excel and then present it in PowerPoint. This provides you with just the flexibility you need for.\n\nWhile a dashboard is a centralized section that displays your data visually typically by using a license BI tool (Tableau, Power BI), staff prefers to present data insights to potential customers or coworkers in Excel or PowerPoint by copying and pasting charts and tables from dashboards.\n\n\n\n\n\n\nIndeed, BI tools like Tableau or Power BI offer options to download data to Excel or csv files, PowerPoint and images.\n\n\n\nAutomating Spreadsheet Data with¬†Python\nI propose the process of creating an ETL from the data warehouse to a spreadsheet using Python, and synchronizing tables and charts from Excel to PowerPoint to get an automated reporting in a local file with the needs of the end user.\nETL (Extract, Transform, Load) is a data integration process that involves 03 main steps:\n\nExtract Phase. Retrieving data from a source system (in this case, a data warehouse).\nTransform Phase. Manipulating, cleaning, and aggregating the extracted data.\nLoad Phase. Storing the transformed data into a target system (in this case, an Excel file).\n\n\n\n\n\n\n\nFigure¬†1: Image by El Mehdi Ettaki\n\n\n\n\n\nCase Study\nI‚Äôll show an example using Snowflake as data warehouse, Python for ETL process, and Excel as destination. Finally, PowerPoint will present the data insights.\n\nBy foregoing BI tools, we can substantially reduce project expenses.\n\n\n\n\n\n\n\nFigure¬†2: Image by author\n\n\n\n\nImport libraries\n\n\nimport pandas as pd\nfrom snowflake.snowpark import Session, Window\nimport snowflake.snowpark.functions as F\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nRead credentials\n\n\n# Credentials\nfile = 'credentials.json'\n# read file\nwith open(file) as f:\n    keys = json.load(f)\n\n\nConnect to Data Warehouse using Snowpark\n\n\n# Snowflake's Snowpark Connection\nconnection = {\n    \"account\": keys['account'],\n    \"user\": keys['user'],\n    \"role\": keys['role'],\n    \"authenticator\": keys['authenticator'],\n    \"warehouse\": keys['warehouse'],\n    \"database\": keys['database'],\n    \"schema\": keys['schema'],\n}\n\ndef snowflake_connection():\n    try:\n        session = Session.builder.configs(connection).create()\n        print(\"Connection successful!\")\n    except:\n        raise ValueError(\"Connection failed!\")\n    return session\n\nsession = snowflake_connection()\n\n\nExtract data using Snowpark\n\n\n# Extract sales data\nsales = conn.sql('''\n  SELECT\n      store_id,\n      SUM(sales_amount) AS total_sales\n  FROM\n      dm_sales\n  WHERE\n      YEAR(dm_sales) = YEAR(CURDATE())\n  GROUP BY\n      store_id\n''')\n# Extract stores data\nstores = (\n  conn.table('dm_stores')\n      .select('location','id','responsible')\n)\n# Label stores region\nstores = stores.with_column(\n        'region',\n        F.when(F.col('region_abv')=='AS', 'ASPAC')\n          .when(F.col('region_abv')=='LA', 'LATAM')\n          .when(F.col('region_abv')=='EU', 'EMEA')\n          .when(F.col('region_abv')=='NA', 'NA')\n          .otherwise('null')\n    )\n# Combine tables\ndata = (sales.join(stores, sales.store_id == stores.id))\n# Save to pandas\ndata = data.to_pandas()\n\n\nTransform data using Pandas\n\n\n# Calculate top 10 products\ntop_10 = data.nlargest(10, 'total_sales')\n\n\nLoad data using Pandas\n\n\n# Write to Excel\ntop_10.to_excel('top_10_products.xlsx', index=False)\n\n\n\nFrom Excel to PowerPoint: Automating Report¬†Creation\nNow, you can customize your tables and charts in Excel with the data saved from Python.¬†\nTo automate your customized tables and charts created in Excel onto PowerPoint, you just need to follow the next steps:\n\n\n\n\n\n\n\n\n\n\nFinally, after customizing your slideshow, you will get a PowerPoint like the following:\n\n\n\n\n\n\nFigure¬†3: Image by author\n\n\n\n\n\nConclusions\nThe use of spreadsheets and slideshows in businesses is not going to disappear soon. Hence, even if BI tools like Tableau or Power BI are used in businesses, Excel and PowerPoint are going to be used by staff to reporting and presentations to coworkers and managers.\nBy following the above steps and leveraging the power of Python, you can efficiently extract, transform, and load data from your data warehouse into Excel for further analysis and insights that fulfills end user‚Äôs requirements.\nMaybe is it time to recalculate the cost-benefit implications for companies to abandon expensive BI licenses in favor of flexible, cost-effective open-source solutions like Python.\n\n\nReferences\n\nBusiness (2023) How to Design a Dashboard Presentation: A Step-by-Step Guide in slidemodel.com\nKarlson, P. (2022) Are Spreadsheets Secretly Running Your Business? in Forbes\nMoore J. (2024) But, Can I Export it to Excel? in Do Mo(o)re with Data\nSchwab, P. (2021) Excel dominates the business world.. and that‚Äôs not about to change in Into the Minds\n\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/gasoline-web-nocode.html",
    "href": "portfolio/posts/python/gasoline-web-nocode.html",
    "title": "Gasoline Prices in Mexico",
    "section": "",
    "text": "Figure¬†1: Gas Prices in Mexico Report"
  },
  {
    "objectID": "portfolio/posts/python/gasoline-web-nocode.html#table-overview",
    "href": "portfolio/posts/python/gasoline-web-nocode.html#table-overview",
    "title": "Gasoline Prices in Mexico",
    "section": "Table overview",
    "text": "Table overview\nThe table below provides a preview of the working dataset, offering a glimpse into the structure and type of data being utilized.\n\n\n\n\n\n\n\n\n\n\nstate\nmunicipality\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nCHIHUAHUA\nJUAREZ\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nCOAHUILA DE ZARAGOZA\nSABINAS\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nCOAHUILA DE ZARAGOZA\nTORREON\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nDURANGO\nGOMEZ PALACIO\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nNUEVO LEON\nABASOLO\n$21.63\n$23.31\n$1.68\n7.76%\n\n\n\n\n\n\nTable¬†1: Dataset preview"
  },
  {
    "objectID": "portfolio/posts/python/gasoline-web-nocode.html#top-05-states-with-highest-gasoline-prices",
    "href": "portfolio/posts/python/gasoline-web-nocode.html#top-05-states-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with highest gasoline prices",
    "text": "Top 05 States with highest gasoline prices\nThe following table presents a snapshot of the States with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nQUINTANA ROO\n$21.46\n$24.80\n$3.34\n15.55%\n\n\nYUCATAN\n$21.46\n$24.79\n$3.33\n15.52%\n\n\nNAYARIT\n$22.54\n$24.76\n$2.23\n9.90%\n\n\nGUERRERO\n$22.37\n$24.76\n$2.39\n10.67%\n\n\nSINALOA\n$22.36\n$24.72\n$2.36\n10.59%\n\n\n\n\n\n\nTable¬†2: Top 05 States with highest prices"
  },
  {
    "objectID": "portfolio/posts/python/gasoline-web-nocode.html#top-05-states-with-lowest-gasoline-prices",
    "href": "portfolio/posts/python/gasoline-web-nocode.html#top-05-states-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with lowest gasoline prices",
    "text": "Top 05 States with lowest gasoline prices\nThe following table presents a snapshot of the States with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nTAMAULIPAS\n$21.22\n$23.10\n$1.87\n8.79%\n\n\nCOAHUILA DE ZARAGOZA\n$22.25\n$23.12\n$0.87\n4.01%\n\n\nCHIHUAHUA\n$21.47\n$23.36\n$1.89\n8.90%\n\n\nNUEVO LEON\n$21.41\n$23.40\n$1.99\n9.32%\n\n\nVERACRUZ DE IGNACIO DE LA LLAVE\n$21.59\n$23.44\n$1.86\n8.62%\n\n\n\n\n\n\nTable¬†3: Top 05 States with lowest prices"
  },
  {
    "objectID": "portfolio/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "href": "portfolio/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with highest gasoline prices",
    "text": "Top 05 Municipalities with highest gasoline prices\nThe following table presents a snapshot of the Municipalities with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nNUEVA ITALIA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nZIRACUARETIRO, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nGABRIEL ZAMORA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nHUIRAMBA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nNAHUATZEN, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\n\n\n\n\nTable¬†4: Top 05 Municipalities with highest prices"
  },
  {
    "objectID": "portfolio/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "href": "portfolio/posts/python/gasoline-web-nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with lowest gasoline prices",
    "text": "Top 05 Municipalities with lowest gasoline prices\nThe following table presents a snapshot of the Municipalities with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nANAHUAC, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nCD. GUERRERO, TAMAULIPAS\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nLAMPAZOS DE NARANJO, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nALLENDE, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\nMORELOS, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\n\n\n\n\nTable¬†5: Top 05 Municipalities with lowest prices"
  },
  {
    "objectID": "portfolio/posts/sql/sql.html#what-is-sql",
    "href": "portfolio/posts/sql/sql.html#what-is-sql",
    "title": "Diving into the World of SQL",
    "section": "What is SQL?",
    "text": "What is SQL?\nSQL stands for Structured Query Language. It is a standardized language for managing data in relational databases. Think of a relational database as a highly organized digital filing cabinet, where information is stored in tables with rows and columns. SQL provides the tools to not only retrieve specific data from these tables but also to create, modify, and delete tables and their contents."
  },
  {
    "objectID": "portfolio/posts/sql/sql.html#why-learn-sql",
    "href": "portfolio/posts/sql/sql.html#why-learn-sql",
    "title": "Diving into the World of SQL",
    "section": "Why Learn SQL?",
    "text": "Why Learn SQL?\nThe widespread adoption of relational databases like MySQL, PostgreSQL, Microsoft SQL Server, and Oracle Database makes SQL a highly sought-after skill. Learning SQL opens doors to a variety of opportunities, including:\n\nData Analysis\n\nSQL allows you to extract meaningful insights from vast datasets, identify trends, and generate reports that drive business decisions.\n\nDatabase Administration\n\nManaging and maintaining databases, ensuring data integrity, and optimizing performance often requires a deep understanding of SQL.\n\nWeb Development\n\nMany web applications rely on databases to store and retrieve information. SQL is essential for building dynamic websites and applications.\n\nData Science\n\nSQL is a fundamental tool for data scientists, enabling them to clean, prepare, and explore data before applying more advanced statistical and machine learning techniques."
  },
  {
    "objectID": "portfolio/posts/sql/sql.html#key-sql-concepts",
    "href": "portfolio/posts/sql/sql.html#key-sql-concepts",
    "title": "Diving into the World of SQL",
    "section": "Key SQL Concepts:",
    "text": "Key SQL Concepts:\nSQL is built around a set of commands that allow you to perform various operations. Here are some of the most important concepts:\n\nSELECT\n\nThis command is used to retrieve data from one or more tables based on specified criteria. You can select specific columns, filter data using conditions, and sort the results. For example: SELECT name, age FROM users WHERE city = ‚ÄòNew York‚Äô;\n\nINSERT\n\nThis command adds new rows of data into a table. For example: INSERT INTO users (name, age, city) VALUES (‚ÄòJohn Doe‚Äô, 30, ‚ÄòLondon‚Äô);\n\nUPDATE\n\nThis command modifies existing data in a table. For example: UPDATE users SET age = 31 WHERE name = ‚ÄòJohn Doe‚Äô;\n\nDELETE\n\nThis command removes rows from a table. For example: DELETE FROM users WHERE age = 65;\n\nCREATE\n\nThis command is used to create new database objects, such as tables, views, and indexes.\n\nDROP: This command deletes existing database objects."
  },
  {
    "objectID": "portfolio/posts/sql/sql.html#tasks-performed-with-sql",
    "href": "portfolio/posts/sql/sql.html#tasks-performed-with-sql",
    "title": "Diving into the World of SQL",
    "section": "Tasks performed with SQL",
    "text": "Tasks performed with SQL\n\nCreate and drop tables\nInsert and update records\nRetrieve data\nDelete data\nManage permissions\nCreate views and procedures"
  },
  {
    "objectID": "portfolio/posts/sql/sql.html#advanced-tasks-in-sql",
    "href": "portfolio/posts/sql/sql.html#advanced-tasks-in-sql",
    "title": "Diving into the World of SQL",
    "section": "Advanced tasks in SQL",
    "text": "Advanced tasks in SQL\n\nJOINs: Combining data from multiple tables based on related columns.\nSubqueries: Embedding queries within other queries to perform more sophisticated data retrieval.\n\nAggregate Functions: Performing calculations on groups of data, such as calculating averages, sums, and counts.\n\nWindow functions:"
  },
  {
    "objectID": "portfolio/posts/sql/sql.html#example-queries",
    "href": "portfolio/posts/sql/sql.html#example-queries",
    "title": "Diving into the World of SQL",
    "section": "Example queries",
    "text": "Example queries\n-- COUNT CUSTOMERS\nSELECT COUNT(1) AS customers\nFROM addresses\n;\n-- GET RETAIL SALES DATASET\nSELECT p.id_purchase\n    , d.priceInfo\n    , d.quantity\n    , s.id_shopper\n    , s.id_region\n    , r.description AS region\n    , s.id_territory\n    , t.description AS territory\n    , s.id_store\n    , b.description AS store\n    , s.NUD\n    , s.shop\n    , s.route\n    , s.purchase_date\n    , p.cancel\nFROM purchases AS p\nINNER JOIN purchase_details AS d ON p.id_purchase = d.id_purchase \nINNER JOIN shops AS s ON p.id_shopper = s.id_shopper\nINNER JOIN territories AS t ON s.id_territory = t.id_territory\nINNER JOIN stores AS b ON s.id_store = b.id_store\nINNER JOIN regions AS r ON s.id_region = r.id_region\n;\n-- GET PURCHASES BY CUSTOMER\nSELECT o.no_order\n    , o.id_order_status\n    , e.description\n    , o.id_social_network\n    , CASE\n        WHEN o.id_social_network = 1 THEN 'I'\n        WHEN o.id_social_network = 2 THEN 'F'\n        ELSE 'W'\n    END AS origin\n    , o.quantity \n    , o.total AS total_order\n    , o.purchase_date\n    , d.route AS delivery_route\n    , d.id_store \n    , d.nud\n    , d.email \n    , d.phone \nFROM orders o\nLEFT JOIN orderStatus e ON o.id_order_status = e.id_order_status\nLEFT JOIN addresses d ON o.id_address = d.id_address\nWHERE 1 = 1\n-- AND DATE(fs_ConvertDateToMexico(o.purchase_date))\n-- BETWEEN '2023-04-01' AND '2023-04-30'\n;\n-- GET UNIQUE BRANDS\nSELECT DISTINCT g.brand_code as Brand_Code\n, g.brand_name as Brand_Name\n, g.style_num_offset as Style_Num_Offset\n, g.active as Active\n, g.parent_brand as Parent_Brand\n, g.reporting_brand as Reporting_Brand\nFROM\ntp_brand g\n;\n-- GET FACTORY DATA WITH NULL COUNTRY OR CITY\nSELECT CONVERT(e.id,char) as Factory_Id\n, e.factory_name  as PO_FTY\n, ifnull(e.factory_city,'No City') as City\n, ifnull(e.factory_country,'No Country') as Country\nFROM\ntp_factories e\n;\n-- INSER DATA INTO TABLE\ninsert into CatAlumnos\nvalues(‚ÄòJuan‚Äô, ‚ÄòPerez‚Äô, ‚ÄòHuerta‚Äô, 1990-02-25, 2, ‚Äòa‚Äô)\n;\n/* GET STUDENT AGE POSTGRES FUNCTION */\nWITH cte_ages AS (\n    SELECT name, age(CURRENT_DATE, bod) AS student_age\n    FROM students\n)\nSELECT name, age\nFROM ages\nWHERE age &lt; 18\n;\n-- CREATE VIEW IN SQL SERVER\nCREATE VIEW student_profiles AS\n    SELECT concat(name, ' ' surname) AS student_name,\n/* Using SQL Server datediff function to reckon age */\n    DATEDIFF(YY, getdate(), bod) as age\n    FROM students\n    ORDER BY name\n;\n/* CONCAT AND CAST DATA TYPES */\nSELECT \n  concat('gsn','_', cast(campaign_id as text)) AS id_gsn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_2\nTOP 5\n;\n/* CREATE A UNION OF TABLES FROM 02 TEMP TABLES (CTEs) */\nWITH table_2_temp AS (\nSELECT\n  concat('gsn', '_', cast(campaign_id as text)) AS id_gsn\n, start_date \n, end_date\n, clicks\n, views\nFROM table_2\n)\n, table_3_temp AS (\nSELECT \nconcat('fbn', '_', cast(campaign_id as text)) AS id_fbn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_3\nWHERE cliks &gt; 0\n)\nSELECT \n  t2.id_gsn\nFROM table_2_temp t2\nUNION ALL\nSELECT \n  t3.id_fbn\nFROM table_3_temp t3\n;\n/* Window functions\nAdd row numbers to the placings table */\nSELECT\n    'year'\n    , host_country\n    , first_place\n    , total_goals\n    , row_number() OVER() AS row_num\nFROM world_cup_placings\n;\n\n/* Using SUM() within our window function */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , SUM(total_goals) OVER() AS all_goals\nFROM world_cup_placings\n;\n\n/* Computing the average number of goals */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , round(avg(total_goals) OVER(), 0) AS mean_goals\nFROM world_cup_placings\n;\n-- Big Query platform\n-- covid cases in North America 2020-2023\nwith years as (\n    select country_region\n        , extract(year from date) as year\n        , sum(confirmed) as total_confirmed\n        , sum(deaths) as total_deaths\n        , sum(cast(recovered as integer)) as total_recovered\n        , sum(active) as total_active\n    from `big-query.public-data.covid19_jhu_csse.summary`\n    where country_region in ('Mexico', 'US', 'Canada')\n    group by 1, 2\n    order by 1, 2\n)\nselect *\nfrom (\n    select \n        year\n        , country_region\n        , total_confirmed\n    from years\n)\npivot(sum(total_confirmed) as confirmed for year in (2020, 2021, 2022, 2023))\n;"
  },
  {
    "objectID": "portfolio/posts/sql/sql.html#contact",
    "href": "portfolio/posts/sql/sql.html#contact",
    "title": "Diving into the World of SQL",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html",
    "href": "portfolio/posts/python/cohort_analysis.html",
    "title": "Cohort Analysis",
    "section": "",
    "text": "Cohort analysis is a type of analysis that follows a group of customers (cohorts) over time to understand their behavior, based on a characteristic that identifies them during a period of time. Periods could be a day, week, month, or even a year. Cohort analysis takes into account time series data over the lifespan of a user to predict and analyze their future consumption patterns.\nCohorts are commonly created in accordance with acquisition date shared by a group of users. An acquisition cohort refers to a group of users who were newly acquired (e.g account creation, first purchase) by a business. On the other hand, users are assigned to behavioural cohorts when they perform a specific action during a time frame.\nBy analysing each cohort in isolation, behavioural differences between groups can be identified. These observations can then be used to inform marketing and product development decisions.\nCohort analyses are important because churn is one of the main problems faced by companies nowadays, and every information that can lead to less churning is a valuable business information.\nSpecifically, cohort analysis can be used to:\nMeasure Customer Lifetime Value (CLV)\nCohort analysis can be used to identify patterns in retention & monetary value of different user groups. By identifying valuable groups, strategies can be developed to replicate their characteristics among other groups.\nEvaluate Marketing Campaigns\nMonitoring how groups respond to different campaigns can help reveal the true effectiveness of marketing. For example, a big discount promotion may drive high levels of acquisition, but these users may have little interest in full price goods ‚Äî making them likely to churn. Over time, it may be observed that CLV is higher for campaigns that drove lower levels of acquisition.\nImprove Product Development\nAssigning users to cohorts, based on the actions they have taken within a product, can provide insights into the features that promote increases in user retention and engagement.\ne-commerce\nCustomers who have started using the service just two or three days ago or had just made their first purchase can be grouped into one group or cohort whereas people who have been on the website for 1 or 2 years may be grouped into another cohort and their purchase activity over the year can be used to predict how many times they will visit the website in the future or how many times they will actually click on a product and purchase it. This can be used to personalize their own home feed so that the click-through rate increases and their retention for that particular website don‚Äôt subside.\nWebsites\nCohort analysis can be used to predict viewer retention when a new person logs in for the first time vs a person who has been on the platform for a long time. To keep the new user on the platform, their home feed will be personalized with the most popular videos that have a huge amount of views, clickbait thumbnails, or videos with popular search terms based on that particular user‚Äôs location data whereas a long-time user will mostly receive video updates from their subscriptions and their most watched channels."
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#what-is-cohort-analysis",
    "href": "portfolio/posts/python/cohort_analysis.html#what-is-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "",
    "text": "Cohort analysis is a type of analysis that follows a group of customers (cohorts) over time to understand their behavior, based on a characteristic that identifies them during a period of time. Periods could be a day, week, month, or even a year. Cohort analysis takes into account time series data over the lifespan of a user to predict and analyze their future consumption patterns.\nCohorts are commonly created in accordance with acquisition date shared by a group of users. An acquisition cohort refers to a group of users who were newly acquired (e.g account creation, first purchase) by a business. On the other hand, users are assigned to behavioural cohorts when they perform a specific action during a time frame.\nBy analysing each cohort in isolation, behavioural differences between groups can be identified. These observations can then be used to inform marketing and product development decisions.\nCohort analyses are important because churn is one of the main problems faced by companies nowadays, and every information that can lead to less churning is a valuable business information.\nSpecifically, cohort analysis can be used to:\nMeasure Customer Lifetime Value (CLV)\nCohort analysis can be used to identify patterns in retention & monetary value of different user groups. By identifying valuable groups, strategies can be developed to replicate their characteristics among other groups.\nEvaluate Marketing Campaigns\nMonitoring how groups respond to different campaigns can help reveal the true effectiveness of marketing. For example, a big discount promotion may drive high levels of acquisition, but these users may have little interest in full price goods ‚Äî making them likely to churn. Over time, it may be observed that CLV is higher for campaigns that drove lower levels of acquisition.\nImprove Product Development\nAssigning users to cohorts, based on the actions they have taken within a product, can provide insights into the features that promote increases in user retention and engagement.\ne-commerce\nCustomers who have started using the service just two or three days ago or had just made their first purchase can be grouped into one group or cohort whereas people who have been on the website for 1 or 2 years may be grouped into another cohort and their purchase activity over the year can be used to predict how many times they will visit the website in the future or how many times they will actually click on a product and purchase it. This can be used to personalize their own home feed so that the click-through rate increases and their retention for that particular website don‚Äôt subside.\nWebsites\nCohort analysis can be used to predict viewer retention when a new person logs in for the first time vs a person who has been on the platform for a long time. To keep the new user on the platform, their home feed will be personalized with the most popular videos that have a huge amount of views, clickbait thumbnails, or videos with popular search terms based on that particular user‚Äôs location data whereas a long-time user will mostly receive video updates from their subscriptions and their most watched channels."
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#steps-for-cohort-analysis",
    "href": "portfolio/posts/python/cohort_analysis.html#steps-for-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "Steps for Cohort Analysis",
    "text": "Steps for Cohort Analysis\nThere are 05 main steps to performing cohort analysis. They are as follows:\n\nDetermining the main objective of the cohort analysis: First and foremost, we need to determine the main intent of performing the analysis, such as to analyze why people on YouTube don‚Äôt watch videos after the 6-minute mark. This sets the ultimate goal of the analysis and uses the huge pool of information for practical issues. This helps in pinpointing the root issue or cause and companies can then work towards improved business practices to provide a better user experience.\nDefining the metrics to respond to the question: The next step would be to identify what defines the problem. In simpler words, from the above example, we need to analyze when a viewer leaves a video or at what minute before moving on to something else, his/her watch time, and click-through rates on YouTube.\nIdentify the particular groups or cohorts that will be relevant: To analyze users we need to pick out a group of viewers who display common behavioral patterns. In order to do this we need to analyze data from different user inputs and identify relevant similarities and differences between them and then separate it into specified cohorts.\nPerforming the Cohort Analysis: Now we will use data visualization techniques to perform the cohort analysis based on the objective of the problem. This can be done using many programming languages out of which the preferred languages are python and R. Cohort analysis in python can be done using libraries such as NumPy and seaborn. Heat maps are usually used to display user retention and visualize data in a tabular form.\nTesting the Results: Last but not the least, the results need to be checked and tested in order to make sure that they can actually reduce company losses and optimize business practices. We will obtain retention rates from the analysis and a heatmap(or any other suitable graph) of user retention and retention rate will help us analyze and improve experiences for the users."
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#environment-setting",
    "href": "portfolio/posts/python/cohort_analysis.html#environment-setting",
    "title": "Cohort Analysis",
    "section": "Environment Setting",
    "text": "Environment Setting\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nShow code\n# read dataset from github\ndf = pd.read_csv('https://raw.githubusercontent.com/nickmancol/python-cohorts/main/data/scanner_data.csv')\n\n\n\n\nShow code\n# show dataframe subset\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\n\n\n\n\n0\n1\n02/01/2016\n2547\n1\nX52\n0EM7L\n1.0\n3.13\n\n\n1\n2\n02/01/2016\n822\n2\n2ML\n68BRQ\n1.0\n5.46\n\n\n2\n3\n02/01/2016\n3686\n3\n0H2\nCZUZX\n1.0\n6.35\n\n\n3\n4\n02/01/2016\n3719\n4\n0H2\n549KK\n1.0\n5.59\n\n\n4\n5\n02/01/2016\n9200\n5\n0H2\nK8EHH\n1.0\n6.88\n\n\n\n\n\n\n\n\n\nShow code\n# drop unnamed column\ndf = df.drop(['Unnamed: 0'], axis = 1)\n# convert column to datetime\ndf.Date = pd.to_datetime(df.Date, format='%d/%m/%Y')\n\n\n\n\nShow code\n# get descriptive stats\ndf.describe(include='number')\n\n\n\n\n\n\n\n\n\nCustomer_ID\nTransaction_ID\nQuantity\nSales_Amount\n\n\n\n\ncount\n131706.000000\n131706.000000\n131706.000000\n131706.000000\n\n\nmean\n12386.450367\n32389.604187\n1.485311\n11.981524\n\n\nstd\n6086.447552\n18709.901238\n3.872667\n19.359699\n\n\nmin\n1.000000\n1.000000\n0.010000\n0.020000\n\n\n25%\n7349.000000\n16134.000000\n1.000000\n4.230000\n\n\n50%\n13496.000000\n32620.000000\n1.000000\n6.920000\n\n\n75%\n17306.000000\n48548.000000\n1.000000\n12.330000\n\n\nmax\n22625.000000\n64682.000000\n400.000000\n707.730000\n\n\n\n\n\n\n\n\n\nShow code\n# count duplicates\ndf.duplicated([\"Date\",\"Customer_ID\"]).sum()\n\n\nnp.int64(68979)\n\n\n\n\nShow code\nprint(f'The dataset has {df.size:,.2f} rows.')\n\n\nThe dataset has 921,942.00 rows.\n\n\n\n\nShow code\n# Group by date and customer and sum quantity and sales selecting last items\ndf = pd.DataFrame(df.groupby([\"Date\",\"Customer_ID\"]).agg({'Transaction_ID':'max'\n                                                          ,'SKU_Category':'max'\n                                                          ,'SKU':'max'\n                                                          ,'Quantity':'sum'\n                                                          ,'Sales_Amount':'sum'})\n                 ).reset_index()\n\n\n\n\nShow code\nprint(f'The dataset now has {df.size:,.2f} rows.')\n\n\nThe dataset now has 439,089.00 rows."
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#data-preparation-for-cohort-analysis",
    "href": "portfolio/posts/python/cohort_analysis.html#data-preparation-for-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "Data Preparation for Cohort Analysis",
    "text": "Data Preparation for Cohort Analysis\nTo run a cohort analysis, we‚Äôll need to:\n\nSplit the data into groups that can be analyzed on the basis of time\nAssign a cohort index for each transaction\nCreate two new columns by applying a lambda function to the date column in order to:\n\nCreate the tx_month column\nTransform tx_month to get the minimum value of tx_month per customer\nAssign tx_month per customer to the acq_month column\n\n\n\n\nShow code\n# create transaction month column with day 1\ndf['tx_month'] = df['Date'].apply(lambda x: dt.date(x.year, x.month,1))\n# create acquisition column based on minimum transaction month column by customer\ndf['acq_month'] = df.groupby('Customer_ID')['tx_month'].transform('min')\n# select rows where transaction dates differ from acquisition date\ndf.loc[df['tx_month'] != df['acq_month']].head()\n\n\n\n\n\n\n\n\n\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\ntx_month\nacq_month\n\n\n\n\n4701\n2016-02-01\n50\n5004\nW41\nZWFSY\n5.0\n2.86\n2016-02-01\n2016-01-01\n\n\n4702\n2016-02-01\n91\n4976\n2ML\n68BRQ\n1.0\n5.79\n2016-02-01\n2016-01-01\n\n\n4709\n2016-02-01\n366\n4852\nJ4R\nVGIW5\n2.0\n15.64\n2016-02-01\n2016-01-01\n\n\n4720\n2016-02-01\n889\n4900\nTEU\nA233P\n2.0\n11.87\n2016-02-01\n2016-01-01\n\n\n4723\n2016-02-01\n1115\n4933\nYMJ\nJNWFX\n1.0\n7.43\n2016-02-01\n2016-01-01\n\n\n\n\n\n\n\n\n\nShow code\n# Claculate the number of months elapsed between transaction and acquisition\ndef diff_month(x):\n    d1 = x['tx_month']\n    d2 = x[\"acq_month\"]\n    return ((d1.year - d2.year) * 12 + d1.month - d2.month)+1\n\n\n\n\nShow code\n# Store the number of months between transaction and acquisition month\ndf['cohort_idx'] = df.apply(lambda x: diff_month(x), axis=1)\n\n\n\n\nShow code\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\ntx_month\nacq_month\ncohort_idx\n\n\n\n\n0\n2016-01-02\n3\n90\nTW8\nY1M2E\n4.0\n10.92\n2016-01-01\n2016-01-01\n1\n\n\n1\n2016-01-02\n178\n84\nR6E\nHO1M5\n2.0\n58.99\n2016-01-01\n2016-01-01\n1\n\n\n2\n2016-01-02\n195\n107\nLGI\nVY2UB\n2.0\n13.10\n2016-01-01\n2016-01-01\n1\n\n\n3\n2016-01-02\n343\n134\nXG4\nZSVWE\n1.0\n6.75\n2016-01-01\n2016-01-01\n1\n\n\n4\n2016-01-02\n399\n136\nP42\nXJLWY\n2.0\n10.43\n2016-01-01\n2016-01-01\n1\n\n\n\n\n\n\n\n\n\nShow code\n# create cohort matrix by counting unique customers\ndef get_cohort_matrix(data, var='Customer_ID', fun=pd.Series.nunique):\n    # group by acquisition and cohort_id\n    cd = data.groupby(['acq_month', 'cohort_idx'])[var].apply(fun).reset_index()\n    # create pivot table with acquisition, cohort_id and counting customers\n    cc = cd.pivot_table(index = 'acq_month',\n                        columns = 'cohort_idx',\n                        values = var)\n    # calculate retention rate\n    # select first column of pivot table, i.e., first cohort_id\n    cs = cc.iloc[:,0]\n    # divide all values in the cohort matrix by the corresponding initial cohort_id size\n    retention = cc.divide(cs, axis = 0)\n    # create percentage and round decimals\n    retention = retention.round(3)\n    # return the matrix and retention rate for each cohort_id\n    return cc, retention\n\n\n\n\nShow code\ncc, retention = get_cohort_matrix(df)\n\n\n\n\nShow code\n# format index datetie\nretention.index = pd.to_datetime(retention.index).strftime('%b %Y')\n\n\nThe rows of the pivot table consist of the beginning of user activity or the month from which the user has started visiting the ecommerce website or has made the first purchase. The columns represent the user‚Äôs retention rate or how long has the user been coming back to purchase since his first time.\n\n\nShow code\nretention\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 2016\n1.0\n0.385\n0.302\n0.176\n0.078\n0.058\n0.045\n0.040\n0.027\n0.016\n0.012\n0.012\n\n\nFeb 2016\n1.0\n0.215\n0.131\n0.066\n0.043\n0.031\n0.031\n0.026\n0.010\n0.008\n0.006\nNaN\n\n\nMar 2016\n1.0\n0.282\n0.246\n0.227\n0.201\n0.197\n0.201\n0.194\n0.194\n0.201\nNaN\nNaN\n\n\nApr 2016\n1.0\n0.288\n0.250\n0.219\n0.224\n0.235\n0.224\n0.224\n0.230\nNaN\nNaN\nNaN\n\n\nMay 2016\n1.0\n0.238\n0.213\n0.192\n0.216\n0.210\n0.198\n0.199\nNaN\nNaN\nNaN\nNaN\n\n\nJun 2016\n1.0\n0.177\n0.175\n0.178\n0.177\n0.177\n0.191\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nJul 2016\n1.0\n0.142\n0.151\n0.156\n0.150\n0.166\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAug 2016\n1.0\n0.154\n0.130\n0.135\n0.119\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSep 2016\n1.0\n0.190\n0.162\n0.183\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nOct 2016\n1.0\n0.141\n0.150\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNov 2016\n1.0\n0.140\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nDec 2016\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12, 11))\nsns.heatmap(data=retention,\n            annot=True,\n            fmt='0.1%',\n            vmin=0.0,\n            vmax=0.1,\n            cmap='Blues')\nplt.show()"
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#segmented-by-quantity",
    "href": "portfolio/posts/python/cohort_analysis.html#segmented-by-quantity",
    "title": "Cohort Analysis",
    "section": "Segmented by Quantity",
    "text": "Segmented by Quantity\n\n\nShow code\ncc_q, ret_q = get_cohort_matrix(df, var='Quantity', fun=pd.Series.mean)\ncc_q\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01\n3.164760\n3.345814\n3.134125\n3.043550\n3.509021\n3.644632\n3.640000\n2.857143\n3.851575\n2.989362\n6.000000\n2.494737\n\n\n2016-02-01\n2.986317\n2.720292\n3.063029\n2.815029\n3.089138\n3.820513\n2.364706\n2.484375\n6.640000\n1.650000\n1.823529\nNaN\n\n\n2016-03-01\n2.919831\n3.354397\n3.753517\n3.625239\n3.923795\n3.637677\n3.749833\n3.790764\n3.421667\n3.419633\nNaN\nNaN\n\n\n2016-04-01\n2.811928\n3.308361\n3.530516\n2.856732\n3.268678\n3.268426\n3.589691\n3.126519\n3.459594\nNaN\nNaN\nNaN\n\n\n2016-05-01\n2.710384\n3.160431\n3.235338\n3.091508\n3.111241\n3.214499\n3.625133\n3.346479\nNaN\nNaN\nNaN\nNaN\n\n\n2016-06-01\n2.712752\n3.067361\n3.405827\n2.577969\n3.165492\n2.727486\n3.466247\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-07-01\n2.730425\n2.467593\n2.674009\n3.529661\n2.740444\n3.260802\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-08-01\n2.607402\n2.758663\n2.699218\n3.588889\n2.625000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-09-01\n2.918562\n3.190777\n3.692153\n3.604326\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-10-01\n2.674215\n2.563017\n3.124542\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-11-01\n2.553461\n2.943689\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-12-01\n2.655255\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12,11))\nsns.heatmap(cc_q, annot=True,cmap='Blues', fmt='.1%')\nplt.show()"
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#segmented-by-sales-amount",
    "href": "portfolio/posts/python/cohort_analysis.html#segmented-by-sales-amount",
    "title": "Cohort Analysis",
    "section": "Segmented by Sales Amount",
    "text": "Segmented by Sales Amount\n\n\nShow code\ncc_sa, ret_sa = get_cohort_matrix(df, var='Sales_Amount', fun=pd.Series.median)\ncc_sa\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01\n11.460\n13.290\n12.67\n12.470\n14.685\n15.730\n13.70\n12.05\n12.110\n11.165\n11.78\n13.75\n\n\n2016-02-01\n12.075\n12.360\n12.08\n11.190\n14.265\n14.705\n11.05\n9.91\n12.160\n9.860\n11.94\nNaN\n\n\n2016-03-01\n12.000\n13.650\n13.02\n13.785\n14.325\n13.140\n14.41\n16.25\n13.845\n14.870\nNaN\nNaN\n\n\n2016-04-01\n11.860\n12.620\n13.44\n12.630\n13.210\n13.430\n13.44\n13.80\n14.290\nNaN\nNaN\nNaN\n\n\n2016-05-01\n11.590\n13.535\n12.61\n13.100\n12.190\n12.160\n13.41\n13.12\nNaN\nNaN\nNaN\nNaN\n\n\n2016-06-01\n12.330\n12.690\n11.93\n11.500\n11.705\n12.645\n13.50\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-07-01\n12.030\n11.570\n12.08\n12.715\n13.750\n13.590\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-08-01\n11.880\n10.020\n11.38\n12.205\n11.620\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-09-01\n11.930\n10.740\n12.43\n13.780\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-10-01\n11.985\n11.010\n12.46\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-11-01\n12.130\n14.050\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-12-01\n12.740\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12,10))\nsns.heatmap(cc_sa, annot=True,cmap='Blues', fmt='.1%')\nplt.show()"
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#references",
    "href": "portfolio/posts/python/cohort_analysis.html#references",
    "title": "Cohort Analysis",
    "section": "References",
    "text": "References\n\n(2021). Bohorquez, N. Cohort Analysis with Python‚Äôs matplotlib, pandas, numpy and datetime in ActiveState, retrieved from https://www.activestate.com/blog/cohort-analysis-with-python"
  },
  {
    "objectID": "portfolio/posts/python/cohort_analysis.html#contact",
    "href": "portfolio/posts/python/cohort_analysis.html#contact",
    "title": "Cohort Analysis",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/regression_analysis_overview.html",
    "href": "portfolio/posts/python/regression_analysis_overview.html",
    "title": "Regression Analysis Overview",
    "section": "",
    "text": "Regression searches for relationships among variables. For example, you can want to understand how salaries depend on diverse features among employees of a company, such as experience, education level, role, city of employment, etc.\nData related to each employee represents one observation. The presumption is that salary depends on experience, education, role, and city.\nBy other hand, you may want to establish the relationship among housing prices on a particular area and number of bedrooms, distance to the city center, among other features.\nIn regression analysis, we consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that at least one of the features depends on the others, you‚Äôll need to establish a relationship among them. In scientific jargon, you need to find a function that maps some features to others sufficiently well.\nCommonly, we call the dependent feature dependent variable or response (generally denoted as y), and the independent features are called independent variables, inputs, or features.\nRegression problems usually works with continuous variables. Features, however,can be continuous, discrete, or even categorical.\nFinally, regression analysis is very important when you want to forecast a response using a new set of features. For example, you may want to predict gasoline consumption of a household for the next period given its price, number of residents in that household, car model, etc."
  },
  {
    "objectID": "portfolio/posts/python/regression_analysis_overview.html#contact",
    "href": "portfolio/posts/python/regression_analysis_overview.html#contact",
    "title": "Regression Analysis Overview",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/mexico_peace_index.html#what-does-it-measue",
    "href": "portfolio/posts/python/mexico_peace_index.html#what-does-it-measue",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "What does it measue?",
    "text": "What does it measue?\n\nMeasures peace across the entire country.\nAnalyzes recent violence and peace trends.\nEstimates the economic cost of violence in Mexico.\nIdentifies factors that drive peace and instability."
  },
  {
    "objectID": "portfolio/posts/python/mexico_peace_index.html#how-its-measured",
    "href": "portfolio/posts/python/mexico_peace_index.html#how-its-measured",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "How it‚Äôs measured",
    "text": "How it‚Äôs measured\nThe MPI isn‚Äôt just about crime rates. It considers 12 sub-indicators grouped into five major categories:\n\nOngoing Conflict\nSafety and Security\nDomesticized Conflict\nMilitarization\nIncarceration"
  },
  {
    "objectID": "portfolio/posts/python/mexico_peace_index.html#display-interactive-chart",
    "href": "portfolio/posts/python/mexico_peace_index.html#display-interactive-chart",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure¬†1: Mexico Peace Index\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\nYou can hover the mouse over the map to get additional information.\n\n\n\n\n\n\n\n\nFigure¬†2: Mexico Peace Index Variation"
  },
  {
    "objectID": "portfolio/posts/python/mexico_peace_index.html#references",
    "href": "portfolio/posts/python/mexico_peace_index.html#references",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "References",
    "text": "References\n\nMexico Peace Index"
  },
  {
    "objectID": "portfolio/posts/python/mexico_peace_index.html#contact",
    "href": "portfolio/posts/python/mexico_peace_index.html#contact",
    "title": "Mexico‚Äôs Peace index 2024",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html",
    "href": "portfolio/posts/python/dw_motherduck.html",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "",
    "text": "Figure¬†1: Data Warehouse by Niklas Lang"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#extraction-from-data-sources",
    "href": "portfolio/posts/python/dw_motherduck.html#extraction-from-data-sources",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Extraction from data sources",
    "text": "Extraction from data sources\n\ncsv files\n\n\nCode\ncsv_files = glob.glob('./datasets/*.csv')\n\n\n\n\nCode\nlist(enumerate(csv_files))\n\n\n[(0, './datasets/watercollection.csv'),\n (1, './datasets/ContainsNull.csv'),\n (2, './datasets/sales_info.csv'),\n (3, './datasets/cdmx-subway.csv'),\n (4, './datasets/airports.csv'),\n (5, './datasets/colors.csv'),\n (6, './datasets/sets.csv'),\n (7, './datasets/appl_stock.csv'),\n (8, './datasets/sales.csv')]\n\n\n\n\njson files\n\n\nCode\njson_files = glob.glob('./datasets/*.json')\n\n\n\n\nCode\nlist(enumerate(json_files))\n\n\n[(0, './datasets/prevalencia.json'), (1, './datasets/people.json')]\n\n\n\n\ndatabase tables\n\n\nCode\ndb_files = glob.glob('datasets/*.db')\n\n\n\n\nCode\nlist(enumerate(db_files))\n\n\n[(0, 'datasets/retail_db.db'), (1, 'datasets/restaurants.db')]"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#data-warehouse-creation",
    "href": "portfolio/posts/python/dw_motherduck.html#data-warehouse-creation",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data warehouse creation",
    "text": "Data warehouse creation\n\n\nCode\nconn = db.connect('my_database.db')"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#data-warehouse-load",
    "href": "portfolio/posts/python/dw_motherduck.html#data-warehouse-load",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data warehouse load",
    "text": "Data warehouse load\n\n\nCode\nconn.sql(f\"create or replace table water_collection as select * from '{csv_files[0]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table contains_null as select * from '{csv_files[1]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sales_info as select * from '{csv_files[2]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table cdmx_subway as select * from '{csv_files[3]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table airports as select * from '{csv_files[4]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table colors as select * from '{csv_files[5]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sets as select * from '{csv_files[6]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table appl_stock as select * from '{csv_files[7]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sales as select * from '{csv_files[8]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table prevalencia as select * from '{json_files[0]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table people as select * from '{json_files[1]}' \")\n\n\n\n\nCode\nretail = db.connect('./datasets/retail_db.db')\nretail.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     name     ‚îÇ\n‚îÇ   varchar    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ retail_sales ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nretail_sales_pl = retail.sql('select * from retail_sales').pl()\n\n\n\n\nCode\nconn.execute(\"create or replace table retail_sales as from retail_sales_pl\");\n\n\n\n\nCode\nrestaurants = db.connect('./datasets/restaurants.db')\nrestaurants.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    name     ‚îÇ\n‚îÇ   varchar   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ restaurants ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nrestaurants_pl = restaurants.sql('select * from restaurants').pl()\n\n\n\n\nCode\nconn.execute(\"create or replace table restaurants as from restaurants_pl\");"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#data-retrieval",
    "href": "portfolio/posts/python/dw_motherduck.html#data-retrieval",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data retrieval",
    "text": "Data retrieval\n\n\nCode\nconn.sql('show databases')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ database_name ‚îÇ\n‚îÇ    varchar    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ my_database   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nconn.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       name       ‚îÇ\n‚îÇ     varchar      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ airports         ‚îÇ\n‚îÇ appl_stock       ‚îÇ\n‚îÇ cdmx_subway      ‚îÇ\n‚îÇ colors           ‚îÇ\n‚îÇ contains_null    ‚îÇ\n‚îÇ people           ‚îÇ\n‚îÇ prevalencia      ‚îÇ\n‚îÇ restaurants      ‚îÇ\n‚îÇ retail_sales     ‚îÇ\n‚îÇ sales            ‚îÇ\n‚îÇ sales_info       ‚îÇ\n‚îÇ sets             ‚îÇ\n‚îÇ water_collection ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     13 rows      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\nconn.sql('select * from restaurants limit 5').pl()\n\n\n\nshape: (5, 6)\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\nstr\ni64\nf64\nstr\nstr\ni64\n\n\n\n\n\"The Golden Wok\"\n1477\n33.620488\n\"Berlin\"\n\"American\"\n5\n\n\n\"Greek Gyros\"\n770\n68.388874\n\"New York\"\n\"French\"\n1\n\n\n\"Taste of Italy\"\n4420\n88.23168\n\"Amsterdam\"\n\"Chinese\"\n0\n\n\n\"Midnight Diner\"\n2155\n12.965985\n\"Lisbon\"\n\"Mexican\"\n1\n\n\n\"Taste of Italy\"\n3375\n52.785226\n\"Sydney\"\n\"Chinese\"\n1"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#cloud-data-warehouse-with-motherduck",
    "href": "portfolio/posts/python/dw_motherduck.html#cloud-data-warehouse-with-motherduck",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Cloud Data Warehouse with Motherduck",
    "text": "Cloud Data Warehouse with Motherduck\n\n\nCode\ndw = db.connect('md')\n\n\n\n\nCode\ndw.sql('select current_database()').show()\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ current_database() ‚îÇ\n‚îÇ      varchar       ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ my_portfolio       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#convert-queries-from-local-database-to-polars",
    "href": "portfolio/posts/python/dw_motherduck.html#convert-queries-from-local-database-to-polars",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Convert queries from local database to polars",
    "text": "Convert queries from local database to polars\n\n\nCode\nairports_pl = conn.sql('select * from airports').pl()\nappl_stock_pl = conn.sql('select * from appl_stock').pl()\ncdmx_subway_pl = conn.sql('select * from cdmx_subway').pl()\ncolors_pl = conn.sql('select * from colors').pl()\ncontains_null_pl = conn.sql('select * from contains_null').pl()\npeople_pl = conn.sql('select * from people').pl()\nprevalencia_pl = conn.sql('select * from prevalencia').pl()\nrestaurants_pl = conn.sql('select * from restaurants').pl()\nretail_sales_pl = conn.sql('select * from retail_sales').pl()\nsales_pl = conn.sql('select * from sales').pl()\nsales_info_pl = conn.sql('select * from sales_info').pl()\nsets_pl = conn.sql('select * from sets').pl()\nwater_collection_pl = conn.sql('select * from water_collection').pl()"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#upload-dataframes-to-motherduck",
    "href": "portfolio/posts/python/dw_motherduck.html#upload-dataframes-to-motherduck",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Upload dataframes to MotherDuck",
    "text": "Upload dataframes to MotherDuck\n\n\nCode\ndw.sql(f\"create or replace table airports as select * from airports_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table appl_stock as select * from appl_stock_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table cdmx_subway as select * from cdmx_subway_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table colors as select * from colors_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table contains_null as select * from contains_null_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table people as select * from people_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table prevalencia as select * from prevalencia_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table restaurants as select * from restaurants_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table retail_sales as select * from retail_sales_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sales as select * from sales_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sales_info as select * from sales_info_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sets as select * from sets_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table water_collection as select * from water_collection_pl\");"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#check-uploaded-tables",
    "href": "portfolio/posts/python/dw_motherduck.html#check-uploaded-tables",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Check uploaded tables",
    "text": "Check uploaded tables\n\n\nCode\ndw.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       name       ‚îÇ\n‚îÇ     varchar      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ airports         ‚îÇ\n‚îÇ appl_stock       ‚îÇ\n‚îÇ cdmx_subway      ‚îÇ\n‚îÇ colors           ‚îÇ\n‚îÇ contains_null    ‚îÇ\n‚îÇ people           ‚îÇ\n‚îÇ prevalencia      ‚îÇ\n‚îÇ restaurants      ‚îÇ\n‚îÇ retail_sales     ‚îÇ\n‚îÇ sales            ‚îÇ\n‚îÇ sales_info       ‚îÇ\n‚îÇ sets             ‚îÇ\n‚îÇ water_collection ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     13 rows      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#close-all-database-connections",
    "href": "portfolio/posts/python/dw_motherduck.html#close-all-database-connections",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Close all database connections",
    "text": "Close all database connections\n\n\nCode\n# close db connections\nconn.close()\nretail.close()\nrestaurants.close()\ndw.close()"
  },
  {
    "objectID": "portfolio/posts/python/dw_motherduck.html#contact",
    "href": "portfolio/posts/python/dw_motherduck.html#contact",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#beers-table",
    "href": "portfolio/posts/python/duckdb_example.html#beers-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Beers table",
    "text": "Beers table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS beers (\n        CodC integer,\n        Package varchar(255),\n        Capacity float,\n        Stock integer\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO beers\n    VALUES (1, 'Botella', 0.2, 3600),\n        (2, 'Botella', 0.33, 1200),\n        (3, 'Lata', 0.33, 2400),\n        (4, 'Botella', 1, 288),\n        (5, 'Barril', 60, 30)\n''')\n\n\nRetrieve data\n\n\nShow code\n# Showing in polars dataframe\ndb.sql('SELECT * FROM beers').df()\n\n\n\n\n\n\n\n\n\nCodC\nPackage\nCapacity\nStock\n\n\n\n\n0\n1\nBotella\n0.20\n3600\n\n\n1\n2\nBotella\n0.33\n1200\n\n\n2\n3\nLata\n0.33\n2400\n\n\n3\n4\nBotella\n1.00\n288\n\n\n4\n5\nBarril\n60.00\n30"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#bars-table",
    "href": "portfolio/posts/python/duckdb_example.html#bars-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Bars table",
    "text": "Bars table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS bars (\n        CodB integer,\n        Name varchar(255),\n        Cif varchar(255),\n        Location varchar(255)\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO bars\n    VALUES (1, 'Stop', '111111X', 'Villa Botijo'),\n        (2, 'Las Vegas', '222222Y', 'Villa Botijo'),\n        (3, 'Club Social', '', 'Las Ranas'),\n        (4, 'Otra Ronda', '333333Z', 'La Esponja')\n''')\n\n\nRetrieve data\n\n\nShow code\n# showing in pandas dataframe\ndb.sql('SELECT * FROM bars').df()\n\n\n\n\n\n\n\n\n\nCodB\nName\nCif\nLocation\n\n\n\n\n0\n1\nStop\n111111X\nVilla Botijo\n\n\n1\n2\nLas Vegas\n222222Y\nVilla Botijo\n\n\n2\n3\nClub Social\n\nLas Ranas\n\n\n3\n4\nOtra Ronda\n333333Z\nLa Esponja"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#employees-table",
    "href": "portfolio/posts/python/duckdb_example.html#employees-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Employees table",
    "text": "Employees table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS employees (\n        CodE integer,\n        Name varchar(255),\n        Salary float\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO employees\n    VALUES (1, 'John Doe', 120000),\n        (2, 'Vicent Meren', 110000),\n        (3, 'Tom Simpson', 100000)\n''')\n\n\nRetrieve data\n\n\nShow code\ndb.sql('SELECT * FROM employees').df()\n\n\n\n\n\n\n\n\n\nCodE\nName\nSalary\n\n\n\n\n0\n1\nJohn Doe\n120000.0\n\n\n1\n2\nVicent Meren\n110000.0\n\n\n2\n3\nTom Simpson\n100000.0"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#delivery-table",
    "href": "portfolio/posts/python/duckdb_example.html#delivery-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Delivery table",
    "text": "Delivery table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS delivery (\n        CodE integer,\n        CodB integer,\n        CodC integer,\n        Delivery_date date,\n        Quantity integer\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO delivery\n    VALUES (1, 1, 1, '2005-10-21', 240),\n        (1, 1, 2, '2005-10-21', 48),\n        (1, 2, 3, '2005-10-22', 60),\n        (1, 4, 5, '2005-10-22', 4),\n        (2, 2, 3, '2005-10-23', 48),\n        (2, 2, 5, '2005-10-23', 2),\n        (2, 4, 1, '2005-10-24', 480),\n        (2, 4, 2, '2005-10-24', 72),\n        (3, 3, 3, '2005-10-24', 48),\n        (3, 3, 4, '2005-10-25', 20)\n''')\n\n\nRetrieve data\n\n\nShow code\ndb.sql('SELECT * FROM delivery').df()\n\n\n\n\n\n\n\n\n\nCodE\nCodB\nCodC\nDelivery_date\nQuantity\n\n\n\n\n0\n1\n1\n1\n2005-10-21\n240\n\n\n1\n1\n1\n2\n2005-10-21\n48\n\n\n2\n1\n2\n3\n2005-10-22\n60\n\n\n3\n1\n4\n5\n2005-10-22\n4\n\n\n4\n2\n2\n3\n2005-10-23\n48\n\n\n5\n2\n2\n5\n2005-10-23\n2\n\n\n6\n2\n4\n1\n2005-10-24\n480\n\n\n7\n2\n4\n2\n2005-10-24\n72\n\n\n8\n3\n3\n3\n2005-10-24\n48\n\n\n9\n3\n3\n4\n2005-10-25\n20"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-1",
    "href": "portfolio/posts/python/duckdb_example.html#query-1",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 1",
    "text": "Query 1\nObtain the name of the employees who delivered to the Stop bar during the week of October 17 to 23, 2005.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT r.Delivery_date\n        ,e.Name\n        ,b.Name\n    FROM delivery AS r\n    LEFT JOIN employees AS e ON r.CodE = e.CodE\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    WHERE b.Name = 'Stop'\n        AND r.Delivery_date BETWEEN '2005-10-17' AND '2005-10-23'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Delivery_date ‚îÇ   Name   ‚îÇ  Name   ‚îÇ\n‚îÇ     date      ‚îÇ varchar  ‚îÇ varchar ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 2005-10-21    ‚îÇ John Doe ‚îÇ Stop    ‚îÇ\n‚îÇ 2005-10-21    ‚îÇ John Doe ‚îÇ Stop    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-2",
    "href": "portfolio/posts/python/duckdb_example.html#query-2",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 2",
    "text": "Query 2\nObtain the CIF and name of the bars to which bottle-type beer with a capacity of less than 1 liter has been distributed, ordered by location.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Cif\n        ,b.Name\n        ,c.Package\n        ,c.Capacity\n        ,b.Location\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Package = 'Botella'\n        AND c.Capacity &lt; 1.0\n    ORDER BY b.Location\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Cif   ‚îÇ    Name    ‚îÇ Package ‚îÇ Capacity ‚îÇ   Location   ‚îÇ\n‚îÇ varchar ‚îÇ  varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ   varchar    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 333333Z ‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ      0.2 ‚îÇ La Esponja   ‚îÇ\n‚îÇ 333333Z ‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ     0.33 ‚îÇ La Esponja   ‚îÇ\n‚îÇ 111111X ‚îÇ Stop       ‚îÇ Botella ‚îÇ      0.2 ‚îÇ Villa Botijo ‚îÇ\n‚îÇ 111111X ‚îÇ Stop       ‚îÇ Botella ‚îÇ     0.33 ‚îÇ Villa Botijo ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-3",
    "href": "portfolio/posts/python/duckdb_example.html#query-3",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 3",
    "text": "Query 3\nObtain the deliveries (name of the bar, container and capacity of the drink, date and quantity) made by Prudencio Caminero.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n        ,r.Delivery_date\n        ,r.Quantity\n        ,e.Name\n        ,e.CodE\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    LEFT JOIN employees AS e ON e.CodE = r.CodE\n    WHERE e.Name ILIKE '%doe%'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name    ‚îÇ Package ‚îÇ Capacity ‚îÇ Delivery_date ‚îÇ Quantity ‚îÇ   Name   ‚îÇ CodE  ‚îÇ\n‚îÇ  varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ     date      ‚îÇ  int32   ‚îÇ varchar  ‚îÇ int32 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Stop       ‚îÇ Botella ‚îÇ      0.2 ‚îÇ 2005-10-21    ‚îÇ      240 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îÇ Stop       ‚îÇ Botella ‚îÇ     0.33 ‚îÇ 2005-10-21    ‚îÇ       48 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îÇ Las Vegas  ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ 2005-10-22    ‚îÇ       60 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îÇ Otra Ronda ‚îÇ Barril  ‚îÇ     60.0 ‚îÇ 2005-10-22    ‚îÇ        4 ‚îÇ John Doe ‚îÇ     1 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-4",
    "href": "portfolio/posts/python/duckdb_example.html#query-4",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 4",
    "text": "Query 4\nObtain the bars to which bottle-type containers with a capacity of 0.2 or 0.33 have been distributed.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Package = 'Botella'\n        AND c.Capacity IN (0.2, 0.33)\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name    ‚îÇ Package ‚îÇ Capacity ‚îÇ\n‚îÇ  varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Stop       ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Stop       ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Otra Ronda ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-5",
    "href": "portfolio/posts/python/duckdb_example.html#query-5",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 5",
    "text": "Query 5\nName of the employees who have distributed bottled beers to the ‚ÄúStop‚Äù and ‚ÄúLas Vegas‚Äù bars.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT e.Name\n        ,b.Name\n        ,c.Package\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    LEFT JOIN employees AS e On e.CodE = r.CodE\n    WHERE b.Name IN ('Stop', 'Las Vegas')\n        AND c.Package = 'Botella'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Name   ‚îÇ  Name   ‚îÇ Package ‚îÇ\n‚îÇ varchar  ‚îÇ varchar ‚îÇ varchar ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ John Doe ‚îÇ Stop    ‚îÇ Botella ‚îÇ\n‚îÇ John Doe ‚îÇ Stop    ‚îÇ Botella ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-6",
    "href": "portfolio/posts/python/duckdb_example.html#query-6",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 6",
    "text": "Query 6\nObtain the name and number of trips that each employee has made outside of Villa Botijo.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT e.Name\n        ,COUNT(b.Location) AS Travles\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN employees AS e On e.CodE = r.CodE\n    WHERE b.Location &lt;&gt; 'Villa Botijo'\n    GROUP BY 1\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     Name     ‚îÇ Travles ‚îÇ\n‚îÇ   varchar    ‚îÇ  int64  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ John Doe     ‚îÇ       1 ‚îÇ\n‚îÇ Tom Simpson  ‚îÇ       2 ‚îÇ\n‚îÇ Vicent Meren ‚îÇ       2 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-7",
    "href": "portfolio/posts/python/duckdb_example.html#query-7",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 7",
    "text": "Query 7\nObtain the name and location of the bar that has purchased the most liters of beer.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,b.Location\n        ,MAX(r.Quantity) AS Liters\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    GROUP BY 1, 2\n    ORDER BY 3 DESC\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name     ‚îÇ   Location   ‚îÇ Liters ‚îÇ\n‚îÇ   varchar   ‚îÇ   varchar    ‚îÇ int32  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Otra Ronda  ‚îÇ La Esponja   ‚îÇ    480 ‚îÇ\n‚îÇ Stop        ‚îÇ Villa Botijo ‚îÇ    240 ‚îÇ\n‚îÇ Las Vegas   ‚îÇ Villa Botijo ‚îÇ     60 ‚îÇ\n‚îÇ Club Social ‚îÇ Las Ranas    ‚îÇ     48 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-8",
    "href": "portfolio/posts/python/duckdb_example.html#query-8",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 8",
    "text": "Query 8\nObtain bars that have purchased all types of beer with bottle packaging and a capacity less than 1 liter.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Capacity IN (SELECT c.Capacity\n                            FROM beers AS c\n                            WHERE c.Package = 'Botella'\n                                AND c.Capacity &lt; 1.0)\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Name     ‚îÇ Package ‚îÇ Capacity ‚îÇ\n‚îÇ   varchar   ‚îÇ varchar ‚îÇ  float   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Stop        ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Stop        ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îÇ Las Vegas   ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ\n‚îÇ Las Vegas   ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ\n‚îÇ Otra Ronda  ‚îÇ Botella ‚îÇ      0.2 ‚îÇ\n‚îÇ Otra Ronda  ‚îÇ Botella ‚îÇ     0.33 ‚îÇ\n‚îÇ Club Social ‚îÇ Lata    ‚îÇ     0.33 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-9",
    "href": "portfolio/posts/python/duckdb_example.html#query-9",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 9",
    "text": "Query 9\nRaise the salary of the employee who has worked the most days by 5%.\n\n\nShow code\n# modify database records\ndb.sql(\n    '''\n    -- Raise 5% salary of workers\n    INSERT INTO employees (CodE, Name, Salary)\n    VALUES (1, 'John Doe', 120000*1.05),\n            (2, 'Vicent Meren', 110000*1.05)\n    '''\n)\n\n\n\n\nShow code\n# verify inserted record\ndb.sql(\n    '''\n    SELECT e.*\n    FROM employees AS e\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ CodE  ‚îÇ     Name     ‚îÇ  Salary  ‚îÇ\n‚îÇ int32 ‚îÇ   varchar    ‚îÇ  float   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     1 ‚îÇ John Doe     ‚îÇ 120000.0 ‚îÇ\n‚îÇ     2 ‚îÇ Vicent Meren ‚îÇ 110000.0 ‚îÇ\n‚îÇ     3 ‚îÇ Tom Simpson  ‚îÇ 100000.0 ‚îÇ\n‚îÇ     1 ‚îÇ John Doe     ‚îÇ 126000.0 ‚îÇ\n‚îÇ     2 ‚îÇ Vicent Meren ‚îÇ 115500.0 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/duckdb_example.html#query-10",
    "href": "portfolio/posts/python/duckdb_example.html#query-10",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 10",
    "text": "Query 10\nInsert a new distribution from the employee ‚ÄúVicent Meren‚Äù to the ‚ÄúStop‚Äù bar of 48 canned beers on 2005-10-26.\n\n\nShow code\n# Insert new record\ndb.sql(\n    '''\n    INSERT INTO delivery (CodE, CodB, CodC, Delivery_date, Quantity)\n    VALUES (2, 1, 3, '2005-10-26', 48)\n    '''\n)\n\n\n\n\nShow code\n# verify inserted record\ndb.sql(\n    '''\n    SELECT * \n    FROM delivery\n    WHERE Delivery_date = '2005-10-26'\n    '''\n)\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ CodE  ‚îÇ CodB  ‚îÇ CodC  ‚îÇ Delivery_date ‚îÇ Quantity ‚îÇ\n‚îÇ int32 ‚îÇ int32 ‚îÇ int32 ‚îÇ     date      ‚îÇ  int32   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ     2 ‚îÇ     1 ‚îÇ     3 ‚îÇ 2005-10-26    ‚îÇ       48 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "portfolio/posts/python/water collection system.html",
    "href": "portfolio/posts/python/water collection system.html",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "",
    "text": "Rainwater harvesting systems are a prominent water collection method in Mexico, especially in areas facing water scarcity.\nThese systems, called ‚ÄúSistemas de Captaci√≥n de Agua de Lluvia‚Äù (SCALL) offer several benefits:"
  },
  {
    "objectID": "portfolio/posts/python/water collection system.html#contact",
    "href": "portfolio/posts/python/water collection system.html#contact",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/dashboards.html",
    "href": "portfolio/posts/python/dashboards.html",
    "title": "Creating Dashboards with Python",
    "section": "",
    "text": "Customization\n\nUnlike pre-built tools, Python allows you to tailor dashboards to your specific needs. Present the data exactly how you want, focusing on the metrics that matter most.\n\nInteractivity\n\nPython lets you create dynamic dashboards where users can explore data, filter information, and drill down for deeper analysis. This fosters engagement and a more intuitive understanding of the data.\n\nPython‚Äôs Ecosystem\n\nPython boasts a rich landscape of libraries like Plotly, Altair and Pyshiny. These libraries provide a plethora of visualization options, enabling you to create clear, compelling, and informative charts, graphs, and other visual elements.\n\nEfficiency\n\nPython streamlines the process of data manipulation and analysis. Automate repetitive tasks and integrate data from various sources, saving you valuable time and effort.\n\nOpen-Source\n\nBeing open-source, Python offers significant cost savings compared to proprietary dashboarding tools. Plus, the large and active Python community provides ample resources and support to help you on your journey.\n\nSharability\n\nPython-built dashboards are readily shareable across your organization. Disseminate valuable insights to colleagues and stakeholders, fostering better collaboration and data-driven decision-making.\nBy leveraging Python for dashboard creation, you gain control, flexibility, and a powerful toolset to unlock the true potential of your data. Get ready to transform information into clear, actionable insights that drive informed decisions and positive outcomes.\n\n\n\n\n\n\n\n\nInfo\n\n\n\nYou can interact with the dashboard herein or you can click here to open in a new page."
  },
  {
    "objectID": "portfolio/posts/python/dashboards.html#contact",
    "href": "portfolio/posts/python/dashboards.html#contact",
    "title": "Creating Dashboards with Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/polynomial_regression.html",
    "href": "portfolio/posts/python/polynomial_regression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Oftentimes we‚Äôll encounter data where the relationship between the feature(s) and the response variable can‚Äôt be best described with a straight line. In these cases, we should use polynomial regression.\nAn example of a polynomial, coud be:\n3x4‚Äì7x3+2x2+113x^4 ‚Äì 7x^3 + 2x^2 + 11\nTerminology\n\nDegree of a polynomial: The highest power in polynomial. In our example, 4\nCoefficient: Each constant (3, 7, 2, 11) in polynomial is a coefficient. In polynomial regression, these coefficients will be estimated\nLeading term: The term with the highest power (3x43x^4). It determines the polynomial‚Äôs graph behavior\nLeading coefficient: The coefficient of the leading term (3)\nConstant term: The y intercept, it never changes: no matter what the value of x is, the constant term remains the same\n\n\n\nLet‚Äôs return to $ 3x4 - 7x3 + 2x2 + 11 $, if we write a polynomial‚Äôs terms from the highest degree term to the lowest degree term, it‚Äôs called a polynomial‚Äôs standard form.\nIn the context of machine learning, you‚Äôll often see it reversed:\ny=Œ≤0+Œ≤1x+Œ≤2x2+‚Ä¶+Œ≤nxny = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_nx^n\nwhere:\n\ny is the response variable we want to predict\nx is the feature\nŒ≤0\\beta_0 is the y intercept\n\nThe other √üs are the coefficients/parameters we‚Äôd like to find when we train our model on the available x and y values\n\nn is the degree of the polynomial (the higher n is, the more complex curved lines you can create)\nThe above polynomial regression formula is very similar to the multiple linear regression formula:\n\ny=Œ≤0+Œ≤1x+Œ≤2x+‚Ä¶+Œ≤nxy = \\beta_0 + \\beta_1x + \\beta_2x + \\dots + \\beta_nx\nIt‚Äôs not a coincidence: polynomial regression is a linear model used for describing non-linear relationships\nHow is this possible? The magic lies in creating new features by raising the original features to a power\nLinear regression is just a first-degree polynomial. Polynomial regression uses higher-degree polynomials. Both of them are linear models, but the first results in a straight line, the latter gives you a curved line.\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\n\nCode\n# create dummy dataset\nx = np.arange(0, 30)\ny = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75,\n     88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]\n\n\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create an polynomial instance\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n\nDegree = 2 means that we want to work with a 2nd degree polynomial,\ny=Œ≤0+Œ≤1x+Œ≤2x2y = \\beta_0 + \\beta_1x + \\beta_2x^2\n\n\nCode\npoly_features = poly.fit_transform(x.reshape(-1, 1))\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n\nIt may seem confusing, why are we importing LinearRegression module? üòÆ\nWe just have to remind that polynomial regression is a linear model, that‚Äôs why we import LinearRegression. üôÇ\n\n\nCode\n# Create a LinearRegression() instance\npoly_reg_model = LinearRegression()\n\n\n\n\nCode\n# Fit model to data\npoly_reg_model.fit(poly_features, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Predict responses\ny_predicted = poly_reg_model.predict(poly_features)\n\n\n\n\nCode\ny_predicted\n\n\narray([  1.70806452,   3.04187987,   4.70292388,   6.69119657,\n         9.00669792,  11.64942794,  14.61938662,  17.91657397,\n        21.54098999,  25.49263467,  29.77150802,  34.37761004,\n        39.31094073,  44.57150008,  50.1592881 ,  56.07430478,\n        62.31655014,  68.88602415,  75.78272684,  83.00665819,\n        90.55781821,  98.4362069 , 106.64182425, 115.17467027,\n       124.03474495, 133.22204831, 142.73658033, 152.57834101,\n       162.74733037, 173.24354839])\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"A Basic Polynomial Regression Example\", size=16)\nplt.scatter(x, y)\nplt.plot(x, y_predicted, c=\"green\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create data\nnp.random.seed(1)\nx_1 = np.absolute(np.random.randn(100, 1) * 10)\nx_2 = np.absolute(np.random.randn(100, 1) * 30)\ny = 2*x_1**2 + 3*x_1 + 2 + np.random.randn(100, 1)*20\n\n\n\n\nCode\n# Create visual\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes[0].scatter(x_1, y)\naxes[1].scatter(x_2, y)\naxes[0].set_title(\"x_1 plotted\")\naxes[1].set_title(\"x_2 plotted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create dataframe\ndf = pd.DataFrame(\n    {\"x_1\":x_1.reshape(100,),\n     \"x_2\":x_2.reshape(100,),\n     \"y\":y.reshape(100,)},\n        index=range(0,100))\ndf\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      y\n    \n  \n  \n    \n      0\n      16.243454\n      13.413857\n      570.412369\n    \n    \n      1\n      6.117564\n      36.735231\n      111.681987\n    \n    \n      2\n      5.281718\n      12.104749\n      62.392124\n    \n    \n      3\n      10.729686\n      17.807356\n      303.538953\n    \n    \n      4\n      8.654076\n      32.847355\n      151.109269\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.773401\n      48.823150\n      -0.430738\n    \n    \n      96\n      3.438537\n      18.069578\n      44.308720\n    \n    \n      97\n      0.435969\n      12.608466\n      19.383456\n    \n    \n      98\n      6.200008\n      24.328550\n      78.371729\n    \n    \n      99\n      6.980320\n      31.333263\n      132.108914\n    \n  \n\n100 rows √ó 3 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Define train and test sets\nX, y = df[[\"x_1\", \"x_2\"]], df[\"y\"]\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Create polynomial regression\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Test model\npoly_reg_y_predicted = poly_reg_model.predict(X_test)\n\n\nThe formula for Root Mean Square Error is:\nRMSE=1n‚àëi=1n(yi‚àíyÃÇi)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\nThe smaller the RMSE metric the better the model\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\npoly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))\nprint(f'Polynomial regression\\nRMSE: {poly_reg_rmse:.2f}')\n\n\nPolynomial regression\nRMSE: 20.94\n\n\n\n\n\n\nCode\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Create linear regression instance\nlin_reg_model = LinearRegression()\n# Fit regression model\nlin_reg_model.fit(X_train, y_train)\n# Create predictions\nlin_reg_y_predicted = lin_reg_model.predict(X_test)\n# Calculate RMSE\nlin_reg_rmse = np.sqrt(mean_squared_error(y_test, lin_reg_y_predicted))\n# Print results\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:,.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nIn the train_test_split method we use X instead of poly_features, and it‚Äôs for a good reason.\nX contains our two original features (x_1 and x_2), so our linear regression model takes the form of:\ny=Œ≤0+Œ≤1x1+Œ≤2x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\n\nCode\nlin_reg_model.coef_\n\n\narray([43.73176255, -0.53140809])\n\n\n\n\nCode\nlin_reg_model.intercept_\n\n\nnp.float64(-117.07280081594811)\n\n\nOn the other hand, poly_features contains new features as well, created out of x_1 and x_2, so our polynomial regression model (based on a 2nd degree polynomial with two features) looks like this:\ny=Œ≤0+Œ≤1x1+Œ≤2x2+Œ≤3x12+Œ≤4x22+Œ≤5x1x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_2^2 + \\beta_5x_1x_2\nThis is because poly.fit_transform(X) added three new features to the original two (x1 (x1x_1) and x2 (x2x_2)): x12x_1^2, x22x_2^2 and x1x2x_1x_2\nx12x_1^2 and x22x_2^2 need no explanation, as we‚Äôve already covered how they are created in the ‚ÄúCoding a polynomial regression model with scikit-learn‚Äù section.\nWhat‚Äôs more interesting is x1x2x_1x_2 ‚Äì when two features are multiplied by each other, it‚Äôs called an interaction term. An interaction term accounts for the fact that one variable‚Äôs value may depend on another variable‚Äôs value (more on this here). poly.fit_transform() automatically created this interaction term for us, isn‚Äôt that cool? üôÇ\n\n\nCode\npoly_reg_model.coef_\n\n\narray([ 3.61945509, -1.0859955 ,  1.89905813,  0.0207338 ,  0.01300394])\n\n\n\n\nCode\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nThe RMSE for the polynomial regression model is 20.94, while the RMSE for the linear regression model is 62.3. The polynomial regression model performs almost 3 times better than the linear regression model!\n\n\n\n\nIn this notebook, we have been able to expose that a basic understanding of polynomial regression. And we have shown RMSE metric for comparing the performance among different ML models.\nWe used a 2nd degree polynomial for ourthis example. Naturally, we should always test and trial before deploying a model to find what degree of polynomial performs best.\n\n\n\n\nUjhelyi T. Polynomial regression I mainly based on.\nHow to become a Data Scientist\n\n\n\n\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/polynomial_regression.html#environment-settings",
    "href": "portfolio/posts/python/polynomial_regression.html#environment-settings",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\n\nCode\n# create dummy dataset\nx = np.arange(0, 30)\ny = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75,\n     88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]\n\n\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create an polynomial instance\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n\nDegree = 2 means that we want to work with a 2nd degree polynomial,\ny=Œ≤0+Œ≤1x+Œ≤2x2y = \\beta_0 + \\beta_1x + \\beta_2x^2\n\n\nCode\npoly_features = poly.fit_transform(x.reshape(-1, 1))\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n\nIt may seem confusing, why are we importing LinearRegression module? üòÆ\nWe just have to remind that polynomial regression is a linear model, that‚Äôs why we import LinearRegression. üôÇ\n\n\nCode\n# Create a LinearRegression() instance\npoly_reg_model = LinearRegression()\n\n\n\n\nCode\n# Fit model to data\npoly_reg_model.fit(poly_features, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Predict responses\ny_predicted = poly_reg_model.predict(poly_features)\n\n\n\n\nCode\ny_predicted\n\n\narray([  1.70806452,   3.04187987,   4.70292388,   6.69119657,\n         9.00669792,  11.64942794,  14.61938662,  17.91657397,\n        21.54098999,  25.49263467,  29.77150802,  34.37761004,\n        39.31094073,  44.57150008,  50.1592881 ,  56.07430478,\n        62.31655014,  68.88602415,  75.78272684,  83.00665819,\n        90.55781821,  98.4362069 , 106.64182425, 115.17467027,\n       124.03474495, 133.22204831, 142.73658033, 152.57834101,\n       162.74733037, 173.24354839])\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"A Basic Polynomial Regression Example\", size=16)\nplt.scatter(x, y)\nplt.plot(x, y_predicted, c=\"green\")\nplt.show()"
  },
  {
    "objectID": "portfolio/posts/python/polynomial_regression.html#polynomial-regression-with-multiple-features",
    "href": "portfolio/posts/python/polynomial_regression.html#polynomial-regression-with-multiple-features",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Code\n# Create data\nnp.random.seed(1)\nx_1 = np.absolute(np.random.randn(100, 1) * 10)\nx_2 = np.absolute(np.random.randn(100, 1) * 30)\ny = 2*x_1**2 + 3*x_1 + 2 + np.random.randn(100, 1)*20\n\n\n\n\nCode\n# Create visual\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes[0].scatter(x_1, y)\naxes[1].scatter(x_2, y)\naxes[0].set_title(\"x_1 plotted\")\naxes[1].set_title(\"x_2 plotted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create dataframe\ndf = pd.DataFrame(\n    {\"x_1\":x_1.reshape(100,),\n     \"x_2\":x_2.reshape(100,),\n     \"y\":y.reshape(100,)},\n        index=range(0,100))\ndf\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      y\n    \n  \n  \n    \n      0\n      16.243454\n      13.413857\n      570.412369\n    \n    \n      1\n      6.117564\n      36.735231\n      111.681987\n    \n    \n      2\n      5.281718\n      12.104749\n      62.392124\n    \n    \n      3\n      10.729686\n      17.807356\n      303.538953\n    \n    \n      4\n      8.654076\n      32.847355\n      151.109269\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.773401\n      48.823150\n      -0.430738\n    \n    \n      96\n      3.438537\n      18.069578\n      44.308720\n    \n    \n      97\n      0.435969\n      12.608466\n      19.383456\n    \n    \n      98\n      6.200008\n      24.328550\n      78.371729\n    \n    \n      99\n      6.980320\n      31.333263\n      132.108914\n    \n  \n\n100 rows √ó 3 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Define train and test sets\nX, y = df[[\"x_1\", \"x_2\"]], df[\"y\"]\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Create polynomial regression\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Test model\npoly_reg_y_predicted = poly_reg_model.predict(X_test)\n\n\nThe formula for Root Mean Square Error is:\nRMSE=1n‚àëi=1n(yi‚àíyÃÇi)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\nThe smaller the RMSE metric the better the model\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\npoly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))\nprint(f'Polynomial regression\\nRMSE: {poly_reg_rmse:.2f}')\n\n\nPolynomial regression\nRMSE: 20.94\n\n\n\n\n\n\nCode\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Create linear regression instance\nlin_reg_model = LinearRegression()\n# Fit regression model\nlin_reg_model.fit(X_train, y_train)\n# Create predictions\nlin_reg_y_predicted = lin_reg_model.predict(X_test)\n# Calculate RMSE\nlin_reg_rmse = np.sqrt(mean_squared_error(y_test, lin_reg_y_predicted))\n# Print results\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:,.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nIn the train_test_split method we use X instead of poly_features, and it‚Äôs for a good reason.\nX contains our two original features (x_1 and x_2), so our linear regression model takes the form of:\ny=Œ≤0+Œ≤1x1+Œ≤2x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\n\nCode\nlin_reg_model.coef_\n\n\narray([43.73176255, -0.53140809])\n\n\n\n\nCode\nlin_reg_model.intercept_\n\n\nnp.float64(-117.07280081594811)\n\n\nOn the other hand, poly_features contains new features as well, created out of x_1 and x_2, so our polynomial regression model (based on a 2nd degree polynomial with two features) looks like this:\ny=Œ≤0+Œ≤1x1+Œ≤2x2+Œ≤3x12+Œ≤4x22+Œ≤5x1x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_2^2 + \\beta_5x_1x_2\nThis is because poly.fit_transform(X) added three new features to the original two (x1 (x1x_1) and x2 (x2x_2)): x12x_1^2, x22x_2^2 and x1x2x_1x_2\nx12x_1^2 and x22x_2^2 need no explanation, as we‚Äôve already covered how they are created in the ‚ÄúCoding a polynomial regression model with scikit-learn‚Äù section.\nWhat‚Äôs more interesting is x1x2x_1x_2 ‚Äì when two features are multiplied by each other, it‚Äôs called an interaction term. An interaction term accounts for the fact that one variable‚Äôs value may depend on another variable‚Äôs value (more on this here). poly.fit_transform() automatically created this interaction term for us, isn‚Äôt that cool? üôÇ\n\n\nCode\npoly_reg_model.coef_\n\n\narray([ 3.61945509, -1.0859955 ,  1.89905813,  0.0207338 ,  0.01300394])\n\n\n\n\nCode\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nThe RMSE for the polynomial regression model is 20.94, while the RMSE for the linear regression model is 62.3. The polynomial regression model performs almost 3 times better than the linear regression model!"
  },
  {
    "objectID": "portfolio/posts/python/polynomial_regression.html#conclusion",
    "href": "portfolio/posts/python/polynomial_regression.html#conclusion",
    "title": "Polynomial Regression",
    "section": "",
    "text": "In this notebook, we have been able to expose that a basic understanding of polynomial regression. And we have shown RMSE metric for comparing the performance among different ML models.\nWe used a 2nd degree polynomial for ourthis example. Naturally, we should always test and trial before deploying a model to find what degree of polynomial performs best."
  },
  {
    "objectID": "portfolio/posts/python/polynomial_regression.html#references",
    "href": "portfolio/posts/python/polynomial_regression.html#references",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Ujhelyi T. Polynomial regression I mainly based on.\nHow to become a Data Scientist"
  },
  {
    "objectID": "portfolio/posts/python/polynomial_regression.html#contact",
    "href": "portfolio/posts/python/polynomial_regression.html#contact",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Jesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html",
    "href": "portfolio/posts/python/mermaid-diagrams.html",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "",
    "text": "Figure¬†1: Mermaid for diagramming"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "href": "portfolio/posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Data Science Cycle Diagram using Mermaid",
    "text": "Data Science Cycle Diagram using Mermaid\n\n\nCode\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D\n\n\n\n\n\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#flow-process",
    "href": "portfolio/posts/python/mermaid-diagrams.html#flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Flow Process",
    "text": "Flow Process\n\n\nCode\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\n\n\nAdding colors\n\n\nCode\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#complex-flow-process",
    "href": "portfolio/posts/python/mermaid-diagrams.html#complex-flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Complex Flow Process",
    "text": "Complex Flow Process\ngraph\n    A[ItemA]--&gt; ItemB\n    A--&gt; ItemC\n    subgraph ItemC\n        D[ItemD]--&gt;E[ItemE]\n        E--&gt;D\n    end\n\n    X[ItemX]==&gt;Decision\n    click X \"https://mermaid.js.org/\"\n    Decision{Item Y?}==&gt;|Yes| Y[ItemY]\n    Decision==&gt;|No| Z[ItemZ]"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#pie-charts",
    "href": "portfolio/posts/python/mermaid-diagrams.html#pie-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Pie charts",
    "text": "Pie charts\npie title Pie chart 2\n    \"Category A\" : 2000\n    \"Category B\" : 500\n    \"Category C\" : 1000"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#gantt-chart",
    "href": "portfolio/posts/python/mermaid-diagrams.html#gantt-chart",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Gantt Chart",
    "text": "Gantt Chart\ngantt \n    title Gantt chart 1\n    dateFormat YYYY-MM-DD\n    axisFormat %b %d\n    section Section A\n        Task A1: a1, 2024-04-15, 7d\n        Task A2: after a1, 5d\n    section Section B\n        Task B1: 2024-04-22, 2024-05-02\n        Task B2: 2024-04-29, 5d"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#journey-charts",
    "href": "portfolio/posts/python/mermaid-diagrams.html#journey-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Journey Charts",
    "text": "Journey Charts\njourney\n    title Journey 1\n    section Section A\n        Activity 1A: 5: Person1\n        Activity 2A: 3: Person2\n        Activity 3A: 2: Person1, Person2\n    section Section B\n        Activity 1B: 4: Person2\n        Activity 1B: 5: Person1"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#state-diagram",
    "href": "portfolio/posts/python/mermaid-diagrams.html#state-diagram",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "State Diagram",
    "text": "State Diagram\nstateDiagram\n    [*] --&gt; StateA\n    StateA --&gt; [*]\n    StateA --&gt; StateB\n    StateB --&gt; StateA\n    StateB --&gt; StateD\n    StateD --&gt; [*]"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#items-diagrams",
    "href": "portfolio/posts/python/mermaid-diagrams.html#items-diagrams",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Items diagrams",
    "text": "Items diagrams\nerDiagram\n    Item1 ||--o{ Item2: text1\n    Item2 ||--|{ Item3: text2\n    Item1 }|..|{ Item4: text3\n    Item4 }o--o{ Item4: text3"
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#conclusions",
    "href": "portfolio/posts/python/mermaid-diagrams.html#conclusions",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Conclusions",
    "text": "Conclusions\nMermaid is a popular markdown-like syntax for generating diagrams and charts using Markdown. Think of it as a way to describe your diagrams in a human-readable format, which Mermaid then renders into a visual representation.\nMermaid Python acts as a bridge, allowing you to generate these Mermaid diagrams within your Python environment and even embed them in your Jupyter notebooks or web applications.\nMermaid Python also allows you to customize the appearance of your diagrams, add styling, and even integrate with other Python libraries. You can dynamically generate diagrams based on your data, making it a powerful tool for data visualization and exploration."
  },
  {
    "objectID": "portfolio/posts/python/mermaid-diagrams.html#contact",
    "href": "portfolio/posts/python/mermaid-diagrams.html#contact",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html",
    "href": "portfolio/posts/python/restaurants_sql.html",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "",
    "text": "Figure¬†1: Restaurants Report"
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#project-phases",
    "href": "portfolio/posts/python/restaurants_sql.html#project-phases",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Project phases",
    "text": "Project phases\n\nData Loading\n\nDuckDB: Load large datasets directly into DuckDB for efficient in-memory operations and query execution.\nPolars: Utilize Polars‚Äô fast data loading capabilities, especially for CSV and Parquet files, to quickly ingest data into its DataFrame structure.\n\nData Transformation\n\nPolars: Perform data cleaning, filtering, aggregation, and other transformations within the Polars DataFrame efficiently.\nDuckDB: Execute complex SQL queries directly on the in-memory data within DuckDB for advanced data manipulation.\n\nData Visualization\n\nPolars: Use Polars‚Äô built-in plotting capabilities for quick exploratory visualizations.\nPlotly: Leverage Plotly‚Äôs extensive library of interactive and customizable plots for in-depth analysis and presentation.\n\nPerformance Optimization\n\nMinimize Data Transfers: Avoid unnecessary data transfers between tools. For example, if possible, perform data transformations within DuckDB and then directly visualize results using Plotly.\nUtilize Parallel Processing: Leverage the parallel processing capabilities of both DuckDB and Polars to speed up data processing tasks.\nOptimize Queries: Write efficient SQL queries and use appropriate data types to maximize DuckDB‚Äôs performance.\nCaching: Cache intermediate results to avoid redundant computations."
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#environment-settings",
    "href": "portfolio/posts/python/restaurants_sql.html#environment-settings",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nCode\nimport numpy as np\nimport polars as pl\nimport random\nimport duckdb as db\nimport plotly.express as px"
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#create-dummy-dataset",
    "href": "portfolio/posts/python/restaurants_sql.html#create-dummy-dataset",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Create dummy dataset",
    "text": "Create dummy dataset\n\n\nCode\ndef generate_dummy_data(num_rows=10_000):\n  \"\"\"\n  Generates dummy data for restaurants with name, rating_count, cost, city, and cuisine.\n  Args:\n    num_rows: Number of rows to generate.\n  Returns:\n    A list of dictionaries, where each dictionary represents a restaurant with the specified fields.\n  \"\"\"\n\n  data = []\n  names = [\"The Cozy Nook\", \"Spice & Bloom\", \"The Golden Wok\", \"Midnight Diner\", \"Ocean Breeze\", \n           \"Cafe Delight\", \"The Burger Joint\", \"Pizza Palace\", \"Taste of Italy\", \"French Delights\",\n           \"The Curry House\", \"Sushi Corner\", \"Greek Gyros\", \"Taco Town\", \"The BBQ Shack\"]\n  cities = [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Rome\", \n            \"Berlin\", \"Sydney\", \"Madrid\", \"Amsterdam\", \"Lisbon\"]\n  cuisines = [\"Italian\", \"Mexican\", \"Indian\", \"Chinese\", \"Japanese\", \n              \"American\", \"French\", \"Thai\", \"Greek\", \"Spanish\"]\n\n  for _ in range(num_rows):\n    restaurant = {\n        'name': random.choice(names),\n        'rating_count': random.randint(100, 5000),\n        'cost': random.uniform(10.0, 100.0),\n        'city': random.choice(cities),\n        'cuisine': random.choice(cuisines),\n        'rating': random.randint(0, 5)\n    }\n    data.append(restaurant)\n\n  return data\n\n\n\n\nCode\n# Generate rows of dummy data\ndummy_restaurants = generate_dummy_data()"
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#create-dataframe",
    "href": "portfolio/posts/python/restaurants_sql.html#create-dataframe",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Create dataframe",
    "text": "Create dataframe\n\n\nCode\n# Create dataframe\nrestaurants = pl.DataFrame(dummy_restaurants)\n\n\n\n\nCode\n# Show dataframe\n(\n    restaurants\n        .to_pandas()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n3,408\n$37.75\nLisbon\nFrench\n2\n\n\nGreek Gyros\n1,484\n$93.44\nParis\nItalian\n0\n\n\nOcean Breeze\n2,792\n$32.26\nBerlin\nMexican\n5\n\n\nGreek Gyros\n2,038\n$20.78\nLondon\nSpanish\n5\n\n\nThe Curry House\n2,321\n$94.79\nLisbon\nFrench\n1\n\n\n\n\n\n\nTable¬†1: Restaurants dataset"
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#connect-to-database",
    "href": "portfolio/posts/python/restaurants_sql.html#connect-to-database",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Connect to database",
    "text": "Connect to database\n\n\nCode\n# connect to database\nconn = db.connect('my_database.db')\n#conn = db.connect('restaurants.db')\n\n\n\n\nCode\n# retrieve data from table\nres = conn.sql('select * from restaurants limit 5')\n\n\n\n\nCode\n# Show table\n(\n    res.df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nGreek Gyros\n770\n$68.39\nNew York\nFrench\n1\n\n\nTaste of Italy\n4,420\n$88.23\nAmsterdam\nChinese\n0\n\n\nMidnight Diner\n2,155\n$12.97\nLisbon\nMexican\n1\n\n\nTaste of Italy\n3,375\n$52.79\nSydney\nChinese\n1\n\n\n\n\n\n\nTable¬†2: Restaurants table from database"
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#queries",
    "href": "portfolio/posts/python/restaurants_sql.html#queries",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Queries",
    "text": "Queries\n\nWhich restaurant of London is visited by the least number of people?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where city = 'London' and rating_count = (select min(rating_count)\n                                            from restaurants \n                                            where city = 'London')\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nGreek Gyros\n101\n$13.82\nLondon\nIndian\n2\n\n\n\n\n\n\nTable¬†3: Query 1 result table\n\n\n\n\n\n\nWhich restaurant has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where cost*rating_count = (select max(cost*rating_count)\n                                from restaurants)\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nOcean Breeze\n4,969\n$99.92\nMadrid\nChinese\n4\n\n\n\n\n\n\nTable¬†4: Query 2 result table\n\n\n\n\n\n\nHow many restaurants are having a rating more than the average rating?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where rating &gt; (select avg(rating)\n                    from restaurants)\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nThe Burger Joint\n3,631\n$25.36\nLondon\nAmerican\n4\n\n\nThe Curry House\n3,033\n$56.05\nAmsterdam\nItalian\n3\n\n\nThe Burger Joint\n4,656\n$19.38\nAmsterdam\nAmerican\n3\n\n\nPizza Palace\n1,862\n$71.95\nParis\nJapanese\n5\n\n\n\n\n\n\nTable¬†5: Query 3 result table\n\n\n\n\n\n\nWhich restaurant of New York has generated the most revenue?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where city = 'New York' and cost*rating_count = (select max(cost*rating_count)\n                                                    from restaurants \n                                                    where city = 'New York')\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Curry House\n4,955\n$97.92\nNew York\nJapanese\n5\n\n\n\n\n\n\nTable¬†6: Query 4 result table\n\n\n\n\n\n\nWhich restaurant chain has the maximum number of restaurants?\n\n\nCode\n(\n    conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nno_of_chains\n\n\n\n\nThe Burger Joint\n721\n\n\nPizza Palace\n703\n\n\nGreek Gyros\n696\n\n\nCafe Delight\n692\n\n\nFrench Delights\n681\n\n\nThe BBQ Shack\n671\n\n\nThe Golden Wok\n667\n\n\nOcean Breeze\n665\n\n\nSpice & Bloom\n665\n\n\nMidnight Diner\n657\n\n\n\n\n\n\nTable¬†7: Query 5 result table\n\n\n\n\n\n\nCode\nres_chains = conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n''').pl()\n\nfig = px.bar(res_chains, \n             x='no_of_chains',\n             y='name',\n             orientation='h',\n             hover_data=['no_of_chains', 'name',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Chains by Restaurant',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='outside',\n                  marker_color='rgb(55, 83, 109)',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,)\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Chains', yaxis_title='Restaurant')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†2: Restaurant Chains\n\n\n\n\n\n\nWhich restaurant chain has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select name, sum(rating_count * cost) as revenue\n    from restaurants\n    group by name\n    order by revenue DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'revenue': '{:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrevenue\n\n\n\n\nThe Burger Joint\n108,820,424.64\n\n\nPizza Palace\n100,382,853.92\n\n\nCafe Delight\n98,037,063.93\n\n\nGreek Gyros\n96,403,445.25\n\n\nThe BBQ Shack\n96,286,414.02\n\n\nOcean Breeze\n95,664,612.77\n\n\nThe Golden Wok\n94,860,125.75\n\n\nSpice & Bloom\n91,824,854.56\n\n\nFrench Delights\n91,236,701.38\n\n\nMidnight Diner\n91,170,162.30\n\n\n\n\n\n\nTable¬†8: Query 6 result table\n\n\n\n\n\n\nCode\nrev = conn.sql('''\n            select name, sum(rating_count * cost) as revenue\n            from restaurants\n            group by name\n            order by revenue DESC\n            limit 10\n            ''').pl()\n\nfig = px.bar(rev, \n             x='revenue',\n             y='name',\n             orientation='h',\n             hover_data=['revenue', 'name',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Revenue by Restaurant',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='inside',\n                  marker_color='#004700',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,\n                  hovertemplate='&lt;br&gt;'.join([\n                      'Restaurant: %{y}',\n                      'Revenue: %{x}',\n                  ]),\n                 )\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Revenue', yaxis_title='Restaurant',\n                 xaxis_tickprefix='$', xaxis_tickformat=',.2f')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†3: Revenue by Restaurant\n\n\n\n\n\n\nWhich city has the maximum number of restaurants?\n\n\nCode\n(\n    conn.sql('''\n    select city, count(*) as no_of_restaurants\n    from restaurants\n    group by city\n    order by no_of_restaurants DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'no_of_restaurants': '{:,.0f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nno_of_restaurants\n\n\n\n\nTokyo\n1,071\n\n\nAmsterdam\n1,038\n\n\nLisbon\n1,005\n\n\nMadrid\n1,003\n\n\nLondon\n993\n\n\nParis\n992\n\n\nRome\n979\n\n\nBerlin\n977\n\n\nNew York\n971\n\n\nSydney\n971\n\n\n\n\n\n\nTable¬†9: Query 7 result table\n\n\n\n\n\n\nWhich city has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select city, sum(rating_count * cost) as revenue\n    from restaurants\n    group by city\n    order by revenue DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'revenue': '${:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nrevenue\n\n\n\n\nAmsterdam\n$148,839,878.62\n\n\nTokyo\n$148,035,421.32\n\n\nMadrid\n$141,487,618.97\n\n\nParis\n$141,219,374.56\n\n\nLondon\n$140,876,613.54\n\n\nRome\n$139,622,129.63\n\n\nNew York\n$138,621,609.28\n\n\nLisbon\n$136,814,247.27\n\n\nBerlin\n$136,434,163.25\n\n\nSydney\n$131,656,513.97\n\n\n\n\n\n\nTable¬†10: Query 8 result table\n\n\n\n\n\n\nCode\ncities = conn.sql('''\n                select city, sum(rating_count * cost) as revenue\n                from restaurants\n                group by city\n                order by revenue DESC\n                limit 10\n                ''').pl()\n\nfig = px.bar(cities, \n             x='revenue',\n             y='city',\n             orientation='h',\n             hover_data=['revenue', 'city',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Revenue by City',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='inside',\n                  marker_color='#880808',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,\n                  hovertemplate='&lt;br&gt;'.join([\n                      'City: %{y}',\n                      'Revenue: %{x}',\n                  ]),\n                 )\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Revenue', yaxis_title='City',\n                 xaxis_tickprefix='$', xaxis_tickformat=',.2f')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure¬†4: Revenue by City\n\n\n\n\n\n\nList 10 least expensive cuisines\n\n\nCode\n(\n    conn.sql('''\n    select cuisine, avg(cost) as avg_cost\n    from restaurants\n    group by cuisine\n    order by avg_cost asc\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncuisine\navg_cost\n\n\n\n\nIndian\n$53.92\n\n\nAmerican\n$53.96\n\n\nItalian\n$54.61\n\n\nThai\n$54.64\n\n\nFrench\n$54.65\n\n\nMexican\n$55.47\n\n\nSpanish\n$55.54\n\n\nJapanese\n$55.55\n\n\nGreek\n$55.77\n\n\nChinese\n$56.26\n\n\n\n\n\n\nTable¬†11: Query 9 result table\n\n\n\n\n\n\nList 10 most expensive cuisines\n\n\nCode\n(\n    conn.sql('''\n    select cuisine, avg(cost) as avg_cost\n    from restaurants\n    group by cuisine\n    order by avg_cost desc\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncuisine\navg_cost\n\n\n\n\nChinese\n$56.26\n\n\nGreek\n$55.77\n\n\nJapanese\n$55.55\n\n\nSpanish\n$55.54\n\n\nMexican\n$55.47\n\n\nFrench\n$54.65\n\n\nThai\n$54.64\n\n\nItalian\n$54.61\n\n\nAmerican\n$53.96\n\n\nIndian\n$53.92\n\n\n\n\n\n\nTable¬†12: Query 10 result table\n\n\n\n\n\n\nWhat city has Mexican food as the most popular cuisine?\n\n\nCode\n(\n    conn.sql('''\n    select city, avg(cost) avg_cost, count(*) as restaurants\n    from restaurants\n    where cuisine = 'Mexican'\n    group by city\n    order by restaurants desc\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\navg_cost\nrestaurants\n\n\n\n\nLondon\n$52.30\n112\n\n\nTokyo\n$48.83\n111\n\n\nRome\n$57.11\n108\n\n\nAmsterdam\n$60.67\n108\n\n\nLisbon\n$56.35\n101\n\n\n\n\n\n\nTable¬†13: Query 11 result table\n\n\n\n\n\n\nCode\n# close connection\nconn.close()"
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#conclusions",
    "href": "portfolio/posts/python/restaurants_sql.html#conclusions",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Conclusions",
    "text": "Conclusions\nThis project highlights the value of combining the strengths of SQL and Python for data-intensive tasks, providing a robust and efficient solution for a wide range of data analysis challenges.\nBy carefully considering the interaction between DuckDB, Polars, and Plotly, you can create a powerful and efficient data analysis and visualization pipeline.\nThis approach can lead organizations to unlock valuable insights from their data more quickly and effectively, driving data-driven decision-making."
  },
  {
    "objectID": "portfolio/posts/python/restaurants_sql.html#contact",
    "href": "portfolio/posts/python/restaurants_sql.html#contact",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html",
    "href": "portfolio/posts/python/spatial_analysis.html",
    "title": "Geospatial Analysis in Python",
    "section": "",
    "text": "Figure¬†1: Image by Henrikki Tenkanen, Vuokko Heikinheimo, David Whipp"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#environment-setting",
    "href": "portfolio/posts/python/spatial_analysis.html#environment-setting",
    "title": "Geospatial Analysis in Python",
    "section": "Environment setting",
    "text": "Environment setting\n\n\nCode\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport folium\nfrom great_tables import GT, md\nfrom warnings import filterwarnings\nfilterwarnings('ignore')"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#data-collection",
    "href": "portfolio/posts/python/spatial_analysis.html#data-collection",
    "title": "Geospatial Analysis in Python",
    "section": "Data collection",
    "text": "Data collection\n\n\nCode\nconn = db.connect('datasets/geospatial.db')\n\n\n\n\nCode\nconn.sql('show tables')\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  name   ‚îÇ\n‚îÇ varchar ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ zomato  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\nCode\ndata = conn.sql('select * from zomato').pl()\n\n\n\n\nCode\ndata.columns\n\n\n['url',\n 'address',\n 'name',\n 'online_order',\n 'book_table',\n 'rate',\n 'votes',\n 'phone',\n 'location',\n 'rest_type',\n 'dish_liked',\n 'cuisines',\n 'approx_cost(for two people)',\n 'reviews_list',\n 'menu_item',\n 'listed_in(type)',\n 'listed_in(city)']\n\n\n\n\nCode\ndata.shape\n\n\n(51717, 17)"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#data-preprocessing",
    "href": "portfolio/posts/python/spatial_analysis.html#data-preprocessing",
    "title": "Geospatial Analysis in Python",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\n\nCode\ndata.is_duplicated().sum()\n\n\n0\n\n\n\n\nCode\ndata.select(pl.all().is_null().sum()).to_dicts()\n\n\n[{'url': 0,\n  'address': 0,\n  'name': 0,\n  'online_order': 0,\n  'book_table': 0,\n  'rate': 7775,\n  'votes': 0,\n  'phone': 1208,\n  'location': 21,\n  'rest_type': 227,\n  'dish_liked': 28078,\n  'cuisines': 45,\n  'approx_cost(for two people)': 346,\n  'reviews_list': 0,\n  'menu_item': 0,\n  'listed_in(type)': 0,\n  'listed_in(city)': 0}]\n\n\n\n\nCode\n# As we have few missing values in location feature ,then we can drop the null\ndata = data.drop_nulls(subset=pl.col('location'))\n\n\n\n\nCode\n(\n    GT(data.select('address','name','rate','votes','location','rest_type','dish_liked','cuisines').head(3))\n    .tab_header(\n        title=md('Zomato Restaurants')\n    )\n    .cols_width(\n        cases={'rate':'50px',\n              }\n               )\n    .tab_source_note(source_note=md('&lt;br&gt; *Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants\n\n\naddress\nname\nrate\nvotes\nlocation\nrest_type\ndish_liked\ncuisines\n\n\n\n\n942, 21st Main Road, 2nd Stage, Banashankari, Bangalore\nJalsa\n4.1/5\n775\nBanashankari\nCasual Dining\nPasta, Lunch Buffet, Masala Papad, Paneer Lajawab, Tomato Shorba, Dum Biryani, Sweet Corn Soup\nNorth Indian, Mughlai, Chinese\n\n\n2nd Floor, 80 Feet Road, Near Big Bazaar, 6th Block, Kathriguppe, 3rd Stage, Banashankari, Bangalore\nSpice Elephant\n4.1/5\n787\nBanashankari\nCasual Dining\nMomos, Lunch Buffet, Chocolate Nirvana, Thai Green Curry, Paneer Tikka, Dum Biryani, Chicken Biryani\nChinese, North Indian, Thai\n\n\n1112, Next to KIMS Medical College, 17th Cross, 2nd Stage, Banashankari, Bangalore\nSan Churro Cafe\n3.8/5\n918\nBanashankari\nCafe, Casual Dining\nChurros, Cannelloni, Minestrone Soup, Hot Chocolate, Pink Sauce Pasta, Salsa, Veg Supreme Pizza\nCafe, Mexican, Italian\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†1: Zomato Restaurants from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\n\n\nCode\n# create a copy\ndf = data.clone()\n\n\nLets make every place more readible so that u will get more more accurate geographical co-ordinates..\n\n\nCode\ndf = df.with_columns(\n    location=(pl.col('location') + ', Bangalore, Karnataka, India')\n)\n\n\n\n\nCode\ndf.select('location').sample(5).to_dicts()\n\n\n[{'location': 'HSR, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'}]\n\n\n\n\nCode\ndf.schema\n\n\nSchema([('url', String),\n        ('address', String),\n        ('name', String),\n        ('online_order', Boolean),\n        ('book_table', Boolean),\n        ('rate', String),\n        ('votes', Int64),\n        ('phone', String),\n        ('location', String),\n        ('rest_type', String),\n        ('dish_liked', String),\n        ('cuisines', String),\n        ('approx_cost(for two people)', String),\n        ('reviews_list', String),\n        ('menu_item', String),\n        ('listed_in(type)', String),\n        ('listed_in(city)', String)])"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#extract-coordinates-from-data",
    "href": "portfolio/posts/python/spatial_analysis.html#extract-coordinates-from-data",
    "title": "Geospatial Analysis in Python",
    "section": "Extract coordinates from data",
    "text": "Extract coordinates from data\nfirst we will learn how to extract Latitudes & longitudes using ‚Äòlocation‚Äô feature\n\n\nCode\nrest_loc = pl.DataFrame()\n\n\n\n\nCode\nrest_loc = pl.DataFrame({'name': df.select('location').unique()})\n\n\n\n\nCode\nrest_loc.sample(5).to_dicts()\n\n\n[{'name': 'Jakkur, Bangalore, Karnataka, India'},\n {'name': 'Kalyan Nagar, Bangalore, Karnataka, India'},\n {'name': 'RT Nagar, Bangalore, Karnataka, India'},\n {'name': 'Koramangala 7th Block, Bangalore, Karnataka, India'},\n {'name': 'Kaggadasapura, Bangalore, Karnataka, India'}]\n\n\n\n\nCode\n# Nominatim is a tool to search OpenStreetMap data by address or location\nfrom geopy.geocoders import Nominatim\n\n\n\n\nCode\ngeolocator = Nominatim(user_agent='app', timeout=None)\n\n\n\n\nCode\nlat = [] # define lat list to store all the latitudes\nlon = [] # define lon list to store all the longitudes\n\nfor name in pl.Series(rest_loc.select('name')):\n    location = geolocator.geocode(name)\n    \n    if location is None:\n        lat.append(np.nan)\n        lon.append(np.nan)\n        \n    else:\n        lat.append(location.latitude)\n        lon.append(location.longitude)\n\n\n\n\nCode\nlat[:10]\n\n\n[13.0621474,\n 12.9846713,\n 12.981015523680384,\n 12.985098650000001,\n 12.9096941,\n nan,\n 12.9067683,\n 12.938455602031697,\n 12.9176571,\n 12.9489339]\n\n\n\n\nCode\nrest_loc = rest_loc.with_columns(\n    lat=pl.Series(lat), # For python lists, construct a Series\n    lon=pl.Series(lon),\n)\n\n\n\n\nCode\n(\n    GT(rest_loc.head(5), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Coordinates')\n    )\n    .fmt_number(columns=['lat','lon'], decimals=4, use_seps=False)\n    .cols_width(\n        cases={'name':'200%',\n               'lat':'90%',\n               'lon':'90%',\n              }\n               )\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Coordinates\n\n\nname\nlat\nlon\n\n\n\n\nSahakara Nagar, Bangalore, Karnataka, India\n13.0621\n77.5801\n\n\nKaggadasapura, Bangalore, Karnataka, India\n12.9847\n77.6791\n\n\nInfantry Road, Bangalore, Karnataka, India\n12.9810\n77.6021\n\n\nCV Raman Nagar, Bangalore, Karnataka, India\n12.9851\n77.6631\n\n\nJP Nagar, Bangalore, Karnataka, India\n12.9097\n77.5866\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†2: Zomato restaurants coordinates from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nWe have found out latitude and longitude of each location listed in the dataset using geopy This is used to plot maps.\n\n\nCode\npl.Series(rest_loc.select('lat')).is_null().sum()\n\n\n0\n\n\n\n\nCode\npl.Series(rest_loc.select('lat')).is_nan().sum()\n\n\n2\n\n\n\n\nCode\nrest_loc.filter(pl.col('lat').is_nan())\n\n\n\nshape: (2, 3)\n\n\n\nname\nlat\nlon\n\n\nstr\nf64\nf64\n\n\n\n\n\"Sadashiv Nagar, Bangalore, Kar‚Ä¶\nNaN\nNaN\n\n\n\"Rammurthy Nagar, Bangalore, Ka‚Ä¶\nNaN\nNaN\n\n\n\n\n\n\n\n\nCode\nrest_loc = rest_loc.drop_nans()"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#where-are-most-number-of-restaurants-located-in-bengalore",
    "href": "portfolio/posts/python/spatial_analysis.html#where-are-most-number-of-restaurants-located-in-bengalore",
    "title": "Geospatial Analysis in Python",
    "section": "Where are most number of restaurants located in Bengalore?",
    "text": "Where are most number of restaurants located in Bengalore?\n\n\nCode\nrest_locations = pl.Series(df.select('location')).value_counts(sort=True, name='total')\n\n\n\n\nCode\nrest_locations = rest_locations.rename({'location':'name', 'total':'count'})\n\n\n\n\nCode\n(\n    GT(rest_locations.head(), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Count')\n    )\n    .cols_width(cases={'name': '200%',})\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Count\n\n\nname\ncount\n\n\n\n\nBTM, Bangalore, Karnataka, India\n5124\n\n\nHSR, Bangalore, Karnataka, India\n2523\n\n\nKoramangala 5th Block, Bangalore, Karnataka, India\n2504\n\n\nJP Nagar, Bangalore, Karnataka, India\n2235\n\n\nWhitefield, Bangalore, Karnataka, India\n2144\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†3: Zomato restaurants count from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nNow we can say that these are locations where most of restaurants are located.\nLets create Heatmap of this results so that it becomes more user-friendly.\nNow, in order to perform spatial analysis, we need latitudes & longitudes of every location, so lets merge both dataframes in order to get geographical co-ordinates.\n\n\nCode\nbeng_rest_locations = rest_locations.join(rest_loc, on='name')\n\n\n\n\nCode\n(\n    GT(beng_rest_locations.head(), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Count & coordinates')\n    )\n    .cols_width(cases={'name': '200%',})\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Count & coordinates\n\n\nname\ncount\nlat\nlon\n\n\n\n\nBTM, Bangalore, Karnataka, India\n5124\n12.9163603\n77.604733\n\n\nHSR, Bangalore, Karnataka, India\n2523\n12.90056335\n77.64947470503677\n\n\nKoramangala 5th Block, Bangalore, Karnataka, India\n2504\n12.9348429\n77.6189768\n\n\nJP Nagar, Bangalore, Karnataka, India\n2235\n12.9096941\n77.5866067\n\n\nWhitefield, Bangalore, Karnataka, India\n2144\n12.9696365\n77.7497448\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable¬†4: Zomato restaurants count and coordinates from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nnow in order to show-case it via Map(Heatmap) ,first we need to create BaseMap so that I can map our Heatmap on top of BaseMap !\n\n\nCode\ndef Generate_basemap():\n    basemap = folium.Map(location=[12.97 , 77.59], zoom_start=11)\n    return basemap\n\n\n\n\nCode\n# Geographic heat maps are used to identify where something occurs, and demonstrate areas of high and low density...\nfrom folium.plugins import HeatMap\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nbeng_rest_locations = beng_rest_locations.to_pandas()\n\n\n\n\nCode\nHeatMap(beng_rest_locations[['lat', 'lon' , 'count']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x3058e0da0&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†2: Zomato Restaurants Heatmap\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out.\n\n\nMajority of the Restaurants are avaiable in the city centre area."
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#performing-marker-cluster-analysis",
    "href": "portfolio/posts/python/spatial_analysis.html#performing-marker-cluster-analysis",
    "title": "Geospatial Analysis in Python",
    "section": "Performing Marker Cluster Analysis",
    "text": "Performing Marker Cluster Analysis\n\n\nCode\nfrom folium.plugins import FastMarkerCluster\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nFastMarkerCluster(beng_rest_locations[['lat', 'lon' , 'count']]).add_to(basemap)\n\n\n&lt;folium.plugins.fast_marker_cluster.FastMarkerCluster at 0x30a2cd280&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†3: Zomato Marker Cluster Map\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out."
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#mapping-all-the-markers-of-places-of-bangalore",
    "href": "portfolio/posts/python/spatial_analysis.html#mapping-all-the-markers-of-places-of-bangalore",
    "title": "Geospatial Analysis in Python",
    "section": "Mapping all the markers of places of Bangalore",
    "text": "Mapping all the markers of places of Bangalore\nPlotting Markers on the Map :\nFolium gives a folium.Marker() class for plotting markers on a map\nJust pass the latitude and longitude of the location, mention the popup and tooltip and add it to the map.\nPlotting markers is a two-step process.\n\nyou need to create a base map on which your markers will be placed\nand then add your markers to it:\n\n\n\nCode\nm = Generate_basemap()\n\n\n\n\nCode\n# Add points to the map\nfor index, row in beng_rest_locations.iterrows():\n    folium.Marker(location=[row['lat'], row['lon']], popup=row['count']).add_to(m)\n\n\n\n\nCode\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†4: Zomato Restaurants Marker Map\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out.\n\n\nRate field cleaning\nIn order to Analyse where are the restaurants situated with high average rate, first we need to clean ‚Äòrate‚Äô feature\n\n\nCode\n(\n    df.filter(\n        pl.col('rate').str.contains('^([^0-9]*)$')\n    )\n    .select('rate')\n    .unique()\n    .to_dicts()\n)\n\n\n[{'rate': '-'}, {'rate': 'NEW'}]\n\n\n\n\nCode\npl.Series(df.select('rate')).is_null().sum()\n\n\n7754\n\n\n\n\nCode\n# approximately 15% of your rating belongs to missing values\npl.Series(df.select('rate')).is_null().sum()/pl.Series(df.select('rate')).len()*100\n\n\n14.999226245744351\n\n\n\n\nCode\ndf = (\n    df.drop_nulls(subset='rate')\n        .with_columns(\n            pl.col('rate').replace(['NEW', '-',], ['0', '0'])\n        )\n        .with_columns(\n            rating=pl.col('rate').str.replace('/5', '')\n        )\n        .with_columns(\n            pl.col('rating').str.strip_chars()\n        )\n        .cast({'rating': pl.Float32})\n)\n\n\n\n\nCode\ndf.select('rating').unique().to_dicts()\n\n\n[{'rating': 2.4000000953674316},\n {'rating': 2.299999952316284},\n {'rating': 0.0},\n {'rating': 3.5},\n {'rating': 4.099999904632568},\n {'rating': 4.400000095367432},\n {'rating': 4.699999809265137},\n {'rating': 4.900000095367432},\n {'rating': 2.700000047683716},\n {'rating': 3.9000000953674316},\n {'rating': 3.799999952316284},\n {'rating': 3.4000000953674316},\n {'rating': 3.0},\n {'rating': 2.5999999046325684},\n {'rating': 3.299999952316284},\n {'rating': 4.199999809265137},\n {'rating': 2.200000047683716},\n {'rating': 4.0},\n {'rating': 4.5},\n {'rating': 2.5},\n {'rating': 3.5999999046325684},\n {'rating': 3.700000047683716},\n {'rating': 2.0999999046325684},\n {'rating': 4.800000190734863},\n {'rating': 3.200000047683716},\n {'rating': 2.799999952316284},\n {'rating': 4.300000190734863},\n {'rating': 2.9000000953674316},\n {'rating': 2.0},\n {'rating': 4.599999904632568},\n {'rating': 3.0999999046325684},\n {'rating': 1.7999999523162842}]"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#most-highest-rated-restaurants",
    "href": "portfolio/posts/python/spatial_analysis.html#most-highest-rated-restaurants",
    "title": "Geospatial Analysis in Python",
    "section": "Most highest rated restaurants",
    "text": "Most highest rated restaurants\n\n\nCode\ndf.select('name','rate','votes','location','dish_liked','rating').sort('rating', descending=True).head()\n\n\n\nshape: (5, 6)\n\n\n\nname\nrate\nvotes\nlocation\ndish_liked\nrating\n\n\nstr\nstr\ni64\nstr\nstr\nf32\n\n\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn‚Ä¶\n\"Cocktails, Dahi Kebab, Rajma C‚Ä¶\n4.9\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn‚Ä¶\n\"Cocktails, Dahi Kebab, Rajma C‚Ä¶\n4.9\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn‚Ä¶\n\"Cocktails, Dahi Kebab, Rajma C‚Ä¶\n4.9\n\n\n\"Belgian Waffle Factory\"\n\"4.9/5\"\n1746\n\"Brigade Road, Bangalore, Karna‚Ä¶\n\"Coffee, Berryblast, Nachos, Ch‚Ä¶\n4.9\n\n\n\"Belgian Waffle Factory\"\n\"4.9/5\"\n1746\n\"Brigade Road, Bangalore, Karna‚Ä¶\n\"Coffee, Berryblast, Nachos, Ch‚Ä¶\n4.9\n\n\n\n\n\n\n\n\nCode\ngrp_df = (\n    df.group_by('location').agg(pl.col('rating').mean(), pl.col('name').count())\n        .rename({'location':'name', 'rating':'avg_rating', 'name':'count'})\n)\n\n\n\n\nCode\ngrp_df\n\n\n\nshape: (92, 3)\n\n\n\nname\navg_rating\ncount\n\n\nstr\nf32\nu32\n\n\n\n\n\"Brookefield, Bangalore, Karnat‚Ä¶\n3.374697\n581\n\n\n\"Thippasandra, Bangalore, Karna‚Ä¶\n3.095396\n152\n\n\n\"Electronic City, Bangalore, Ka‚Ä¶\n3.04191\n964\n\n\n\"Koramangala 1st Block, Bangalo‚Ä¶\n3.263946\n965\n\n\n\"Koramangala 3rd Block, Bangalo‚Ä¶\n3.978755\n193\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"RT Nagar, Bangalore, Karnataka‚Ä¶\n3.278125\n64\n\n\n\"Jalahalli, Bangalore, Karnatak‚Ä¶\n3.486956\n23\n\n\n\"Commercial Street, Bangalore, ‚Ä¶\n3.109709\n309\n\n\n\"Banaswadi, Bangalore, Karnatak‚Ä¶\n3.362927\n499\n\n\n\"Koramangala 5th Block, Bangalo‚Ä¶\n3.901511\n2381\n\n\n\n\n\n\nlets consider only those restaurants who have send atleast 400 orders\n\n\nCode\ntemp_df = grp_df.filter(pl.col('count')&gt;400)\n\n\n\n\nCode\ntemp_df.shape\n\n\n(35, 3)\n\n\n\n\nCode\ntemp_df\n\n\n\nshape: (35, 3)\n\n\n\nname\navg_rating\ncount\n\n\nstr\nf32\nu32\n\n\n\n\n\"Brookefield, Bangalore, Karnat‚Ä¶\n3.374697\n581\n\n\n\"Electronic City, Bangalore, Ka‚Ä¶\n3.04191\n964\n\n\n\"Koramangala 1st Block, Bangalo‚Ä¶\n3.263946\n965\n\n\n\"Bannerghatta Road, Bangalore, ‚Ä¶\n3.271675\n1324\n\n\n\"HSR, Bangalore, Karnataka, Ind‚Ä¶\n3.484063\n2128\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Richmond Road, Bangalore, Karn‚Ä¶\n3.688013\n634\n\n\n\"Koramangala 7th Block, Bangalo‚Ä¶\n3.747846\n1089\n\n\n\"Frazer Town, Bangalore, Karnat‚Ä¶\n3.56488\n578\n\n\n\"Banaswadi, Bangalore, Karnatak‚Ä¶\n3.362927\n499\n\n\n\"Koramangala 5th Block, Bangalo‚Ä¶\n3.901511\n2381\n\n\n\n\n\n\n\n\nCode\nrest_loc\n\n\n\nshape: (91, 3)\n\n\n\nname\nlat\nlon\n\n\nstr\nf64\nf64\n\n\n\n\n\"Sahakara Nagar, Bangalore, Kar‚Ä¶\n13.062147\n77.580061\n\n\n\"Kaggadasapura, Bangalore, Karn‚Ä¶\n12.984671\n77.679091\n\n\n\"Infantry Road, Bangalore, Karn‚Ä¶\n12.981016\n77.602133\n\n\n\"CV Raman Nagar, Bangalore, Kar‚Ä¶\n12.985099\n77.663117\n\n\n\"JP Nagar, Bangalore, Karnataka‚Ä¶\n12.909694\n77.586607\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Seshadripuram, Bangalore, Karn‚Ä¶\n12.993188\n77.575342\n\n\n\"Jakkur, Bangalore, Karnataka, ‚Ä¶\n13.078474\n77.606894\n\n\n\"Bommanahalli, Bangalore, Karna‚Ä¶\n12.908945\n77.623904\n\n\n\"Kammanahalli, Bangalore, Karna‚Ä¶\n13.009346\n77.637709\n\n\n\"Nagawara, Bangalore, Karnataka‚Ä¶\n13.042279\n77.624858\n\n\n\n\n\n\nlets merge both the dataframe so that we can get coordinates as well\n\n\nCode\nratings_locations = temp_df.join(rest_loc, on='name')\n\n\n\n\nCode\nratings_locations\n\n\n\nshape: (35, 5)\n\n\n\nname\navg_rating\ncount\nlat\nlon\n\n\nstr\nf32\nu32\nf64\nf64\n\n\n\n\n\"JP Nagar, Bangalore, Karnataka‚Ä¶\n3.412929\n1849\n12.909694\n77.586607\n\n\n\"Koramangala 4th Block, Bangalo‚Ä¶\n3.814351\n864\n12.932778\n77.629405\n\n\n\"Whitefield, Bangalore, Karnata‚Ä¶\n3.384171\n1693\n12.969637\n77.749745\n\n\n\"Bannerghatta Road, Bangalore, ‚Ä¶\n3.271675\n1324\n12.951856\n77.604011\n\n\n\"Jayanagar, Bangalore, Karnatak‚Ä¶\n3.61525\n1718\n12.939904\n77.582638\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"Ulsoor, Bangalore, Karnataka, ‚Ä¶\n3.541396\n901\n12.977879\n77.62467\n\n\n\"Frazer Town, Bangalore, Karnat‚Ä¶\n3.56488\n578\n12.998683\n77.615525\n\n\n\"Indiranagar, Bangalore, Karnat‚Ä¶\n3.652168\n1936\n12.996298\n77.545278\n\n\n\"Koramangala 6th Block, Bangalo‚Ä¶\n3.662465\n1111\n12.939025\n77.623848\n\n\n\"Kammanahalli, Bangalore, Karna‚Ä¶\n3.499809\n525\n13.009346\n77.637709\n\n\n\n\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nratings_locations = ratings_locations.to_pandas()\n\n\n\n\nCode\nHeatMap(ratings_locations[['lat', 'lon' , 'avg_rating']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x30a39bcb0&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure¬†5: Highest-rated Zomato Restaurants Heatmap\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out."
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#conclusions",
    "href": "portfolio/posts/python/spatial_analysis.html#conclusions",
    "title": "Geospatial Analysis in Python",
    "section": "Conclusions",
    "text": "Conclusions\nPython, with its powerful libraries and ease of use, has become an indispensable tool for geospatial analysis. By leveraging the capabilities of libraries like GeoPandas, Shapely, and folium, data scientists can effectively explore and analyze geospatial data, gain valuable insights, and make informed decisions.\nIn this article, we have shown a brief overview of geospatial analysis in Python."
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#references",
    "href": "portfolio/posts/python/spatial_analysis.html#references",
    "title": "Geospatial Analysis in Python",
    "section": "References",
    "text": "References\n\nSingh, S (2024) Spatial Analysis & Geospatial Data Science in Python\nTenkanen, H et al (2022) Introduction to Python for Geographic Data Analysis"
  },
  {
    "objectID": "portfolio/posts/python/spatial_analysis.html#contact",
    "href": "portfolio/posts/python/spatial_analysis.html#contact",
    "title": "Geospatial Analysis in Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/slides.html",
    "href": "portfolio/posts/python/slides.html",
    "title": "Creating Slides with Python and Quarto",
    "section": "",
    "text": "Quarto offers a powerful and versatile tool for data scientists to create presentations with code that are informative, transparent, reproducible, and visually appealing."
  },
  {
    "objectID": "portfolio/posts/python/slides.html#contact",
    "href": "portfolio/posts/python/slides.html#contact",
    "title": "Creating Slides with Python and Quarto",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/plotting-with-holoviews.html#overview",
    "href": "portfolio/posts/python/plotting-with-holoviews.html#overview",
    "title": "Creating Charts with HoloViews",
    "section": "Overview",
    "text": "Overview\nFor data analytics, the magic lies in transforming raw numbers into visuals that reveal hidden patterns and trends. But the process of creating these visualizations can often be cumbersome, requiring extensive coding and customization. Here‚Äôs where HoloViews steps in, offering a refreshing approach to data visualization in Python."
  },
  {
    "objectID": "portfolio/posts/python/plotting-with-holoviews.html#what-is-holoviews",
    "href": "portfolio/posts/python/plotting-with-holoviews.html#what-is-holoviews",
    "title": "Creating Charts with HoloViews",
    "section": "What is HoloViews?",
    "text": "What is HoloViews?\nHoloViews is an open-source Python library designed to streamline data analysis and visualization. It departs from the traditional method of meticulously crafting plots line by line. Instead, HoloViews focuses on a declarative approach, where you describe your data and desired visualization, and it takes care of the intricate details. This allows you to express your ideas with concise code, freeing you to delve deeper into your story-telling."
  },
  {
    "objectID": "portfolio/posts/python/plotting-with-holoviews.html#why-choose-holoviews",
    "href": "portfolio/posts/python/plotting-with-holoviews.html#why-choose-holoviews",
    "title": "Creating Charts with HoloViews",
    "section": "Why Choose HoloViews?",
    "text": "Why Choose HoloViews?\nSeveral factors make HoloViews an attractive option for data visualization:\n\nSimplicity\n\nHoloViews boasts a user-friendly syntax, enabling you to create complex visualizations with minimal code. This focus on brevity empowers you to iterate quickly and explore your data efficiently.\n\nFlexibility\n\nHoloViews integrates seamlessly with popular data structures like NumPy and Pandas, effortlessly handling your data. Additionally, it plays well with different plotting backends like Bokeh, Plotly and Matplotlib, giving you control over the final look and feel of your visualizations.\n\nInteractivity\n\nHoloViews visualizations are not static images. They can be interactive, allowing users to zoom, pan, and explore the data from various angles. This interactivity fosters deeper engagement and a richer understanding of the information.\n\nComprehensiveness\n\nThe HoloViews ecosystem extends beyond the core library. It encompasses projects like hvPlot for quick visualizations and GeoViews for crafting geographical visualizations. This suite of tools caters to a wide range of data exploration needs."
  },
  {
    "objectID": "portfolio/posts/python/plotting-with-holoviews.html#environment-settings",
    "href": "portfolio/posts/python/plotting-with-holoviews.html#environment-settings",
    "title": "Creating Charts with HoloViews",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nCode\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom itables import init_notebook_mode\ninit_notebook_mode(all_interactive=True)\nimport holoviews as hv\nhv.extension('bokeh')\n\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nThis is the init_notebook_mode cell from ITables v2.2.3\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\nCode\n# Get data\nurl = 'https://raw.githubusercontent.com/shoukewei/data/main/data-pydm/gdp_china_outlier_treated.csv'\ndf = pd.read_csv(url)\n\n\n\n\nCode\n# Dataset preview\nfrom itables import show\n\nshow(df, lengthMenu=[10, 25, 50, 100,])\n\n\n\n\n    \n      \n      prov\n      gdpr\n      year\n      gdp\n      pop\n      finv\n      trade\n      fexpen\n      uinc\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.3 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n\n\nCode\n# Change column name\ndf = df.rename(columns={'prov':'Province',})\n\n\n\n\nCode\n# Convert to holviews dataset\nhd = hv.Dataset(df)"
  },
  {
    "objectID": "portfolio/posts/python/plotting-with-holoviews.html#creating-charts",
    "href": "portfolio/posts/python/plotting-with-holoviews.html#creating-charts",
    "title": "Creating Charts with HoloViews",
    "section": "Creating Charts",
    "text": "Creating Charts\n\n\nCode\n# Customization specs\ngrid_style = {'grid_line_color': 'white',\n              'grid_line_width': 2.,\n             }\n\n# Create chart\ncurves_app = hd.to(hv.Bars, kdims=['year'], vdims=['gdp'], groupby='Province',)\n\n# Chart options\ncurves_app.opts(height=500,\n                width=650,\n                xlabel='Year',\n                tools=['hover'],\n                ylabel='GDP',\n                xrotation=45,\n                toolbar=None, #above, below, left, right\n                fill_color='#1c2841',\n                line_color='black',\n                bgcolor='#f6f6f6',\n                show_grid=True,\n                gridstyle=grid_style,\n               )\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can select a Province from the filter to get its correspondent chart."
  },
  {
    "objectID": "portfolio/posts/python/plotting-with-holoviews.html#conclusions",
    "href": "portfolio/posts/python/plotting-with-holoviews.html#conclusions",
    "title": "Creating Charts with HoloViews",
    "section": "Conclusions",
    "text": "Conclusions\nAs you gain experience with HoloViews, you can delve into its more advanced features.\nHoloViews allows for extensive customization, enabling you to tailor your visualizations to perfectly suit your needs and branding.\nFurthermore, HoloViews integrates well with other data science libraries within the Python ecosystem, fostering a powerful and cohesive environment for data exploration and analysis."
  },
  {
    "objectID": "portfolio/posts/python/plotting-with-holoviews.html#contact",
    "href": "portfolio/posts/python/plotting-with-holoviews.html#contact",
    "title": "Creating Charts with HoloViews",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/datawrapper_api.html",
    "href": "portfolio/posts/python/datawrapper_api.html",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "",
    "text": "Figure¬†1: Datawrapper Charts Catalogue"
  },
  {
    "objectID": "portfolio/posts/python/datawrapper_api.html#key-features-of-datawrapper",
    "href": "portfolio/posts/python/datawrapper_api.html#key-features-of-datawrapper",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Key Features of Datawrapper",
    "text": "Key Features of Datawrapper\n\nEasy-to-use interface\n\nDatawrapper‚Äôs intuitive design makes it accessible to users of all skill levels. Even those without a strong background in data visualization can create professional-looking charts and maps.\n\nDiverse chart¬†types\n\nFrom simple bar charts and line graphs to more complex maps and scatter plots, Datawrapper offers a variety of chart types to suit different data sets and storytelling needs.\n\nCustomization\n\nUsers can customize their visualizations to match their brand or personal style. This includes options for changing colors, fonts, and layouts.\n\nInteractivity\n\nDatawrapper allows users to create interactive visualizations that respond to user input. For example, users can hover over data points to see more detailed information or filter data based on specific criteria.\n\nEmbedding and¬†sharing\n\nOnce a visualization is created, it can be easily embedded into websites, blogs, or social media posts. Users can also share their visualizations directly with others.\n\nCollaboration\n\nDatawrapper supports collaboration, allowing multiple users to work on the same visualization simultaneously. This is particularly useful for teams or organizations that need to create data visualizations together."
  },
  {
    "objectID": "portfolio/posts/python/datawrapper_api.html#use-cases-for-datawrapper",
    "href": "portfolio/posts/python/datawrapper_api.html#use-cases-for-datawrapper",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Use Cases for Datawrapper",
    "text": "Use Cases for Datawrapper\n\nJournalism\n\nDatawrapper is a popular tool among journalists who want to present complex data in a clear and visually engaging way. It can be used to create interactive charts, maps, and infographics that enhance storytelling.\n\nResearch\n\nResearchers can use Datawrapper to visualize their findings and make them more accessible to a wider audience. By presenting data in a visual format, researchers can communicate their ideas more effectively and increase the impact of their work.\n\nBusiness\n\nBusinesses can use Datawrapper to create dashboards, reports, and presentations that help them make data-driven decisions. By visualizing key metrics and trends, businesses can gain valuable insights into their performance.\n\nEducation\n\nTeachers and students can use Datawrapper to create interactive visualizations that help them understand and learn from data. It can be used to teach data analysis, statistics, and other subjects.\n\nVisualization Types\nCollection of available Datawrapper chart types and their IDs\n\n\n\nTable¬†1: Datawrapper Chart Types\n\n\n\n\n\nChart\nTypeID\n\n\n\n\nBar Chart\nd3-bars\n\n\nSplit Bars\nd3-bars-split\n\n\nStacked Bars\nd3-bars-stacked\n\n\nBullet Bars\nd3-bars-bullet\n\n\nDot Plot\nd3-dot-plot\n\n\nRange Plot\nd3-range-plot\n\n\nArrow Plot\nd3-arrow-plot\n\n\nColumn Chart\ncolumn-chart\n\n\nGrouped Column Chart\ngrouped-column-chart\n\n\nStacked Column Chart\nstacked-column-chart\n\n\nArea Chart\nd3-area\n\n\nLine Chart\nd3-lines\n\n\nMultiple Lines Chart\nmultiple-lines\n\n\nPie Chart\nd3-pies\n\n\nDonut Chart\nd3-donuts\n\n\nMultiple Pies\nd3-multiple-pies\n\n\nMultiple Donuts\nd3-multiple-donuts\n\n\nScatter Plot\nd3-scatter-plot\n\n\nElection Donut\nelection-donut-chart\n\n\nTable\ntables\n\n\nChoropleth Map\nd3-maps-choropleth\n\n\nSymbol Map\nd3-maps-symbols\n\n\nLocator Map\nlocator-map\n\n\n\n\n\n\nMore information\n\n\nEnvironment settings\n\n\nShow code\n# Import libraries\nfrom datawrapper import Datawrapper\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow code\n# Token gotten from datawrapper\nfilename = 'credentials.json'\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n# read credentials\ntoken = keys['datawrapper_api']\n\n\n\n\nShow code\n# Access datawrapper api\ndw = Datawrapper(access_token = token)\n\n\n\n\nDatawrapper account\n\n\nShow code\n# Check your account details\ndw.get_my_account()\n\n\n\n\nDataset\n\n\nShow code\n# Read dataset courtesy of Sergio Sanchez\ndf = pd.read_csv(\n    'https://raw.githubusercontent.com/chekos/datasets/master/data/datawrapper_example.csv',\n    sep=\";\",\n)\n\n\n\n\nShow code\n# Select columns\ndf.columns = ['country','%pop in the capital','%pop in urban areas','%pop in rural areas']\n\n\n\n\nShow code\n# Show dataframe\ndf\n\n\n\n\n\n\n\n\n\ncountry\n%pop in the capital\n%pop in urban areas\n%pop in rural areas\n\n\n\n\n0\nIceland (Reykjav√≠k)\n56.02\n38.0\n6.0\n\n\n1\nArgentina (Buenos Aires)\n34.95\n56.6\n8.4\n\n\n2\nJapan (Tokyo)\n29.52\n63.5\n7.0\n\n\n3\nUK (London)\n22.70\n59.6\n17.7\n\n\n4\nDenmark (Copenhagen)\n22.16\n65.3\n12.5\n\n\n5\nFrance (Paris)\n16.77\n62.5\n20.7\n\n\n6\nRussia (Moscow)\n8.39\n65.5\n26.1\n\n\n7\nNiger (Niamey)\n5.53\n12.9\n81.5\n\n\n8\nGermany (Berlin)\n4.35\n70.7\n24.9\n\n\n9\nIndia (Delhi)\n1.93\n30.4\n67.6\n\n\n10\nUSA (Washington, D.C.)\n1.54\n79.9\n18.6\n\n\n11\nChina (Beijing)\n1.40\n53.0\n45.6\n\n\n\n\n\n\n\n\n\nCreate stackbar chart\n\n\nShow code\n# Create Datawrapper bar chart\npop = dw.create_chart(\n    title='Where do people live?', chart_type='d3-bars-stacked', data=df\n)\n\n\n\n\nUpdate chart description\n\n\nShow code\ndw.update_description(\n    pop['id'],\n    source_name = 'UN Population Division',\n    source_url = 'https://population.un.org/wup/',\n    byline = 'Jesus L. Monroy&lt;br&gt;Economist & Data Scientist&lt;br&gt;&lt;br&gt;',\n    intro = 'Population percentage living in the capital by Country'\n)\n\n\n\n\nPublish chart\n\n\nShow code\ndw.publish_chart(chart_id = pop['id'])\n\n\n\n\nCustomize metadata\n\n\nShow code\ndw.update_chart(\n    chart_id = pop['id'],\n    metadata = {\n        'visualize': {\n            'sharing': {'enabled': True},\n            'thick': True,\n            'custom-colors': {\n                '%pop in rural areas': '#dadada',\n                '%pop in urban areas': '#1d81a2',\n                '%pop in the capital': '#15607a',\n               },\n           },\n        'publish': {\n            'blocks': {'get-the-data': False},\n           },\n    }\n)\n\n\n\n\nRepublish chart\n\n\nShow code\ndw.publish_chart(pop['id'])"
  },
  {
    "objectID": "portfolio/posts/python/datawrapper_api.html#display-interactive-chart",
    "href": "portfolio/posts/python/datawrapper_api.html#display-interactive-chart",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure¬†2: Datawrapper Interactive Chart\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can hover over the bars on the above chart to get dynamic results."
  },
  {
    "objectID": "portfolio/posts/python/datawrapper_api.html#conclusions",
    "href": "portfolio/posts/python/datawrapper_api.html#conclusions",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Conclusions",
    "text": "Conclusions\nTo start using Datawrapper, simply create a free account on their website. Once you‚Äôre logged in, you can begin creating your first visualization.\nDatawrapper offers a variety of tutorials and resources to help you get started and make the most of the platform.\nBy leveraging the power of Datawrapper, you can create compelling data visualizations that help you tell your story more effectively.\nWhether you‚Äôre a seasoned data analyst or just starting out, Datawrapper is a valuable tool that can help you bring your data to life."
  },
  {
    "objectID": "portfolio/posts/python/datawrapper_api.html#references",
    "href": "portfolio/posts/python/datawrapper_api.html#references",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "References",
    "text": "References\n\nDatawrapper (2024) Barcharts in Datawrapper Academy\nKhanchandani E. (2020) How to use Datawrapper for journalists in Interhacktives\nSanchez, S. (2023) A lightweight Python wrapper for the Datawrapper API in Datawrapper API"
  },
  {
    "objectID": "portfolio/posts/python/datawrapper_api.html#contact",
    "href": "portfolio/posts/python/datawrapper_api.html#contact",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/python/airports.html",
    "href": "portfolio/posts/python/airports.html",
    "title": "Flying into Mexico",
    "section": "",
    "text": "Figure¬†1: Mexico City‚Äôs International Airport"
  },
  {
    "objectID": "portfolio/posts/python/airports.html#contact",
    "href": "portfolio/posts/python/airports.html#contact",
    "title": "Flying into Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/tableau/covid-cases.html",
    "href": "portfolio/posts/tableau/covid-cases.html",
    "title": "Covid Cases in Americas",
    "section": "",
    "text": "This dashboard was designed to track cases and deaths by covid in Americas in 2024 by region and country.\n\nData Sources: The data comes from Worldometer information about cases and deaths by countries.\nKey Metrics: The dashboard shows cases and deaths KPIs.\nVisualizations: The types of charts used are bar and map charts by country.\nTarget Audience: The dashboard was designed for health follow-up."
  },
  {
    "objectID": "portfolio/posts/tableau/covid-cases.html#overview",
    "href": "portfolio/posts/tableau/covid-cases.html#overview",
    "title": "Covid Cases in Americas",
    "section": "",
    "text": "This dashboard was designed to track cases and deaths by covid in Americas in 2024 by region and country.\n\nData Sources: The data comes from Worldometer information about cases and deaths by countries.\nKey Metrics: The dashboard shows cases and deaths KPIs.\nVisualizations: The types of charts used are bar and map charts by country.\nTarget Audience: The dashboard was designed for health follow-up."
  },
  {
    "objectID": "portfolio/posts/tableau/covid-cases.html#contact",
    "href": "portfolio/posts/tableau/covid-cases.html#contact",
    "title": "Covid Cases in Americas",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/tableau/missing-people.html",
    "href": "portfolio/posts/tableau/missing-people.html",
    "title": "Missing People in Mexico",
    "section": "",
    "text": "This dashboard was designed to track missing people by location, last place seen and place type.\n\nData Sources: The data mainly come from government agency dedicated to attend requests about missing people.\nKey Metrics: The dashboard shows locations where missing people was seen for last time, location origin and missing people by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for government agency for public statistics and operational performance."
  },
  {
    "objectID": "portfolio/posts/tableau/missing-people.html#overview",
    "href": "portfolio/posts/tableau/missing-people.html#overview",
    "title": "Missing People in Mexico",
    "section": "",
    "text": "This dashboard was designed to track missing people by location, last place seen and place type.\n\nData Sources: The data mainly come from government agency dedicated to attend requests about missing people.\nKey Metrics: The dashboard shows locations where missing people was seen for last time, location origin and missing people by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for government agency for public statistics and operational performance."
  },
  {
    "objectID": "portfolio/posts/tableau/missing-people.html#contact",
    "href": "portfolio/posts/tableau/missing-people.html#contact",
    "title": "Missing People in Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/posts/tableau/mexico-city-crimes.html",
    "href": "portfolio/posts/tableau/mexico-city-crimes.html",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "",
    "text": "This dashboard was designed to track crime incidence in Mexico City during 2019-2024 by time, gender, age, mayorship and neighborhood.\n\nData Sources: The data comes from Mexico City Open Data Webpage.\nKey Metrics: The dashboard shows crime KPIs.\nVisualizations: The types of charts used are bar and map charts.\nTarget Audience: The dashboard was designed for crime incidence follow-up."
  },
  {
    "objectID": "portfolio/posts/tableau/mexico-city-crimes.html#overview",
    "href": "portfolio/posts/tableau/mexico-city-crimes.html#overview",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "",
    "text": "This dashboard was designed to track crime incidence in Mexico City during 2019-2024 by time, gender, age, mayorship and neighborhood.\n\nData Sources: The data comes from Mexico City Open Data Webpage.\nKey Metrics: The dashboard shows crime KPIs.\nVisualizations: The types of charts used are bar and map charts.\nTarget Audience: The dashboard was designed for crime incidence follow-up."
  },
  {
    "objectID": "portfolio/posts/tableau/mexico-city-crimes.html#contact",
    "href": "portfolio/posts/tableau/mexico-city-crimes.html#contact",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "portfolio/about.html",
    "href": "portfolio/about.html",
    "title": "About me",
    "section": "",
    "text": "Economist & Data Scientist\n\nI‚Äôve applied data analysis and visualization to drive strategic value for 10+ years across industries like public security, e-commerce, and healthcare.\nI use Python1, SQL, and Tableau to transform raw data into clear, actionable data products (webpages, slideshows, interactive reports) that support decision-making, occasionally incorporating regression and classification analyses.\nCommitted to fostering data literacy, I share data posts on my Blog and also on Learning Data and T3CH via Medium.\nBackground in Economics (BA), Information Technology (MA) and Data Science and Machine Learning (Certification).\nVegan.\n¬© 2025"
  },
  {
    "objectID": "portfolio/about.html#footnotes",
    "href": "portfolio/about.html#footnotes",
    "title": "About me",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIncluding libraries such as numpy, pandas, polars, duckdb, matplotlib, seaborn, plotly, folium, sckitlearn.‚Ü©Ô∏é"
  }
]