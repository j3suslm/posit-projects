[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "An Application of Location Quotients to High-Impact Offenses\n\n\nCrime Localization in Mexico 2025\n\n\n\npython\n\n\n\nTraditional crime rate metrics often mask geographic risk disparities by presenting aggregate averages.  In contrast, the LQ provides a robust analytical tool for determining whether a specific geographic area experiences a disproportionate share of a given crime type relative to a larger reference area, thereby highlighting true criminal hotspots.\n\n\n\n\n\nDec 8, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City Crime According to the Statistics\n\n\nIssues and Figures, 2019-2024\n\n\n\npython\n\n\n\nCrime in Mexico City presents a complex and evolving challenge. The city, a sprawling metropolis, grapples with a range of criminal activities, from petty theft and street-level drug offenses to organized crime and violent acts. Factors contributing to this multifaceted issue include socioeconomic disparities, corruption, and the influence of transnational criminal organizations.\n\n\n\n\n\nJun 29, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City Crime Incidence 2019-2024\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nStop Building Dashboards\n\n\nStart Understanding the White Collar Workflow\n\n\n\npython\n\nsql\n\n\n\nIn today’s data-driven world, Business Intelligence (BI) tools promise powerful insights and streamlined reporting. Yet, the humble spreadsheet and slideshow persist in the white-collar world. While BI tools manages complex analysis and visualization, spreadsheets and slideshows offer unique advantages that keep them firmly entrenched in our workflows.\n\n\n\n\n\nFeb 24, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGasoline Prices in Mexico\n\n\nFuel Price Fluctuations, A Constant Concern\n\n\n\npython\n\n\n\nGasoline prices in Mexico are a complex issue influenced by various factors. In this article we examine the gasoline prices in the 32 States that constitute Mexico and also their differences in cost, sale price and profits. Challenges remain in stabilizing prices and mitigating the impact on consumers and the economy.\n\n\n\n\n\nFeb 3, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nThis project explores how to leverage the strengths of DuckDB and MotherDuck to build a robust data processing and storage solution. DuckDB excels at fast in-memory analytics, while MotherDuck provides a scalable and cost-effective cloud data warehouse. By combining these technologies, you can achieve optimal performance for both local and cloud-based data operations.\n\n\n\n\n\nJan 13, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nData Alchemy, SQL Analysis within Python\n\n\nA Case Study using Duckdb, Polars and Plotly\n\n\n\npython\n\nsql\n\n\n\nThis project focuses on leveraging the strengths of DuckDB, Polars, and Plotly for efficient data analysis and visualization. DuckDB is used for fast in-memory data processing, Polars provides a user-friendly and high-performance DataFrame library, and Plotly offers interactive and customizable visualizations.\n\n\n\n\n\nJan 6, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Analysis in Python\n\n\nA Case Study using Duckdb, Polars and Folium\n\n\n\npython\n\n\n\nGeospatial analysis involves the application of spatial concepts and techniques to data that has geographic coordinates. With the rise of big data and the increasing availability of geospatial information, the demand for effective geospatial analysis tools has grown significantly. Python, with its rich ecosystem of libraries, has emerged as a powerful and popular choice for geospatial data scientists.\n\n\n\n\n\nJan 2, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSports Commerce\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy BI Tools Fall Short, A Failure to Capture the Office Workflow\n\n\nSlideshows and Spreadsheets Still Rule the Business World\n\n\n\npython\n\n\n\nDespite BI solutions, spreadsheets & slideshows persist in data-driven decision making.\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge Your SQL Analysis with Python and DuckDB\n\n\nRethinking Data Analysis: A New Paradigm\n\n\n\npython\n\nsql\n\n\n\nThis article explores the synergy between Python and DuckDB, a powerful in-memory database, to revolutionize SQL-based data analysis. By leveraging Python’s extensive data science ecosystem and DuckDB’s lightning-fast query execution, data professionals can significantly accelerate their workflows.\n\n\n\n\n\nNov 23, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid Cases in Americas\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSpotipy, A Python Library for Spotify Music Lovers\n\n\nDiscover your Music Habits with Python\n\n\n\npython\n\n\n\nEver wondered about the intricacies of your listening habits on Spotify? Or perhaps you’re a developer looking to build a music-related application? Enter Spotipy, a fantastic Python library that acts as a bridge between your code and the Spotify Web API.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFlying into Mexico\n\n\nA Bird’s-Eye View of Major Airports\n\n\n\npython\n\n\n\nAs a popular tourist destination, Mexico offers visitors a diverse range of experiences, from ancient ruins to pristine beaches. The country’s well-connected airport system ensures seamless travel to these captivating destinations.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Stunning Charts with Datawrapper and Python\n\n\nAn Overview\n\n\n\npython\n\n\n\nDatawrapper is a powerful online tool designed to help you create engaging and informative data visualizations. Whether you’re a journalist, researcher, or simply someone who wants to present data in a more visually appealing way, Datawrapper can make the process quick and easy.\n\n\n\n\n\nSep 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City’s Underground\n\n\nA History of Challenges and Triumphs\n\n\n\npython\n\n\n\nMexico City’s metro is the largest and busiest in Latin America, serving more than 5.5 million passengers daily in its 195 stations and 12 lines\n\n\n\n\n\nSep 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico’s Peace index 2024\n\n\n\n\n\n\npython\n\n\n\nThe Mexico Peace Index (MPI), created by the Institute for Economics and Peace (IEP), is a valuable tool for understanding peacefulness in Mexico. According to IEP, Mexico’s peacefulness has improved for 04 years in a row. However, the situation isn’t uniform. While 15 states showed improvement, 17 got worse. Drug cartel activity and political violence remain challenges.\n\n\n\n\n\nMay 17, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Data Pipelines with BigQuery and Python\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn the realm of data analytics, Extract, Transform, Load (ETL) processes play a pivotal role. They streamline the integration of data from various sources, enabling its cleaning, manipulation, and loading into target systems like BigQuery, Google’s cloud-based data warehouse. By leveraging Python’s versatility and BigQuery’s scalability, you can construct powerful ETL pipelines to prepare your data for insightful analysis.\n\n\n\n\n\nMay 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Charts with HoloViews\n\n\nUnveiling Insights with Less Effort by Using HoloViews\n\n\n\npython\n\n\n\nHoloViews offers a compelling alternative for data visualization in Python. With its emphasis on simplicity, flexibility, and interactivity, HoloViews empowers you to create insightful visualizations that effectively communicate your data’s story.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nNational Guard’s Deployed Personnel in Mexico\n\n\n\n\n\n\npython\n\n\n\nMexico’s National Guard is a relatively new security force established in 2019. It was created to address the country’s high crime rates and complement traditional law enforcement.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Dashboards with Python\n\n\nUnleash the Power of Python\n\n\n\npython\n\n\n\nPython, a versatile programming language, empowers you to create interactive dashboards that unlock the hidden potential of your data.\n\n\n\n\n\nApr 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Use Mermaid to Create Diagrams as a Data Scientist\n\n\nUse Python code as your Secret Weapon\n\n\n\npython\n\n\n\nIn this article, we will show how you can create diagrams with code within Jupyter and stop using external diagramming graphical user interfaces (GUIs) like draw.io or Lucidchart.\n\n\n\n\n\nMar 29, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Slides with Python and Quarto\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico’s Political Landscape\n\n\n\n\n\n\npython\n\n\n\nIn this article, we will show an overview of Mexico’s Political Scenario and its correlation with Mexico City’s.\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDuckDB: A Compelling Solution for Data Analysis within Python Environment\n\n\nA valuable tool for data scientists working with Python due to its speed, ease of use, and tight integration.\n\n\n\npython\n\nsql\n\n\n\nDuckDB is a fast, embedded analytical database that outstands in in-memory operations. It provides a SQL interface, making it easy for users with database querying experience.\n\n\n\n\n\nMar 18, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico: A Populous Nation with a Dynamic Demographic Landscape\n\n\nAn Overview of Mexico’s Population Dynamics since 1950\n\n\n\npython\n\n\n\nMexico boasts a population of nearly 130 million, ranking it as the 10th most populous country globally. Mexico is a highly urbanized nation, with 88% of the population residing in cities. This trend is expected to continue, driven by economic opportunities and infrastructure development.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWater Collection System in Mexico City - 2022\n\n\nRainwater harvesting systems are a prominent water collection method in Mexico\n\n\n\npython\n\n\n\nRainwater harvesting is a sustainable and effective way to manage water resources in Mexico. By promoting water conservation and increasing water security, these systems offer a path towards a more water-resilient future.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating JSON Files with Python\n\n\nAn Overview Using Pandas\n\n\n\npython\n\n\n\nThis article presents a straightforward and efficient method for reading JSON files using Python Pandas library. We will show how to import JSON data into DataFrames, enabling comprehensive analysis and exploration. This approach significantly simplifies the process of working with JSON data, making it accessible for data analysis.\n\n\n\n\n\nMar 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Lego Toys with Polars\n\n\nA quick summary about Lego bricks\n\n\n\npython\n\n\n\nThe Lego brick was invented in 1949 by Ole Kirk Christiansen, and the company has since grown to become one of the world’s leading toy manufacturers. Lego products are sold in over 140 countries, and the company has over 40,000 employees worldwide.\n\n\n\n\n\nMar 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFederal Prisons in Mexico\n\n\nA Geographical Overview of Mexico’s Federal Penal System\n\n\n\npython\n\n\n\nMexico’s federal prisons, known as Ceferesos (Centros Federales de Readaptación Social), are a complex system facing challenges. While intended for rehabilitation, reports often highlight overcrowding, violence, and inadequate living conditions. In this article we will show the location of federal prisons in Mexico, including the famous Islas Marías, which closed in 2019. We wil use Folium, a powerful Python library to enhance data analysis and geographic visualization.\n\n\n\n\n\nMar 6, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression\n\n\nAn Extension for Linear Regression Models\n\n\n\npython\n\n\n\nIn this article, we shall show where linear regression falls short and we should use polynomial regression instead.\n\n\n\n\n\nMar 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Analysis Overview\n\n\nA quick summary about linear and nonlinear regression\n\n\n\npython\n\n\n\nWhether you want to do statistics, machine learning, or economic analysis, it’s likely that you will have to use regression analysis. Regression analysis is one of the most important fields in statistics, economics and machine learning. We shall briefly explore the different techniques about regression.\n\n\n\n\n\nFeb 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCohort Analysis\n\n\nUnderstanding Customer Behavior\n\n\n\npython\n\n\n\nCohort analysis is an extremely helpful tool that can be used to improve business practices and can effectively increase user retention if businesses implement necessary changes according to the test results. In today’s world where data is everywhere, cohort analysis is effective in extracting useful information by analyzing the behavioral patterns of customers in order to predict the future of the business. Observing cohorts over time gives insight into user experience and helps in developing better tactics.\n\n\n\n\n\nFeb 9, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the Power of Big Data with PySpark\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn today’s data-driven world, we’re constantly bombarded with massive amounts of information. Analyzing this data efficiently and effectively is crucial for businesses and researchers alike. That’s where PySpark comes in. It’s a powerful tool that brings the distributed computing capabilities of Apache Spark to the familiar and versatile Python ecosystem.\n\n\n\n\n\nJan 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMissing People in Mexico\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUS Sales Dashboard\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nLooker Studio\n\n\n\n\n\n\nlooker-studio\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nTableau Charts example\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDiving into the World of SQL\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nJesus L. Monroy\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sql/sql.html#what-is-sql",
    "href": "posts/sql/sql.html#what-is-sql",
    "title": "Diving into the World of SQL",
    "section": "What is SQL?",
    "text": "What is SQL?\nSQL stands for Structured Query Language. It is a standardized language for managing data in relational databases. Think of a relational database as a highly organized digital filing cabinet, where information is stored in tables with rows and columns. SQL provides the tools to not only retrieve specific data from these tables but also to create, modify, and delete tables and their contents."
  },
  {
    "objectID": "posts/sql/sql.html#why-learn-sql",
    "href": "posts/sql/sql.html#why-learn-sql",
    "title": "Diving into the World of SQL",
    "section": "Why Learn SQL?",
    "text": "Why Learn SQL?\nThe widespread adoption of relational databases like MySQL, PostgreSQL, Microsoft SQL Server, and Oracle Database makes SQL a highly sought-after skill. Learning SQL opens doors to a variety of opportunities, including:\n\nData Analysis\n\nSQL allows you to extract meaningful insights from vast datasets, identify trends, and generate reports that drive business decisions.\n\nDatabase Administration\n\nManaging and maintaining databases, ensuring data integrity, and optimizing performance often requires a deep understanding of SQL.\n\nWeb Development\n\nMany web applications rely on databases to store and retrieve information. SQL is essential for building dynamic websites and applications.\n\nData Science\n\nSQL is a fundamental tool for data scientists, enabling them to clean, prepare, and explore data before applying more advanced statistical and machine learning techniques."
  },
  {
    "objectID": "posts/sql/sql.html#key-sql-concepts",
    "href": "posts/sql/sql.html#key-sql-concepts",
    "title": "Diving into the World of SQL",
    "section": "Key SQL Concepts:",
    "text": "Key SQL Concepts:\nSQL is built around a set of commands that allow you to perform various operations. Here are some of the most important concepts:\n\nSELECT\n\nThis command is used to retrieve data from one or more tables based on specified criteria. You can select specific columns, filter data using conditions, and sort the results. For example: SELECT name, age FROM users WHERE city = ‘New York’;\n\nINSERT\n\nThis command adds new rows of data into a table. For example: INSERT INTO users (name, age, city) VALUES (‘John Doe’, 30, ‘London’);\n\nUPDATE\n\nThis command modifies existing data in a table. For example: UPDATE users SET age = 31 WHERE name = ‘John Doe’;\n\nDELETE\n\nThis command removes rows from a table. For example: DELETE FROM users WHERE age = 65;\n\nCREATE\n\nThis command is used to create new database objects, such as tables, views, and indexes.\n\nDROP: This command deletes existing database objects."
  },
  {
    "objectID": "posts/sql/sql.html#tasks-performed-with-sql",
    "href": "posts/sql/sql.html#tasks-performed-with-sql",
    "title": "Diving into the World of SQL",
    "section": "Tasks performed with SQL",
    "text": "Tasks performed with SQL\n\nCreate and drop tables\nInsert and update records\nRetrieve data\nDelete data\nManage permissions\nCreate views and procedures"
  },
  {
    "objectID": "posts/sql/sql.html#advanced-tasks-in-sql",
    "href": "posts/sql/sql.html#advanced-tasks-in-sql",
    "title": "Diving into the World of SQL",
    "section": "Advanced tasks in SQL",
    "text": "Advanced tasks in SQL\n\nJOINs: Combining data from multiple tables based on related columns.\nSubqueries: Embedding queries within other queries to perform more sophisticated data retrieval.\n\nAggregate Functions: Performing calculations on groups of data, such as calculating averages, sums, and counts.\n\nWindow functions:"
  },
  {
    "objectID": "posts/sql/sql.html#example-queries",
    "href": "posts/sql/sql.html#example-queries",
    "title": "Diving into the World of SQL",
    "section": "Example queries",
    "text": "Example queries\n-- COUNT CUSTOMERS\nSELECT COUNT(1) AS customers\nFROM addresses\n;\n-- GET RETAIL SALES DATASET\nSELECT p.id_purchase\n    , d.priceInfo\n    , d.quantity\n    , s.id_shopper\n    , s.id_region\n    , r.description AS region\n    , s.id_territory\n    , t.description AS territory\n    , s.id_store\n    , b.description AS store\n    , s.NUD\n    , s.shop\n    , s.route\n    , s.purchase_date\n    , p.cancel\nFROM purchases AS p\nINNER JOIN purchase_details AS d ON p.id_purchase = d.id_purchase \nINNER JOIN shops AS s ON p.id_shopper = s.id_shopper\nINNER JOIN territories AS t ON s.id_territory = t.id_territory\nINNER JOIN stores AS b ON s.id_store = b.id_store\nINNER JOIN regions AS r ON s.id_region = r.id_region\n;\n-- GET PURCHASES BY CUSTOMER\nSELECT o.no_order\n    , o.id_order_status\n    , e.description\n    , o.id_social_network\n    , CASE\n        WHEN o.id_social_network = 1 THEN 'I'\n        WHEN o.id_social_network = 2 THEN 'F'\n        ELSE 'W'\n    END AS origin\n    , o.quantity \n    , o.total AS total_order\n    , o.purchase_date\n    , d.route AS delivery_route\n    , d.id_store \n    , d.nud\n    , d.email \n    , d.phone \nFROM orders o\nLEFT JOIN orderStatus e ON o.id_order_status = e.id_order_status\nLEFT JOIN addresses d ON o.id_address = d.id_address\nWHERE 1 = 1\n-- AND DATE(fs_ConvertDateToMexico(o.purchase_date))\n-- BETWEEN '2023-04-01' AND '2023-04-30'\n;\n-- GET UNIQUE BRANDS\nSELECT DISTINCT g.brand_code as Brand_Code\n, g.brand_name as Brand_Name\n, g.style_num_offset as Style_Num_Offset\n, g.active as Active\n, g.parent_brand as Parent_Brand\n, g.reporting_brand as Reporting_Brand\nFROM\ntp_brand g\n;\n-- GET FACTORY DATA WITH NULL COUNTRY OR CITY\nSELECT CONVERT(e.id,char) as Factory_Id\n, e.factory_name  as PO_FTY\n, ifnull(e.factory_city,'No City') as City\n, ifnull(e.factory_country,'No Country') as Country\nFROM\ntp_factories e\n;\n-- INSER DATA INTO TABLE\ninsert into CatAlumnos\nvalues(‘Juan’, ‘Perez’, ‘Huerta’, 1990-02-25, 2, ‘a’)\n;\n/* GET STUDENT AGE POSTGRES FUNCTION */\nWITH cte_ages AS (\n    SELECT name, age(CURRENT_DATE, bod) AS student_age\n    FROM students\n)\nSELECT name, age\nFROM ages\nWHERE age &lt; 18\n;\n-- CREATE VIEW IN SQL SERVER\nCREATE VIEW student_profiles AS\n    SELECT concat(name, ' ' surname) AS student_name,\n/* Using SQL Server datediff function to reckon age */\n    DATEDIFF(YY, getdate(), bod) as age\n    FROM students\n    ORDER BY name\n;\n/* CONCAT AND CAST DATA TYPES */\nSELECT \n  concat('gsn','_', cast(campaign_id as text)) AS id_gsn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_2\nTOP 5\n;\n/* CREATE A UNION OF TABLES FROM 02 TEMP TABLES (CTEs) */\nWITH table_2_temp AS (\nSELECT\n  concat('gsn', '_', cast(campaign_id as text)) AS id_gsn\n, start_date \n, end_date\n, clicks\n, views\nFROM table_2\n)\n, table_3_temp AS (\nSELECT \nconcat('fbn', '_', cast(campaign_id as text)) AS id_fbn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_3\nWHERE cliks &gt; 0\n)\nSELECT \n  t2.id_gsn\nFROM table_2_temp t2\nUNION ALL\nSELECT \n  t3.id_fbn\nFROM table_3_temp t3\n;\n/* Window functions\nAdd row numbers to the placings table */\nSELECT\n    'year'\n    , host_country\n    , first_place\n    , total_goals\n    , row_number() OVER() AS row_num\nFROM world_cup_placings\n;\n\n/* Using SUM() within our window function */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , SUM(total_goals) OVER() AS all_goals\nFROM world_cup_placings\n;\n\n/* Computing the average number of goals */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , round(avg(total_goals) OVER(), 0) AS mean_goals\nFROM world_cup_placings\n;\n-- Big Query platform\n-- covid cases in North America 2020-2023\nwith years as (\n    select country_region\n        , extract(year from date) as year\n        , sum(confirmed) as total_confirmed\n        , sum(deaths) as total_deaths\n        , sum(cast(recovered as integer)) as total_recovered\n        , sum(active) as total_active\n    from `big-query.public-data.covid19_jhu_csse.summary`\n    where country_region in ('Mexico', 'US', 'Canada')\n    group by 1, 2\n    order by 1, 2\n)\nselect *\nfrom (\n    select \n        year\n        , country_region\n        , total_confirmed\n    from years\n)\npivot(sum(total_confirmed) as confirmed for year in (2020, 2021, 2022, 2023))\n;"
  },
  {
    "objectID": "posts/sql/sql.html#contact",
    "href": "posts/sql/sql.html#contact",
    "title": "Diving into the World of SQL",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/airports.html",
    "href": "posts/python/airports.html",
    "title": "Flying into Mexico",
    "section": "",
    "text": "Figure 1: Mexico City’s International Airport"
  },
  {
    "objectID": "posts/python/airports.html#contact",
    "href": "posts/python/airports.html#contact",
    "title": "Flying into Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html",
    "href": "posts/python/mermaid-diagrams.html",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "",
    "text": "Figure 1: Mermaid for diagramming"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "href": "posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Data Science Cycle Diagram using Mermaid",
    "text": "Data Science Cycle Diagram using Mermaid\n\n\nCode\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D\n\n\n\n\n\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#flow-process",
    "href": "posts/python/mermaid-diagrams.html#flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Flow Process",
    "text": "Flow Process\n\n\nCode\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\n\n\nAdding colors\n\n\nCode\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#complex-flow-process",
    "href": "posts/python/mermaid-diagrams.html#complex-flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Complex Flow Process",
    "text": "Complex Flow Process\ngraph\n    A[ItemA]--&gt; ItemB\n    A--&gt; ItemC\n    subgraph ItemC\n        D[ItemD]--&gt;E[ItemE]\n        E--&gt;D\n    end\n\n    X[ItemX]==&gt;Decision\n    click X \"https://mermaid.js.org/\"\n    Decision{Item Y?}==&gt;|Yes| Y[ItemY]\n    Decision==&gt;|No| Z[ItemZ]"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#pie-charts",
    "href": "posts/python/mermaid-diagrams.html#pie-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Pie charts",
    "text": "Pie charts\npie title Pie chart 2\n    \"Category A\" : 2000\n    \"Category B\" : 500\n    \"Category C\" : 1000"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#gantt-chart",
    "href": "posts/python/mermaid-diagrams.html#gantt-chart",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Gantt Chart",
    "text": "Gantt Chart\ngantt \n    title Gantt chart 1\n    dateFormat YYYY-MM-DD\n    axisFormat %b %d\n    section Section A\n        Task A1: a1, 2024-04-15, 7d\n        Task A2: after a1, 5d\n    section Section B\n        Task B1: 2024-04-22, 2024-05-02\n        Task B2: 2024-04-29, 5d"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#journey-charts",
    "href": "posts/python/mermaid-diagrams.html#journey-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Journey Charts",
    "text": "Journey Charts\njourney\n    title Journey 1\n    section Section A\n        Activity 1A: 5: Person1\n        Activity 2A: 3: Person2\n        Activity 3A: 2: Person1, Person2\n    section Section B\n        Activity 1B: 4: Person2\n        Activity 1B: 5: Person1"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#state-diagram",
    "href": "posts/python/mermaid-diagrams.html#state-diagram",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "State Diagram",
    "text": "State Diagram\nstateDiagram\n    [*] --&gt; StateA\n    StateA --&gt; [*]\n    StateA --&gt; StateB\n    StateB --&gt; StateA\n    StateB --&gt; StateD\n    StateD --&gt; [*]"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#items-diagrams",
    "href": "posts/python/mermaid-diagrams.html#items-diagrams",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Items diagrams",
    "text": "Items diagrams\nerDiagram\n    Item1 ||--o{ Item2: text1\n    Item2 ||--|{ Item3: text2\n    Item1 }|..|{ Item4: text3\n    Item4 }o--o{ Item4: text3"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#conclusions",
    "href": "posts/python/mermaid-diagrams.html#conclusions",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Conclusions",
    "text": "Conclusions\nMermaid is a popular markdown-like syntax for generating diagrams and charts using Markdown. Think of it as a way to describe your diagrams in a human-readable format, which Mermaid then renders into a visual representation.\nMermaid Python acts as a bridge, allowing you to generate these Mermaid diagrams within your Python environment and even embed them in your Jupyter notebooks or web applications.\nMermaid Python also allows you to customize the appearance of your diagrams, add styling, and even integrate with other Python libraries. You can dynamically generate diagrams based on your data, making it a powerful tool for data visualization and exploration."
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#contact",
    "href": "posts/python/mermaid-diagrams.html#contact",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/gasoline-web-nocode.html",
    "href": "posts/python/gasoline-web-nocode.html",
    "title": "Gasoline Prices in Mexico",
    "section": "",
    "text": "Figure 1: Gas Prices in Mexico Report"
  },
  {
    "objectID": "posts/python/gasoline-web-nocode.html#table-overview",
    "href": "posts/python/gasoline-web-nocode.html#table-overview",
    "title": "Gasoline Prices in Mexico",
    "section": "Table overview",
    "text": "Table overview\nThe table below provides a preview of the working dataset, offering a glimpse into the structure and type of data being utilized.\n\n\n\n\n\n\n\n\n\n\nstate\nmunicipality\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nCHIHUAHUA\nJUAREZ\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nCOAHUILA DE ZARAGOZA\nSABINAS\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nCOAHUILA DE ZARAGOZA\nTORREON\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nDURANGO\nGOMEZ PALACIO\n$21.63\n$23.31\n$1.68\n7.76%\n\n\nNUEVO LEON\nABASOLO\n$21.63\n$23.31\n$1.68\n7.76%\n\n\n\n\n\n\nTable 1: Dataset preview"
  },
  {
    "objectID": "posts/python/gasoline-web-nocode.html#top-05-states-with-highest-gasoline-prices",
    "href": "posts/python/gasoline-web-nocode.html#top-05-states-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with highest gasoline prices",
    "text": "Top 05 States with highest gasoline prices\nThe following table presents a snapshot of the States with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nQUINTANA ROO\n$21.46\n$24.80\n$3.34\n15.55%\n\n\nYUCATAN\n$21.46\n$24.79\n$3.33\n15.52%\n\n\nNAYARIT\n$22.54\n$24.76\n$2.23\n9.90%\n\n\nGUERRERO\n$22.37\n$24.76\n$2.39\n10.67%\n\n\nSINALOA\n$22.36\n$24.72\n$2.36\n10.59%\n\n\n\n\n\n\nTable 2: Top 05 States with highest prices"
  },
  {
    "objectID": "posts/python/gasoline-web-nocode.html#top-05-states-with-lowest-gasoline-prices",
    "href": "posts/python/gasoline-web-nocode.html#top-05-states-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 States with lowest gasoline prices",
    "text": "Top 05 States with lowest gasoline prices\nThe following table presents a snapshot of the States with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nstate\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nTAMAULIPAS\n$21.22\n$23.10\n$1.87\n8.79%\n\n\nCOAHUILA DE ZARAGOZA\n$22.25\n$23.12\n$0.87\n4.01%\n\n\nCHIHUAHUA\n$21.47\n$23.36\n$1.89\n8.90%\n\n\nNUEVO LEON\n$21.41\n$23.40\n$1.99\n9.32%\n\n\nVERACRUZ DE IGNACIO DE LA LLAVE\n$21.59\n$23.44\n$1.86\n8.62%\n\n\n\n\n\n\nTable 3: Top 05 States with lowest prices"
  },
  {
    "objectID": "posts/python/gasoline-web-nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "href": "posts/python/gasoline-web-nocode.html#top-05-municipalities-with-highest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with highest gasoline prices",
    "text": "Top 05 Municipalities with highest gasoline prices\nThe following table presents a snapshot of the Municipalities with highest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nNUEVA ITALIA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nZIRACUARETIRO, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nGABRIEL ZAMORA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nHUIRAMBA, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\nNAHUATZEN, MICHOACAN DE OCAMPO\n$22.52\n$25.17\n$2.65\n11.78%\n\n\n\n\n\n\nTable 4: Top 05 Municipalities with highest prices"
  },
  {
    "objectID": "posts/python/gasoline-web-nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "href": "posts/python/gasoline-web-nocode.html#top-05-municipalities-with-lowest-gasoline-prices",
    "title": "Gasoline Prices in Mexico",
    "section": "Top 05 Municipalities with lowest gasoline prices",
    "text": "Top 05 Municipalities with lowest gasoline prices\nThe following table presents a snapshot of the Municipalities with lowest gasoline prices in Mexico.\n\n\n\n\n\n\n\n\n\n\nmun_state\ngasoline_cost\nsale_price\nprofit\nprofit%\n\n\n\n\nANAHUAC, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nCD. GUERRERO, TAMAULIPAS\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nLAMPAZOS DE NARANJO, NUEVO LEON\n$20.57\n$20.74\n$0.17\n0.84%\n\n\nALLENDE, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\nMORELOS, COAHUILA DE ZARAGOZA\n$22.59\n$21.02\n$-1.57\n-6.94%\n\n\n\n\n\n\nTable 5: Top 05 Municipalities with lowest prices"
  },
  {
    "objectID": "posts/python/dashboards.html",
    "href": "posts/python/dashboards.html",
    "title": "Creating Dashboards with Python",
    "section": "",
    "text": "Customization\n\nUnlike pre-built tools, Python allows you to tailor dashboards to your specific needs. Present the data exactly how you want, focusing on the metrics that matter most.\n\nInteractivity\n\nPython lets you create dynamic dashboards where users can explore data, filter information, and drill down for deeper analysis. This fosters engagement and a more intuitive understanding of the data.\n\nPython’s Ecosystem\n\nPython boasts a rich landscape of libraries like Plotly, Altair and Pyshiny. These libraries provide a plethora of visualization options, enabling you to create clear, compelling, and informative charts, graphs, and other visual elements.\n\nEfficiency\n\nPython streamlines the process of data manipulation and analysis. Automate repetitive tasks and integrate data from various sources, saving you valuable time and effort.\n\nOpen-Source\n\nBeing open-source, Python offers significant cost savings compared to proprietary dashboarding tools. Plus, the large and active Python community provides ample resources and support to help you on your journey.\n\nSharability\n\nPython-built dashboards are readily shareable across your organization. Disseminate valuable insights to colleagues and stakeholders, fostering better collaboration and data-driven decision-making.\nBy leveraging Python for dashboard creation, you gain control, flexibility, and a powerful toolset to unlock the true potential of your data. Get ready to transform information into clear, actionable insights that drive informed decisions and positive outcomes.\n\n\n\n\n\n\n\n\nNoteInfo\n\n\n\nYou can interact with the dashboard herein or you can click here to open in a new page."
  },
  {
    "objectID": "posts/python/dashboards.html#contact",
    "href": "posts/python/dashboards.html#contact",
    "title": "Creating Dashboards with Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/spatial_analysis.html",
    "href": "posts/python/spatial_analysis.html",
    "title": "Geospatial Analysis in Python",
    "section": "",
    "text": "Figure 1: Image by Henrikki Tenkanen, Vuokko Heikinheimo, David Whipp"
  },
  {
    "objectID": "posts/python/spatial_analysis.html#environment-setting",
    "href": "posts/python/spatial_analysis.html#environment-setting",
    "title": "Geospatial Analysis in Python",
    "section": "Environment setting",
    "text": "Environment setting\n\n\nCode\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport folium\nfrom great_tables import GT, md\nfrom warnings import filterwarnings\nfilterwarnings('ignore')"
  },
  {
    "objectID": "posts/python/spatial_analysis.html#data-collection",
    "href": "posts/python/spatial_analysis.html#data-collection",
    "title": "Geospatial Analysis in Python",
    "section": "Data collection",
    "text": "Data collection\n\n\nCode\nconn = db.connect('datasets/geospatial.db')\n\n\n\n\nCode\nconn.sql('show tables')\n\n\n┌─────────┐\n│  name   │\n│ varchar │\n├─────────┤\n│ zomato  │\n└─────────┘\n\n\n\n\nCode\ndata = conn.sql('select * from zomato').pl()\n\n\n\n\nCode\ndata.columns\n\n\n['url',\n 'address',\n 'name',\n 'online_order',\n 'book_table',\n 'rate',\n 'votes',\n 'phone',\n 'location',\n 'rest_type',\n 'dish_liked',\n 'cuisines',\n 'approx_cost(for two people)',\n 'reviews_list',\n 'menu_item',\n 'listed_in(type)',\n 'listed_in(city)']\n\n\n\n\nCode\ndata.shape\n\n\n(51717, 17)"
  },
  {
    "objectID": "posts/python/spatial_analysis.html#data-preprocessing",
    "href": "posts/python/spatial_analysis.html#data-preprocessing",
    "title": "Geospatial Analysis in Python",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\n\nCode\ndata.is_duplicated().sum()\n\n\n0\n\n\n\n\nCode\ndata.select(pl.all().is_null().sum()).to_dicts()\n\n\n[{'url': 0,\n  'address': 0,\n  'name': 0,\n  'online_order': 0,\n  'book_table': 0,\n  'rate': 7775,\n  'votes': 0,\n  'phone': 1208,\n  'location': 21,\n  'rest_type': 227,\n  'dish_liked': 28078,\n  'cuisines': 45,\n  'approx_cost(for two people)': 346,\n  'reviews_list': 0,\n  'menu_item': 0,\n  'listed_in(type)': 0,\n  'listed_in(city)': 0}]\n\n\n\n\nCode\n# As we have few missing values in location feature ,then we can drop the null\ndata = data.drop_nulls(subset=pl.col('location'))\n\n\n\n\nCode\n(\n    GT(data.select('address','name','rate','votes','location','rest_type','dish_liked','cuisines').head(3))\n    .tab_header(\n        title=md('Zomato Restaurants')\n    )\n    .cols_width(\n        cases={'rate':'50px',\n              }\n               )\n    .tab_source_note(source_note=md('&lt;br&gt; *Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants\n\n\naddress\nname\nrate\nvotes\nlocation\nrest_type\ndish_liked\ncuisines\n\n\n\n\n942, 21st Main Road, 2nd Stage, Banashankari, Bangalore\nJalsa\n4.1/5\n775\nBanashankari\nCasual Dining\nPasta, Lunch Buffet, Masala Papad, Paneer Lajawab, Tomato Shorba, Dum Biryani, Sweet Corn Soup\nNorth Indian, Mughlai, Chinese\n\n\n2nd Floor, 80 Feet Road, Near Big Bazaar, 6th Block, Kathriguppe, 3rd Stage, Banashankari, Bangalore\nSpice Elephant\n4.1/5\n787\nBanashankari\nCasual Dining\nMomos, Lunch Buffet, Chocolate Nirvana, Thai Green Curry, Paneer Tikka, Dum Biryani, Chicken Biryani\nChinese, North Indian, Thai\n\n\n1112, Next to KIMS Medical College, 17th Cross, 2nd Stage, Banashankari, Bangalore\nSan Churro Cafe\n3.8/5\n918\nBanashankari\nCafe, Casual Dining\nChurros, Cannelloni, Minestrone Soup, Hot Chocolate, Pink Sauce Pasta, Salsa, Veg Supreme Pizza\nCafe, Mexican, Italian\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable 1: Zomato Restaurants from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\n\n\nCode\n# create a copy\ndf = data.clone()\n\n\nLets make every place more readible so that u will get more more accurate geographical co-ordinates..\n\n\nCode\ndf = df.with_columns(\n    location=(pl.col('location') + ', Bangalore, Karnataka, India')\n)\n\n\n\n\nCode\ndf.select('location').sample(5).to_dicts()\n\n\n[{'location': 'HSR, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'},\n {'location': 'BTM, Bangalore, Karnataka, India'}]\n\n\n\n\nCode\ndf.schema\n\n\nSchema([('url', String),\n        ('address', String),\n        ('name', String),\n        ('online_order', Boolean),\n        ('book_table', Boolean),\n        ('rate', String),\n        ('votes', Int64),\n        ('phone', String),\n        ('location', String),\n        ('rest_type', String),\n        ('dish_liked', String),\n        ('cuisines', String),\n        ('approx_cost(for two people)', String),\n        ('reviews_list', String),\n        ('menu_item', String),\n        ('listed_in(type)', String),\n        ('listed_in(city)', String)])"
  },
  {
    "objectID": "posts/python/spatial_analysis.html#extract-coordinates-from-data",
    "href": "posts/python/spatial_analysis.html#extract-coordinates-from-data",
    "title": "Geospatial Analysis in Python",
    "section": "Extract coordinates from data",
    "text": "Extract coordinates from data\nfirst we will learn how to extract Latitudes & longitudes using ‘location’ feature\n\n\nCode\nrest_loc = pl.DataFrame()\n\n\n\n\nCode\nrest_loc = pl.DataFrame({'name': df.select('location').unique()})\n\n\n\n\nCode\nrest_loc.sample(5).to_dicts()\n\n\n[{'name': 'Jakkur, Bangalore, Karnataka, India'},\n {'name': 'Kalyan Nagar, Bangalore, Karnataka, India'},\n {'name': 'RT Nagar, Bangalore, Karnataka, India'},\n {'name': 'Koramangala 7th Block, Bangalore, Karnataka, India'},\n {'name': 'Kaggadasapura, Bangalore, Karnataka, India'}]\n\n\n\n\nCode\n# Nominatim is a tool to search OpenStreetMap data by address or location\nfrom geopy.geocoders import Nominatim\n\n\n\n\nCode\ngeolocator = Nominatim(user_agent='app', timeout=None)\n\n\n\n\nCode\nlat = [] # define lat list to store all the latitudes\nlon = [] # define lon list to store all the longitudes\n\nfor name in pl.Series(rest_loc.select('name')):\n    location = geolocator.geocode(name)\n    \n    if location is None:\n        lat.append(np.nan)\n        lon.append(np.nan)\n        \n    else:\n        lat.append(location.latitude)\n        lon.append(location.longitude)\n\n\n\n\nCode\nlat[:10]\n\n\n[13.0621474,\n 12.9846713,\n 12.981015523680384,\n 12.985098650000001,\n 12.9096941,\n nan,\n 12.9067683,\n 12.938455602031697,\n 12.9176571,\n 12.9489339]\n\n\n\n\nCode\nrest_loc = rest_loc.with_columns(\n    lat=pl.Series(lat), # For python lists, construct a Series\n    lon=pl.Series(lon),\n)\n\n\n\n\nCode\n(\n    GT(rest_loc.head(5), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Coordinates')\n    )\n    .fmt_number(columns=['lat','lon'], decimals=4, use_seps=False)\n    .cols_width(\n        cases={'name':'200%',\n               'lat':'90%',\n               'lon':'90%',\n              }\n               )\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Coordinates\n\n\nname\nlat\nlon\n\n\n\n\nSahakara Nagar, Bangalore, Karnataka, India\n13.0621\n77.5801\n\n\nKaggadasapura, Bangalore, Karnataka, India\n12.9847\n77.6791\n\n\nInfantry Road, Bangalore, Karnataka, India\n12.9810\n77.6021\n\n\nCV Raman Nagar, Bangalore, Karnataka, India\n12.9851\n77.6631\n\n\nJP Nagar, Bangalore, Karnataka, India\n12.9097\n77.5866\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable 2: Zomato restaurants coordinates from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nWe have found out latitude and longitude of each location listed in the dataset using geopy This is used to plot maps.\n\n\nCode\npl.Series(rest_loc.select('lat')).is_null().sum()\n\n\n0\n\n\n\n\nCode\npl.Series(rest_loc.select('lat')).is_nan().sum()\n\n\n2\n\n\n\n\nCode\nrest_loc.filter(pl.col('lat').is_nan())\n\n\n\nshape: (2, 3)\n\n\n\nname\nlat\nlon\n\n\nstr\nf64\nf64\n\n\n\n\n\"Sadashiv Nagar, Bangalore, Kar…\nNaN\nNaN\n\n\n\"Rammurthy Nagar, Bangalore, Ka…\nNaN\nNaN\n\n\n\n\n\n\n\n\nCode\nrest_loc = rest_loc.drop_nans()"
  },
  {
    "objectID": "posts/python/spatial_analysis.html#where-are-most-number-of-restaurants-located-in-bengalore",
    "href": "posts/python/spatial_analysis.html#where-are-most-number-of-restaurants-located-in-bengalore",
    "title": "Geospatial Analysis in Python",
    "section": "Where are most number of restaurants located in Bengalore?",
    "text": "Where are most number of restaurants located in Bengalore?\n\n\nCode\nrest_locations = pl.Series(df.select('location')).value_counts(sort=True, name='total')\n\n\n\n\nCode\nrest_locations = rest_locations.rename({'location':'name', 'total':'count'})\n\n\n\n\nCode\n(\n    GT(rest_locations.head(), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Count')\n    )\n    .cols_width(cases={'name': '200%',})\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Count\n\n\nname\ncount\n\n\n\n\nBTM, Bangalore, Karnataka, India\n5124\n\n\nHSR, Bangalore, Karnataka, India\n2523\n\n\nKoramangala 5th Block, Bangalore, Karnataka, India\n2504\n\n\nJP Nagar, Bangalore, Karnataka, India\n2235\n\n\nWhitefield, Bangalore, Karnataka, India\n2144\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable 3: Zomato restaurants count from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nNow we can say that these are locations where most of restaurants are located.\nLets create Heatmap of this results so that it becomes more user-friendly.\nNow, in order to perform spatial analysis, we need latitudes & longitudes of every location, so lets merge both dataframes in order to get geographical co-ordinates.\n\n\nCode\nbeng_rest_locations = rest_locations.join(rest_loc, on='name')\n\n\n\n\nCode\n(\n    GT(beng_rest_locations.head(), auto_align=True)\n    .tab_header(\n        title=md('Zomato Restaurants Count & coordinates')\n    )\n    .cols_width(cases={'name': '200%',})\n    .tab_source_note(source_note=md('&lt;br&gt;*Source: Shan Singh*'))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZomato Restaurants Count & coordinates\n\n\nname\ncount\nlat\nlon\n\n\n\n\nBTM, Bangalore, Karnataka, India\n5124\n12.9163603\n77.604733\n\n\nHSR, Bangalore, Karnataka, India\n2523\n12.90056335\n77.64947470503677\n\n\nKoramangala 5th Block, Bangalore, Karnataka, India\n2504\n12.9348429\n77.6189768\n\n\nJP Nagar, Bangalore, Karnataka, India\n2235\n12.9096941\n77.5866067\n\n\nWhitefield, Bangalore, Karnataka, India\n2144\n12.9696365\n77.7497448\n\n\n\n\nSource: Shan Singh\n\n\n\n\n\n\n\n\n\n\nTable 4: Zomato restaurants count and coordinates from Singh, S (2024) Geospatial Data Science in Python\n\n\n\n\nnow in order to show-case it via Map(Heatmap) ,first we need to create BaseMap so that I can map our Heatmap on top of BaseMap !\n\n\nCode\ndef Generate_basemap():\n    basemap = folium.Map(location=[12.97 , 77.59], zoom_start=11)\n    return basemap\n\n\n\n\nCode\n# Geographic heat maps are used to identify where something occurs, and demonstrate areas of high and low density...\nfrom folium.plugins import HeatMap\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nbeng_rest_locations = beng_rest_locations.to_pandas()\n\n\n\n\nCode\nHeatMap(beng_rest_locations[['lat', 'lon' , 'count']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x3058e0da0&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 2: Zomato Restaurants Heatmap\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out.\n\n\nMajority of the Restaurants are avaiable in the city centre area."
  },
  {
    "objectID": "posts/python/spatial_analysis.html#performing-marker-cluster-analysis",
    "href": "posts/python/spatial_analysis.html#performing-marker-cluster-analysis",
    "title": "Geospatial Analysis in Python",
    "section": "Performing Marker Cluster Analysis",
    "text": "Performing Marker Cluster Analysis\n\n\nCode\nfrom folium.plugins import FastMarkerCluster\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nFastMarkerCluster(beng_rest_locations[['lat', 'lon' , 'count']]).add_to(basemap)\n\n\n&lt;folium.plugins.fast_marker_cluster.FastMarkerCluster at 0x30a2cd280&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 3: Zomato Marker Cluster Map\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out."
  },
  {
    "objectID": "posts/python/spatial_analysis.html#mapping-all-the-markers-of-places-of-bangalore",
    "href": "posts/python/spatial_analysis.html#mapping-all-the-markers-of-places-of-bangalore",
    "title": "Geospatial Analysis in Python",
    "section": "Mapping all the markers of places of Bangalore",
    "text": "Mapping all the markers of places of Bangalore\nPlotting Markers on the Map :\nFolium gives a folium.Marker() class for plotting markers on a map\nJust pass the latitude and longitude of the location, mention the popup and tooltip and add it to the map.\nPlotting markers is a two-step process.\n\nyou need to create a base map on which your markers will be placed\nand then add your markers to it:\n\n\n\nCode\nm = Generate_basemap()\n\n\n\n\nCode\n# Add points to the map\nfor index, row in beng_rest_locations.iterrows():\n    folium.Marker(location=[row['lat'], row['lon']], popup=row['count']).add_to(m)\n\n\n\n\nCode\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 4: Zomato Restaurants Marker Map\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out.\n\n\nRate field cleaning\nIn order to Analyse where are the restaurants situated with high average rate, first we need to clean ‘rate’ feature\n\n\nCode\n(\n    df.filter(\n        pl.col('rate').str.contains('^([^0-9]*)$')\n    )\n    .select('rate')\n    .unique()\n    .to_dicts()\n)\n\n\n[{'rate': '-'}, {'rate': 'NEW'}]\n\n\n\n\nCode\npl.Series(df.select('rate')).is_null().sum()\n\n\n7754\n\n\n\n\nCode\n# approximately 15% of your rating belongs to missing values\npl.Series(df.select('rate')).is_null().sum()/pl.Series(df.select('rate')).len()*100\n\n\n14.999226245744351\n\n\n\n\nCode\ndf = (\n    df.drop_nulls(subset='rate')\n        .with_columns(\n            pl.col('rate').replace(['NEW', '-',], ['0', '0'])\n        )\n        .with_columns(\n            rating=pl.col('rate').str.replace('/5', '')\n        )\n        .with_columns(\n            pl.col('rating').str.strip_chars()\n        )\n        .cast({'rating': pl.Float32})\n)\n\n\n\n\nCode\ndf.select('rating').unique().to_dicts()\n\n\n[{'rating': 2.4000000953674316},\n {'rating': 2.299999952316284},\n {'rating': 0.0},\n {'rating': 3.5},\n {'rating': 4.099999904632568},\n {'rating': 4.400000095367432},\n {'rating': 4.699999809265137},\n {'rating': 4.900000095367432},\n {'rating': 2.700000047683716},\n {'rating': 3.9000000953674316},\n {'rating': 3.799999952316284},\n {'rating': 3.4000000953674316},\n {'rating': 3.0},\n {'rating': 2.5999999046325684},\n {'rating': 3.299999952316284},\n {'rating': 4.199999809265137},\n {'rating': 2.200000047683716},\n {'rating': 4.0},\n {'rating': 4.5},\n {'rating': 2.5},\n {'rating': 3.5999999046325684},\n {'rating': 3.700000047683716},\n {'rating': 2.0999999046325684},\n {'rating': 4.800000190734863},\n {'rating': 3.200000047683716},\n {'rating': 2.799999952316284},\n {'rating': 4.300000190734863},\n {'rating': 2.9000000953674316},\n {'rating': 2.0},\n {'rating': 4.599999904632568},\n {'rating': 3.0999999046325684},\n {'rating': 1.7999999523162842}]"
  },
  {
    "objectID": "posts/python/spatial_analysis.html#most-highest-rated-restaurants",
    "href": "posts/python/spatial_analysis.html#most-highest-rated-restaurants",
    "title": "Geospatial Analysis in Python",
    "section": "Most highest rated restaurants",
    "text": "Most highest rated restaurants\n\n\nCode\ndf.select('name','rate','votes','location','dish_liked','rating').sort('rating', descending=True).head()\n\n\n\nshape: (5, 6)\n\n\n\nname\nrate\nvotes\nlocation\ndish_liked\nrating\n\n\nstr\nstr\ni64\nstr\nstr\nf32\n\n\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn…\n\"Cocktails, Dahi Kebab, Rajma C…\n4.9\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn…\n\"Cocktails, Dahi Kebab, Rajma C…\n4.9\n\n\n\"Byg Brewski Brewing Company\"\n\"4.9/5\"\n16345\n\"Sarjapur Road, Bangalore, Karn…\n\"Cocktails, Dahi Kebab, Rajma C…\n4.9\n\n\n\"Belgian Waffle Factory\"\n\"4.9/5\"\n1746\n\"Brigade Road, Bangalore, Karna…\n\"Coffee, Berryblast, Nachos, Ch…\n4.9\n\n\n\"Belgian Waffle Factory\"\n\"4.9/5\"\n1746\n\"Brigade Road, Bangalore, Karna…\n\"Coffee, Berryblast, Nachos, Ch…\n4.9\n\n\n\n\n\n\n\n\nCode\ngrp_df = (\n    df.group_by('location').agg(pl.col('rating').mean(), pl.col('name').count())\n        .rename({'location':'name', 'rating':'avg_rating', 'name':'count'})\n)\n\n\n\n\nCode\ngrp_df\n\n\n\nshape: (92, 3)\n\n\n\nname\navg_rating\ncount\n\n\nstr\nf32\nu32\n\n\n\n\n\"Brookefield, Bangalore, Karnat…\n3.374697\n581\n\n\n\"Thippasandra, Bangalore, Karna…\n3.095396\n152\n\n\n\"Electronic City, Bangalore, Ka…\n3.04191\n964\n\n\n\"Koramangala 1st Block, Bangalo…\n3.263946\n965\n\n\n\"Koramangala 3rd Block, Bangalo…\n3.978755\n193\n\n\n…\n…\n…\n\n\n\"RT Nagar, Bangalore, Karnataka…\n3.278125\n64\n\n\n\"Jalahalli, Bangalore, Karnatak…\n3.486956\n23\n\n\n\"Commercial Street, Bangalore, …\n3.109709\n309\n\n\n\"Banaswadi, Bangalore, Karnatak…\n3.362927\n499\n\n\n\"Koramangala 5th Block, Bangalo…\n3.901511\n2381\n\n\n\n\n\n\nlets consider only those restaurants who have send atleast 400 orders\n\n\nCode\ntemp_df = grp_df.filter(pl.col('count')&gt;400)\n\n\n\n\nCode\ntemp_df.shape\n\n\n(35, 3)\n\n\n\n\nCode\ntemp_df\n\n\n\nshape: (35, 3)\n\n\n\nname\navg_rating\ncount\n\n\nstr\nf32\nu32\n\n\n\n\n\"Brookefield, Bangalore, Karnat…\n3.374697\n581\n\n\n\"Electronic City, Bangalore, Ka…\n3.04191\n964\n\n\n\"Koramangala 1st Block, Bangalo…\n3.263946\n965\n\n\n\"Bannerghatta Road, Bangalore, …\n3.271675\n1324\n\n\n\"HSR, Bangalore, Karnataka, Ind…\n3.484063\n2128\n\n\n…\n…\n…\n\n\n\"Richmond Road, Bangalore, Karn…\n3.688013\n634\n\n\n\"Koramangala 7th Block, Bangalo…\n3.747846\n1089\n\n\n\"Frazer Town, Bangalore, Karnat…\n3.56488\n578\n\n\n\"Banaswadi, Bangalore, Karnatak…\n3.362927\n499\n\n\n\"Koramangala 5th Block, Bangalo…\n3.901511\n2381\n\n\n\n\n\n\n\n\nCode\nrest_loc\n\n\n\nshape: (91, 3)\n\n\n\nname\nlat\nlon\n\n\nstr\nf64\nf64\n\n\n\n\n\"Sahakara Nagar, Bangalore, Kar…\n13.062147\n77.580061\n\n\n\"Kaggadasapura, Bangalore, Karn…\n12.984671\n77.679091\n\n\n\"Infantry Road, Bangalore, Karn…\n12.981016\n77.602133\n\n\n\"CV Raman Nagar, Bangalore, Kar…\n12.985099\n77.663117\n\n\n\"JP Nagar, Bangalore, Karnataka…\n12.909694\n77.586607\n\n\n…\n…\n…\n\n\n\"Seshadripuram, Bangalore, Karn…\n12.993188\n77.575342\n\n\n\"Jakkur, Bangalore, Karnataka, …\n13.078474\n77.606894\n\n\n\"Bommanahalli, Bangalore, Karna…\n12.908945\n77.623904\n\n\n\"Kammanahalli, Bangalore, Karna…\n13.009346\n77.637709\n\n\n\"Nagawara, Bangalore, Karnataka…\n13.042279\n77.624858\n\n\n\n\n\n\nlets merge both the dataframe so that we can get coordinates as well\n\n\nCode\nratings_locations = temp_df.join(rest_loc, on='name')\n\n\n\n\nCode\nratings_locations\n\n\n\nshape: (35, 5)\n\n\n\nname\navg_rating\ncount\nlat\nlon\n\n\nstr\nf32\nu32\nf64\nf64\n\n\n\n\n\"JP Nagar, Bangalore, Karnataka…\n3.412929\n1849\n12.909694\n77.586607\n\n\n\"Koramangala 4th Block, Bangalo…\n3.814351\n864\n12.932778\n77.629405\n\n\n\"Whitefield, Bangalore, Karnata…\n3.384171\n1693\n12.969637\n77.749745\n\n\n\"Bannerghatta Road, Bangalore, …\n3.271675\n1324\n12.951856\n77.604011\n\n\n\"Jayanagar, Bangalore, Karnatak…\n3.61525\n1718\n12.939904\n77.582638\n\n\n…\n…\n…\n…\n…\n\n\n\"Ulsoor, Bangalore, Karnataka, …\n3.541396\n901\n12.977879\n77.62467\n\n\n\"Frazer Town, Bangalore, Karnat…\n3.56488\n578\n12.998683\n77.615525\n\n\n\"Indiranagar, Bangalore, Karnat…\n3.652168\n1936\n12.996298\n77.545278\n\n\n\"Koramangala 6th Block, Bangalo…\n3.662465\n1111\n12.939025\n77.623848\n\n\n\"Kammanahalli, Bangalore, Karna…\n3.499809\n525\n13.009346\n77.637709\n\n\n\n\n\n\n\n\nCode\nbasemap = Generate_basemap()\n\n\n\n\nCode\nratings_locations = ratings_locations.to_pandas()\n\n\n\n\nCode\nHeatMap(ratings_locations[['lat', 'lon' , 'avg_rating']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x30a39bcb0&gt;\n\n\n\n\nCode\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 5: Highest-rated Zomato Restaurants Heatmap\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can interact with the above map by zooming in or out."
  },
  {
    "objectID": "posts/python/spatial_analysis.html#conclusions",
    "href": "posts/python/spatial_analysis.html#conclusions",
    "title": "Geospatial Analysis in Python",
    "section": "Conclusions",
    "text": "Conclusions\nPython, with its powerful libraries and ease of use, has become an indispensable tool for geospatial analysis. By leveraging the capabilities of libraries like GeoPandas, Shapely, and folium, data scientists can effectively explore and analyze geospatial data, gain valuable insights, and make informed decisions.\nIn this article, we have shown a brief overview of geospatial analysis in Python."
  },
  {
    "objectID": "posts/python/spatial_analysis.html#references",
    "href": "posts/python/spatial_analysis.html#references",
    "title": "Geospatial Analysis in Python",
    "section": "References",
    "text": "References\n\nSingh, S (2024) Spatial Analysis & Geospatial Data Science in Python\nTenkanen, H et al (2022) Introduction to Python for Geographic Data Analysis"
  },
  {
    "objectID": "posts/python/spatial_analysis.html#contact",
    "href": "posts/python/spatial_analysis.html#contact",
    "title": "Geospatial Analysis in Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/pyspark example.html",
    "href": "posts/python/pyspark example.html",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "",
    "text": "Figure 1: Pyspark Logo"
  },
  {
    "objectID": "posts/python/pyspark example.html#conclusions",
    "href": "posts/python/pyspark example.html#conclusions",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "Conclusions",
    "text": "Conclusions\nPySpark is a powerful and versatile tool for working with big data. Its combination of Spark’s distributed computing capabilities and Python’s ease of use makes it an excellent choice for data scientists and analysts looking to tackle large-scale data processing tasks.\nWhether you’re performing complex data transformations, building machine learning models, or analyzing real-time streams, PySpark can help you unlock the potential of your data. So, dive in, explore its features, and unleash the power of big data!"
  },
  {
    "objectID": "posts/python/pyspark example.html#references",
    "href": "posts/python/pyspark example.html#references",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "References",
    "text": "References\n\nOfficial PySpark Documentation\nGetting Started Guide\nTutorials & Examples"
  },
  {
    "objectID": "posts/python/pyspark example.html#contact",
    "href": "posts/python/pyspark example.html#contact",
    "title": "Unleashing the Power of Big Data with PySpark",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/mexico_pol_scenario.html#mexicos-political-scenario",
    "href": "posts/python/mexico_pol_scenario.html#mexicos-political-scenario",
    "title": "Mexico’s Political Landscape",
    "section": "Mexico’s Political Scenario",
    "text": "Mexico’s Political Scenario\nA complex mix of democratic strides and ongoing hurdles\n\nElectoral Democracy\n\nSince 2000, Mexico has functioned as an electoral democracy with regular power transitions between parties, both nationally and regionally.\n\nMulti-Party System\n\nMexico boasts a vibrant multi-party system, with several parties competing for power, but the current dominant force is the National Regeneration Movement (MORENA).\n\nChallenges\n\nRule of Law\n\nDeficiencies in the rule of law weaken citizen confidence in political institutions. Issues like corruption, violence by criminal groups, and limited accountability create a complex environment.\n\nPublic Security\n\nDrug trafficking and organized crime pose a major threat to public safety and stability. The government’s strategies to combat these issues are a source of ongoing debate.\n\nMilitary’s Role\n\nThe expanding role of the military in public security raises concerns about potential human rights abuses and a shift away from civilian control.\n\n\n\n\n\n\nFigure 1: Governors of States in Mexico\n\n\n\n\n\n\n\n\n\nNoteTip\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "posts/python/mexico_pol_scenario.html#mexico-citys-political-landscape",
    "href": "posts/python/mexico_pol_scenario.html#mexico-citys-political-landscape",
    "title": "Mexico’s Political Landscape",
    "section": "Mexico City’s Political Landscape",
    "text": "Mexico City’s Political Landscape\nMexico City’s political scene is fascinating due to its unique position.\n\nNational Influence\n\nAs the capital, Mexico City is the hub of national politics. Federal government buildings are located here, and residents elect representatives to the national Congress. This creates a tight link between the local and the national.\n\nShifting Power\n\nHistorically, the mayor was appointed by the president. Since 1997, residents directly elect their leader for a six-year term.\n\nRecent Elections (2021)\n\nThe MORENA party solidified its power in many states, including Mexico City. However, opposition parties gained control of most city boroughs, chipping away at MORENA’s local dominance.\n\nActive Citizenry\n\nMexico City residents have a strong voice due to their large numbers and ability to protest. This makes them a powerful force that politicians need to consider.\n\nChallenges\n\nLike other parts of Mexico, concerns exist about democratic processes and security during protests. Violence against women’s rights demonstrations is a recent point of tension.\nMexico City’s political situation is dynamic and reflects the national landscape. The interplay between federal and local power, recent election results, and an engaged citizenry all contribute to a complex and interesting political environment.\n\n\n\n\n\n\nFigure 2: Mayors of Mexico City\n\n\n\n\n\n\n\n\n\nNoteTip\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "posts/python/mexico_pol_scenario.html#conclusions",
    "href": "posts/python/mexico_pol_scenario.html#conclusions",
    "title": "Mexico’s Political Landscape",
    "section": "Conclusions",
    "text": "Conclusions\nMexico’s political future hinges on its ability to address these challenges.\nStrengthening the rule of law, tackling corruption, and finding effective solutions for security issues will be crucial for a more stable and prosperous democracy."
  },
  {
    "objectID": "posts/python/mexico_pol_scenario.html#contact",
    "href": "posts/python/mexico_pol_scenario.html#contact",
    "title": "Mexico’s Political Landscape",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/dw_motherduck.html",
    "href": "posts/python/dw_motherduck.html",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "",
    "text": "Figure 1: Data Warehouse by Niklas Lang"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#extraction-from-data-sources",
    "href": "posts/python/dw_motherduck.html#extraction-from-data-sources",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Extraction from data sources",
    "text": "Extraction from data sources\n\ncsv files\n\n\nCode\ncsv_files = glob.glob('./datasets/*.csv')\n\n\n\n\nCode\nlist(enumerate(csv_files))\n\n\n[(0, './datasets/watercollection.csv'),\n (1, './datasets/ContainsNull.csv'),\n (2, './datasets/sales_info.csv'),\n (3, './datasets/cdmx-subway.csv'),\n (4, './datasets/airports.csv'),\n (5, './datasets/colors.csv'),\n (6, './datasets/sets.csv'),\n (7, './datasets/appl_stock.csv'),\n (8, './datasets/sales.csv')]\n\n\n\n\njson files\n\n\nCode\njson_files = glob.glob('./datasets/*.json')\n\n\n\n\nCode\nlist(enumerate(json_files))\n\n\n[(0, './datasets/prevalencia.json'), (1, './datasets/people.json')]\n\n\n\n\ndatabase tables\n\n\nCode\ndb_files = glob.glob('datasets/*.db')\n\n\n\n\nCode\nlist(enumerate(db_files))\n\n\n[(0, 'datasets/retail_db.db'), (1, 'datasets/restaurants.db')]"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#data-warehouse-creation",
    "href": "posts/python/dw_motherduck.html#data-warehouse-creation",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data warehouse creation",
    "text": "Data warehouse creation\n\n\nCode\nconn = db.connect('my_database.db')"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#data-warehouse-load",
    "href": "posts/python/dw_motherduck.html#data-warehouse-load",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data warehouse load",
    "text": "Data warehouse load\n\n\nCode\nconn.sql(f\"create or replace table water_collection as select * from '{csv_files[0]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table contains_null as select * from '{csv_files[1]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sales_info as select * from '{csv_files[2]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table cdmx_subway as select * from '{csv_files[3]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table airports as select * from '{csv_files[4]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table colors as select * from '{csv_files[5]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sets as select * from '{csv_files[6]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table appl_stock as select * from '{csv_files[7]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table sales as select * from '{csv_files[8]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table prevalencia as select * from '{json_files[0]}' \")\n\n\n\n\nCode\nconn.sql(f\"create or replace table people as select * from '{json_files[1]}' \")\n\n\n\n\nCode\nretail = db.connect('./datasets/retail_db.db')\nretail.sql('show tables')\n\n\n┌──────────────┐\n│     name     │\n│   varchar    │\n├──────────────┤\n│ retail_sales │\n└──────────────┘\n\n\n\n\nCode\nretail_sales_pl = retail.sql('select * from retail_sales').pl()\n\n\n\n\nCode\nconn.execute(\"create or replace table retail_sales as from retail_sales_pl\");\n\n\n\n\nCode\nrestaurants = db.connect('./datasets/restaurants.db')\nrestaurants.sql('show tables')\n\n\n┌─────────────┐\n│    name     │\n│   varchar   │\n├─────────────┤\n│ restaurants │\n└─────────────┘\n\n\n\n\nCode\nrestaurants_pl = restaurants.sql('select * from restaurants').pl()\n\n\n\n\nCode\nconn.execute(\"create or replace table restaurants as from restaurants_pl\");"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#data-retrieval",
    "href": "posts/python/dw_motherduck.html#data-retrieval",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Data retrieval",
    "text": "Data retrieval\n\n\nCode\nconn.sql('show databases')\n\n\n┌───────────────┐\n│ database_name │\n│    varchar    │\n├───────────────┤\n│ my_database   │\n└───────────────┘\n\n\n\n\nCode\nconn.sql('show tables')\n\n\n┌──────────────────┐\n│       name       │\n│     varchar      │\n├──────────────────┤\n│ airports         │\n│ appl_stock       │\n│ cdmx_subway      │\n│ colors           │\n│ contains_null    │\n│ people           │\n│ prevalencia      │\n│ restaurants      │\n│ retail_sales     │\n│ sales            │\n│ sales_info       │\n│ sets             │\n│ water_collection │\n├──────────────────┤\n│     13 rows      │\n└──────────────────┘\n\n\n\n\nCode\nconn.sql('select * from restaurants limit 5').pl()\n\n\n\nshape: (5, 6)\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\nstr\ni64\nf64\nstr\nstr\ni64\n\n\n\n\n\"The Golden Wok\"\n1477\n33.620488\n\"Berlin\"\n\"American\"\n5\n\n\n\"Greek Gyros\"\n770\n68.388874\n\"New York\"\n\"French\"\n1\n\n\n\"Taste of Italy\"\n4420\n88.23168\n\"Amsterdam\"\n\"Chinese\"\n0\n\n\n\"Midnight Diner\"\n2155\n12.965985\n\"Lisbon\"\n\"Mexican\"\n1\n\n\n\"Taste of Italy\"\n3375\n52.785226\n\"Sydney\"\n\"Chinese\"\n1"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#cloud-data-warehouse-with-motherduck",
    "href": "posts/python/dw_motherduck.html#cloud-data-warehouse-with-motherduck",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Cloud Data Warehouse with Motherduck",
    "text": "Cloud Data Warehouse with Motherduck\n\n\nCode\ndw = db.connect('md')\n\n\n\n\nCode\ndw.sql('select current_database()').show()\n\n\n┌────────────────────┐\n│ current_database() │\n│      varchar       │\n├────────────────────┤\n│ my_portfolio       │\n└────────────────────┘"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#convert-queries-from-local-database-to-polars",
    "href": "posts/python/dw_motherduck.html#convert-queries-from-local-database-to-polars",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Convert queries from local database to polars",
    "text": "Convert queries from local database to polars\n\n\nCode\nairports_pl = conn.sql('select * from airports').pl()\nappl_stock_pl = conn.sql('select * from appl_stock').pl()\ncdmx_subway_pl = conn.sql('select * from cdmx_subway').pl()\ncolors_pl = conn.sql('select * from colors').pl()\ncontains_null_pl = conn.sql('select * from contains_null').pl()\npeople_pl = conn.sql('select * from people').pl()\nprevalencia_pl = conn.sql('select * from prevalencia').pl()\nrestaurants_pl = conn.sql('select * from restaurants').pl()\nretail_sales_pl = conn.sql('select * from retail_sales').pl()\nsales_pl = conn.sql('select * from sales').pl()\nsales_info_pl = conn.sql('select * from sales_info').pl()\nsets_pl = conn.sql('select * from sets').pl()\nwater_collection_pl = conn.sql('select * from water_collection').pl()"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#upload-dataframes-to-motherduck",
    "href": "posts/python/dw_motherduck.html#upload-dataframes-to-motherduck",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Upload dataframes to MotherDuck",
    "text": "Upload dataframes to MotherDuck\n\n\nCode\ndw.sql(f\"create or replace table airports as select * from airports_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table appl_stock as select * from appl_stock_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table cdmx_subway as select * from cdmx_subway_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table colors as select * from colors_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table contains_null as select * from contains_null_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table people as select * from people_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table prevalencia as select * from prevalencia_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table restaurants as select * from restaurants_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table retail_sales as select * from retail_sales_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sales as select * from sales_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sales_info as select * from sales_info_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table sets as select * from sets_pl\");\n\n\n\n\nCode\ndw.sql(f\"create or replace table water_collection as select * from water_collection_pl\");"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#check-uploaded-tables",
    "href": "posts/python/dw_motherduck.html#check-uploaded-tables",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Check uploaded tables",
    "text": "Check uploaded tables\n\n\nCode\ndw.sql('show tables')\n\n\n┌──────────────────┐\n│       name       │\n│     varchar      │\n├──────────────────┤\n│ airports         │\n│ appl_stock       │\n│ cdmx_subway      │\n│ colors           │\n│ contains_null    │\n│ people           │\n│ prevalencia      │\n│ restaurants      │\n│ retail_sales     │\n│ sales            │\n│ sales_info       │\n│ sets             │\n│ water_collection │\n├──────────────────┤\n│     13 rows      │\n└──────────────────┘"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#close-all-database-connections",
    "href": "posts/python/dw_motherduck.html#close-all-database-connections",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Close all database connections",
    "text": "Close all database connections\n\n\nCode\n# close db connections\nconn.close()\nretail.close()\nrestaurants.close()\ndw.close()"
  },
  {
    "objectID": "posts/python/dw_motherduck.html#contact",
    "href": "posts/python/dw_motherduck.html#contact",
    "title": "Implementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/cohort_analysis.html",
    "href": "posts/python/cohort_analysis.html",
    "title": "Cohort Analysis",
    "section": "",
    "text": "Cohort analysis is a type of analysis that follows a group of customers (cohorts) over time to understand their behavior, based on a characteristic that identifies them during a period of time. Periods could be a day, week, month, or even a year. Cohort analysis takes into account time series data over the lifespan of a user to predict and analyze their future consumption patterns.\nCohorts are commonly created in accordance with acquisition date shared by a group of users. An acquisition cohort refers to a group of users who were newly acquired (e.g account creation, first purchase) by a business. On the other hand, users are assigned to behavioural cohorts when they perform a specific action during a time frame.\nBy analysing each cohort in isolation, behavioural differences between groups can be identified. These observations can then be used to inform marketing and product development decisions.\nCohort analyses are important because churn is one of the main problems faced by companies nowadays, and every information that can lead to less churning is a valuable business information.\nSpecifically, cohort analysis can be used to:\nMeasure Customer Lifetime Value (CLV)\nCohort analysis can be used to identify patterns in retention & monetary value of different user groups. By identifying valuable groups, strategies can be developed to replicate their characteristics among other groups.\nEvaluate Marketing Campaigns\nMonitoring how groups respond to different campaigns can help reveal the true effectiveness of marketing. For example, a big discount promotion may drive high levels of acquisition, but these users may have little interest in full price goods — making them likely to churn. Over time, it may be observed that CLV is higher for campaigns that drove lower levels of acquisition.\nImprove Product Development\nAssigning users to cohorts, based on the actions they have taken within a product, can provide insights into the features that promote increases in user retention and engagement.\ne-commerce\nCustomers who have started using the service just two or three days ago or had just made their first purchase can be grouped into one group or cohort whereas people who have been on the website for 1 or 2 years may be grouped into another cohort and their purchase activity over the year can be used to predict how many times they will visit the website in the future or how many times they will actually click on a product and purchase it. This can be used to personalize their own home feed so that the click-through rate increases and their retention for that particular website don’t subside.\nWebsites\nCohort analysis can be used to predict viewer retention when a new person logs in for the first time vs a person who has been on the platform for a long time. To keep the new user on the platform, their home feed will be personalized with the most popular videos that have a huge amount of views, clickbait thumbnails, or videos with popular search terms based on that particular user’s location data whereas a long-time user will mostly receive video updates from their subscriptions and their most watched channels."
  },
  {
    "objectID": "posts/python/cohort_analysis.html#what-is-cohort-analysis",
    "href": "posts/python/cohort_analysis.html#what-is-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "",
    "text": "Cohort analysis is a type of analysis that follows a group of customers (cohorts) over time to understand their behavior, based on a characteristic that identifies them during a period of time. Periods could be a day, week, month, or even a year. Cohort analysis takes into account time series data over the lifespan of a user to predict and analyze their future consumption patterns.\nCohorts are commonly created in accordance with acquisition date shared by a group of users. An acquisition cohort refers to a group of users who were newly acquired (e.g account creation, first purchase) by a business. On the other hand, users are assigned to behavioural cohorts when they perform a specific action during a time frame.\nBy analysing each cohort in isolation, behavioural differences between groups can be identified. These observations can then be used to inform marketing and product development decisions.\nCohort analyses are important because churn is one of the main problems faced by companies nowadays, and every information that can lead to less churning is a valuable business information.\nSpecifically, cohort analysis can be used to:\nMeasure Customer Lifetime Value (CLV)\nCohort analysis can be used to identify patterns in retention & monetary value of different user groups. By identifying valuable groups, strategies can be developed to replicate their characteristics among other groups.\nEvaluate Marketing Campaigns\nMonitoring how groups respond to different campaigns can help reveal the true effectiveness of marketing. For example, a big discount promotion may drive high levels of acquisition, but these users may have little interest in full price goods — making them likely to churn. Over time, it may be observed that CLV is higher for campaigns that drove lower levels of acquisition.\nImprove Product Development\nAssigning users to cohorts, based on the actions they have taken within a product, can provide insights into the features that promote increases in user retention and engagement.\ne-commerce\nCustomers who have started using the service just two or three days ago or had just made their first purchase can be grouped into one group or cohort whereas people who have been on the website for 1 or 2 years may be grouped into another cohort and their purchase activity over the year can be used to predict how many times they will visit the website in the future or how many times they will actually click on a product and purchase it. This can be used to personalize their own home feed so that the click-through rate increases and their retention for that particular website don’t subside.\nWebsites\nCohort analysis can be used to predict viewer retention when a new person logs in for the first time vs a person who has been on the platform for a long time. To keep the new user on the platform, their home feed will be personalized with the most popular videos that have a huge amount of views, clickbait thumbnails, or videos with popular search terms based on that particular user’s location data whereas a long-time user will mostly receive video updates from their subscriptions and their most watched channels."
  },
  {
    "objectID": "posts/python/cohort_analysis.html#steps-for-cohort-analysis",
    "href": "posts/python/cohort_analysis.html#steps-for-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "Steps for Cohort Analysis",
    "text": "Steps for Cohort Analysis\nThere are 05 main steps to performing cohort analysis. They are as follows:\n\nDetermining the main objective of the cohort analysis: First and foremost, we need to determine the main intent of performing the analysis, such as to analyze why people on YouTube don’t watch videos after the 6-minute mark. This sets the ultimate goal of the analysis and uses the huge pool of information for practical issues. This helps in pinpointing the root issue or cause and companies can then work towards improved business practices to provide a better user experience.\nDefining the metrics to respond to the question: The next step would be to identify what defines the problem. In simpler words, from the above example, we need to analyze when a viewer leaves a video or at what minute before moving on to something else, his/her watch time, and click-through rates on YouTube.\nIdentify the particular groups or cohorts that will be relevant: To analyze users we need to pick out a group of viewers who display common behavioral patterns. In order to do this we need to analyze data from different user inputs and identify relevant similarities and differences between them and then separate it into specified cohorts.\nPerforming the Cohort Analysis: Now we will use data visualization techniques to perform the cohort analysis based on the objective of the problem. This can be done using many programming languages out of which the preferred languages are python and R. Cohort analysis in python can be done using libraries such as NumPy and seaborn. Heat maps are usually used to display user retention and visualize data in a tabular form.\nTesting the Results: Last but not the least, the results need to be checked and tested in order to make sure that they can actually reduce company losses and optimize business practices. We will obtain retention rates from the analysis and a heatmap(or any other suitable graph) of user retention and retention rate will help us analyze and improve experiences for the users."
  },
  {
    "objectID": "posts/python/cohort_analysis.html#environment-setting",
    "href": "posts/python/cohort_analysis.html#environment-setting",
    "title": "Cohort Analysis",
    "section": "Environment Setting",
    "text": "Environment Setting\n\n\nShow code\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nShow code\n# read dataset from github\ndf = pd.read_csv('https://raw.githubusercontent.com/nickmancol/python-cohorts/main/data/scanner_data.csv')\n\n\n\n\nShow code\n# show dataframe subset\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\n\n\n\n\n0\n1\n02/01/2016\n2547\n1\nX52\n0EM7L\n1.0\n3.13\n\n\n1\n2\n02/01/2016\n822\n2\n2ML\n68BRQ\n1.0\n5.46\n\n\n2\n3\n02/01/2016\n3686\n3\n0H2\nCZUZX\n1.0\n6.35\n\n\n3\n4\n02/01/2016\n3719\n4\n0H2\n549KK\n1.0\n5.59\n\n\n4\n5\n02/01/2016\n9200\n5\n0H2\nK8EHH\n1.0\n6.88\n\n\n\n\n\n\n\n\n\nShow code\n# drop unnamed column\ndf = df.drop(['Unnamed: 0'], axis = 1)\n# convert column to datetime\ndf.Date = pd.to_datetime(df.Date, format='%d/%m/%Y')\n\n\n\n\nShow code\n# get descriptive stats\ndf.describe(include='number')\n\n\n\n\n\n\n\n\n\nCustomer_ID\nTransaction_ID\nQuantity\nSales_Amount\n\n\n\n\ncount\n131706.000000\n131706.000000\n131706.000000\n131706.000000\n\n\nmean\n12386.450367\n32389.604187\n1.485311\n11.981524\n\n\nstd\n6086.447552\n18709.901238\n3.872667\n19.359699\n\n\nmin\n1.000000\n1.000000\n0.010000\n0.020000\n\n\n25%\n7349.000000\n16134.000000\n1.000000\n4.230000\n\n\n50%\n13496.000000\n32620.000000\n1.000000\n6.920000\n\n\n75%\n17306.000000\n48548.000000\n1.000000\n12.330000\n\n\nmax\n22625.000000\n64682.000000\n400.000000\n707.730000\n\n\n\n\n\n\n\n\n\nShow code\n# count duplicates\ndf.duplicated([\"Date\",\"Customer_ID\"]).sum()\n\n\nnp.int64(68979)\n\n\n\n\nShow code\nprint(f'The dataset has {df.size:,.2f} rows.')\n\n\nThe dataset has 921,942.00 rows.\n\n\n\n\nShow code\n# Group by date and customer and sum quantity and sales selecting last items\ndf = pd.DataFrame(df.groupby([\"Date\",\"Customer_ID\"]).agg({'Transaction_ID':'max'\n                                                          ,'SKU_Category':'max'\n                                                          ,'SKU':'max'\n                                                          ,'Quantity':'sum'\n                                                          ,'Sales_Amount':'sum'})\n                 ).reset_index()\n\n\n\n\nShow code\nprint(f'The dataset now has {df.size:,.2f} rows.')\n\n\nThe dataset now has 439,089.00 rows."
  },
  {
    "objectID": "posts/python/cohort_analysis.html#data-preparation-for-cohort-analysis",
    "href": "posts/python/cohort_analysis.html#data-preparation-for-cohort-analysis",
    "title": "Cohort Analysis",
    "section": "Data Preparation for Cohort Analysis",
    "text": "Data Preparation for Cohort Analysis\nTo run a cohort analysis, we’ll need to:\n\nSplit the data into groups that can be analyzed on the basis of time\nAssign a cohort index for each transaction\nCreate two new columns by applying a lambda function to the date column in order to:\n\nCreate the tx_month column\nTransform tx_month to get the minimum value of tx_month per customer\nAssign tx_month per customer to the acq_month column\n\n\n\n\nShow code\n# create transaction month column with day 1\ndf['tx_month'] = df['Date'].apply(lambda x: dt.date(x.year, x.month,1))\n# create acquisition column based on minimum transaction month column by customer\ndf['acq_month'] = df.groupby('Customer_ID')['tx_month'].transform('min')\n# select rows where transaction dates differ from acquisition date\ndf.loc[df['tx_month'] != df['acq_month']].head()\n\n\n\n\n\n\n\n\n\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\ntx_month\nacq_month\n\n\n\n\n4701\n2016-02-01\n50\n5004\nW41\nZWFSY\n5.0\n2.86\n2016-02-01\n2016-01-01\n\n\n4702\n2016-02-01\n91\n4976\n2ML\n68BRQ\n1.0\n5.79\n2016-02-01\n2016-01-01\n\n\n4709\n2016-02-01\n366\n4852\nJ4R\nVGIW5\n2.0\n15.64\n2016-02-01\n2016-01-01\n\n\n4720\n2016-02-01\n889\n4900\nTEU\nA233P\n2.0\n11.87\n2016-02-01\n2016-01-01\n\n\n4723\n2016-02-01\n1115\n4933\nYMJ\nJNWFX\n1.0\n7.43\n2016-02-01\n2016-01-01\n\n\n\n\n\n\n\n\n\nShow code\n# Claculate the number of months elapsed between transaction and acquisition\ndef diff_month(x):\n    d1 = x['tx_month']\n    d2 = x[\"acq_month\"]\n    return ((d1.year - d2.year) * 12 + d1.month - d2.month)+1\n\n\n\n\nShow code\n# Store the number of months between transaction and acquisition month\ndf['cohort_idx'] = df.apply(lambda x: diff_month(x), axis=1)\n\n\n\n\nShow code\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nCustomer_ID\nTransaction_ID\nSKU_Category\nSKU\nQuantity\nSales_Amount\ntx_month\nacq_month\ncohort_idx\n\n\n\n\n0\n2016-01-02\n3\n90\nTW8\nY1M2E\n4.0\n10.92\n2016-01-01\n2016-01-01\n1\n\n\n1\n2016-01-02\n178\n84\nR6E\nHO1M5\n2.0\n58.99\n2016-01-01\n2016-01-01\n1\n\n\n2\n2016-01-02\n195\n107\nLGI\nVY2UB\n2.0\n13.10\n2016-01-01\n2016-01-01\n1\n\n\n3\n2016-01-02\n343\n134\nXG4\nZSVWE\n1.0\n6.75\n2016-01-01\n2016-01-01\n1\n\n\n4\n2016-01-02\n399\n136\nP42\nXJLWY\n2.0\n10.43\n2016-01-01\n2016-01-01\n1\n\n\n\n\n\n\n\n\n\nShow code\n# create cohort matrix by counting unique customers\ndef get_cohort_matrix(data, var='Customer_ID', fun=pd.Series.nunique):\n    # group by acquisition and cohort_id\n    cd = data.groupby(['acq_month', 'cohort_idx'])[var].apply(fun).reset_index()\n    # create pivot table with acquisition, cohort_id and counting customers\n    cc = cd.pivot_table(index = 'acq_month',\n                        columns = 'cohort_idx',\n                        values = var)\n    # calculate retention rate\n    # select first column of pivot table, i.e., first cohort_id\n    cs = cc.iloc[:,0]\n    # divide all values in the cohort matrix by the corresponding initial cohort_id size\n    retention = cc.divide(cs, axis = 0)\n    # create percentage and round decimals\n    retention = retention.round(3)\n    # return the matrix and retention rate for each cohort_id\n    return cc, retention\n\n\n\n\nShow code\ncc, retention = get_cohort_matrix(df)\n\n\n\n\nShow code\n# format index datetie\nretention.index = pd.to_datetime(retention.index).strftime('%b %Y')\n\n\nThe rows of the pivot table consist of the beginning of user activity or the month from which the user has started visiting the ecommerce website or has made the first purchase. The columns represent the user’s retention rate or how long has the user been coming back to purchase since his first time.\n\n\nShow code\nretention\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 2016\n1.0\n0.385\n0.302\n0.176\n0.078\n0.058\n0.045\n0.040\n0.027\n0.016\n0.012\n0.012\n\n\nFeb 2016\n1.0\n0.215\n0.131\n0.066\n0.043\n0.031\n0.031\n0.026\n0.010\n0.008\n0.006\nNaN\n\n\nMar 2016\n1.0\n0.282\n0.246\n0.227\n0.201\n0.197\n0.201\n0.194\n0.194\n0.201\nNaN\nNaN\n\n\nApr 2016\n1.0\n0.288\n0.250\n0.219\n0.224\n0.235\n0.224\n0.224\n0.230\nNaN\nNaN\nNaN\n\n\nMay 2016\n1.0\n0.238\n0.213\n0.192\n0.216\n0.210\n0.198\n0.199\nNaN\nNaN\nNaN\nNaN\n\n\nJun 2016\n1.0\n0.177\n0.175\n0.178\n0.177\n0.177\n0.191\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nJul 2016\n1.0\n0.142\n0.151\n0.156\n0.150\n0.166\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAug 2016\n1.0\n0.154\n0.130\n0.135\n0.119\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSep 2016\n1.0\n0.190\n0.162\n0.183\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nOct 2016\n1.0\n0.141\n0.150\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNov 2016\n1.0\n0.140\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nDec 2016\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12, 11))\nsns.heatmap(data=retention,\n            annot=True,\n            fmt='0.1%',\n            vmin=0.0,\n            vmax=0.1,\n            cmap='Blues')\nplt.show()"
  },
  {
    "objectID": "posts/python/cohort_analysis.html#segmented-by-quantity",
    "href": "posts/python/cohort_analysis.html#segmented-by-quantity",
    "title": "Cohort Analysis",
    "section": "Segmented by Quantity",
    "text": "Segmented by Quantity\n\n\nShow code\ncc_q, ret_q = get_cohort_matrix(df, var='Quantity', fun=pd.Series.mean)\ncc_q\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01\n3.164760\n3.345814\n3.134125\n3.043550\n3.509021\n3.644632\n3.640000\n2.857143\n3.851575\n2.989362\n6.000000\n2.494737\n\n\n2016-02-01\n2.986317\n2.720292\n3.063029\n2.815029\n3.089138\n3.820513\n2.364706\n2.484375\n6.640000\n1.650000\n1.823529\nNaN\n\n\n2016-03-01\n2.919831\n3.354397\n3.753517\n3.625239\n3.923795\n3.637677\n3.749833\n3.790764\n3.421667\n3.419633\nNaN\nNaN\n\n\n2016-04-01\n2.811928\n3.308361\n3.530516\n2.856732\n3.268678\n3.268426\n3.589691\n3.126519\n3.459594\nNaN\nNaN\nNaN\n\n\n2016-05-01\n2.710384\n3.160431\n3.235338\n3.091508\n3.111241\n3.214499\n3.625133\n3.346479\nNaN\nNaN\nNaN\nNaN\n\n\n2016-06-01\n2.712752\n3.067361\n3.405827\n2.577969\n3.165492\n2.727486\n3.466247\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-07-01\n2.730425\n2.467593\n2.674009\n3.529661\n2.740444\n3.260802\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-08-01\n2.607402\n2.758663\n2.699218\n3.588889\n2.625000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-09-01\n2.918562\n3.190777\n3.692153\n3.604326\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-10-01\n2.674215\n2.563017\n3.124542\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-11-01\n2.553461\n2.943689\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-12-01\n2.655255\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12,11))\nsns.heatmap(cc_q, annot=True,cmap='Blues', fmt='.1%')\nplt.show()"
  },
  {
    "objectID": "posts/python/cohort_analysis.html#segmented-by-sales-amount",
    "href": "posts/python/cohort_analysis.html#segmented-by-sales-amount",
    "title": "Cohort Analysis",
    "section": "Segmented by Sales Amount",
    "text": "Segmented by Sales Amount\n\n\nShow code\ncc_sa, ret_sa = get_cohort_matrix(df, var='Sales_Amount', fun=pd.Series.median)\ncc_sa\n\n\n\n\n\n\n\n\ncohort_idx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\nacq_month\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2016-01-01\n11.460\n13.290\n12.67\n12.470\n14.685\n15.730\n13.70\n12.05\n12.110\n11.165\n11.78\n13.75\n\n\n2016-02-01\n12.075\n12.360\n12.08\n11.190\n14.265\n14.705\n11.05\n9.91\n12.160\n9.860\n11.94\nNaN\n\n\n2016-03-01\n12.000\n13.650\n13.02\n13.785\n14.325\n13.140\n14.41\n16.25\n13.845\n14.870\nNaN\nNaN\n\n\n2016-04-01\n11.860\n12.620\n13.44\n12.630\n13.210\n13.430\n13.44\n13.80\n14.290\nNaN\nNaN\nNaN\n\n\n2016-05-01\n11.590\n13.535\n12.61\n13.100\n12.190\n12.160\n13.41\n13.12\nNaN\nNaN\nNaN\nNaN\n\n\n2016-06-01\n12.330\n12.690\n11.93\n11.500\n11.705\n12.645\n13.50\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-07-01\n12.030\n11.570\n12.08\n12.715\n13.750\n13.590\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-08-01\n11.880\n10.020\n11.38\n12.205\n11.620\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-09-01\n11.930\n10.740\n12.43\n13.780\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-10-01\n11.985\n11.010\n12.46\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-11-01\n12.130\n14.050\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2016-12-01\n12.740\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nShow code\n#creation of heatmap and visualization\nplt.figure(figsize=(12,10))\nsns.heatmap(cc_sa, annot=True,cmap='Blues', fmt='.1%')\nplt.show()"
  },
  {
    "objectID": "posts/python/cohort_analysis.html#references",
    "href": "posts/python/cohort_analysis.html#references",
    "title": "Cohort Analysis",
    "section": "References",
    "text": "References\n\n(2021). Bohorquez, N. Cohort Analysis with Python’s matplotlib, pandas, numpy and datetime in ActiveState, retrieved from https://www.activestate.com/blog/cohort-analysis-with-python"
  },
  {
    "objectID": "posts/python/cohort_analysis.html#contact",
    "href": "posts/python/cohort_analysis.html#contact",
    "title": "Cohort Analysis",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/slides.html",
    "href": "posts/python/slides.html",
    "title": "Creating Slides with Python and Quarto",
    "section": "",
    "text": "Quarto offers a powerful and versatile tool for data scientists to create presentations with code that are informative, transparent, reproducible, and visually appealing."
  },
  {
    "objectID": "posts/python/slides.html#contact",
    "href": "posts/python/slides.html#contact",
    "title": "Creating Slides with Python and Quarto",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/clq.html",
    "href": "posts/python/clq.html",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "",
    "text": "Figure 1: Several violent episodes in Mexico suggest a worrying trend\nSource: The Economist"
  },
  {
    "objectID": "posts/python/clq.html#spatial-concentration-of-high-impact-crime-in-mexico",
    "href": "posts/python/clq.html#spatial-concentration-of-high-impact-crime-in-mexico",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Spatial Concentration of High Impact Crime in Mexico",
    "text": "Spatial Concentration of High Impact Crime in Mexico\nThis study utilizes the Location Quotient (LQ) methodology to systematically identify and quantify the spatial concentration of High-Impact Crimes across Mexican municipalities, January through October 2025. We have reduced the research to analyze homicides, in order to establish baseline LQs for the target year.\nFindings are anticipated to reveal significant spatial segregation of criminal risk, with certain key urban and border jurisdictions demonstrating LQs significantly greater than 1.0, indicating a severe concentration of high-impact offenses.\nThe primary objective is to move beyond generalized security strategies by providing policymakers and public security agencies with granular, evidence-based intelligence for targeted resource allocation.\nThe resulting maps and risk profiles offer a critical foundation for designing preventative interventions, optimizing police deployment, and implementing precision policing strategies to effectively combat concentrated insecurity in Mexico."
  },
  {
    "objectID": "posts/python/clq.html#key-elements",
    "href": "posts/python/clq.html#key-elements",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Key Elements",
    "text": "Key Elements\n\nProblem: Traditional crime rates mask disparity.\nMethodology: Location Quotient (LQ) analysis.\nFocus: High-Impact Homicides in Mexico.\nTimeframe: January through October 2025.\nKey Findings: Significant spatial segregation/concentration (LQ &gt; 1.0) will be revealed.\nImpact: Provides intelligence for targeted resource allocation and precision policing."
  },
  {
    "objectID": "posts/python/clq.html#foundations-of-location-quotients",
    "href": "posts/python/clq.html#foundations-of-location-quotients",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Foundations of Location Quotients",
    "text": "Foundations of Location Quotients\nThe Location Quotient, primarily developed by economists to study regional specialization and industrial concentration, provides a measure of relative importance.\nHoover (1936) and Isard (1951) are credited with pioneering the concept. In economics, the LQ for an industry in a region is the ratio of that industry’s share of regional employment to its share of national employment.\nAn LQ greater than 1.0 signifies that the region is specialized in that industry compared to the nation. This concept is directly translated to crime analysis, replacing ‘industry’ with ‘crime type’ and ‘employment’ with ‘crime counts’ or ‘population.’"
  },
  {
    "objectID": "posts/python/clq.html#datasets",
    "href": "posts/python/clq.html#datasets",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Datasets",
    "text": "Datasets\n\nHomicides\n\nhomicides = Path('homicides.parquet')\n\ndf_homicides = pl.read_parquet(homicides)\n\ndf2 = df_homicides.to_pandas()\n\n\n(\n    df2 \n        .head(10)\n        .style.hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\ncve_geo\nEntidad\nMunicipio\nHomicidios\n\n\n\n\n1001\nAguascalientes\nAguascalientes\n36\n\n\n1002\nAguascalientes\nAsientos\n3\n\n\n1003\nAguascalientes\nCalvillo\n0\n\n\n1004\nAguascalientes\nCosío\n2\n\n\n1010\nAguascalientes\nEl Llano\n4\n\n\n1005\nAguascalientes\nJesús María\n14\n\n\n1006\nAguascalientes\nPabellón de Arteaga\n8\n\n\n1007\nAguascalientes\nRincón de Romos\n6\n\n\n1011\nAguascalientes\nSan Francisco de los Romo\n5\n\n\n1008\nAguascalientes\nSan José de Gracia\n1\n\n\n\n\n\n\nTable 1: Preview of homicides dataset\n\n\n\n\n\n\nPopulation\n\npop = Path('population.parquet')\n\ndf_pop = pl.read_parquet(pop)\n\ndf3 = df_pop.to_pandas()\n\n\n(\n  df3\n      .head(10)\n      .style.hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\ncve_geo\nEntidad\nMunicipio\nPob\n\n\n\n\n1001\nAGUASCALIENTES\nAGUASCALIENTES\n947544\n\n\n1002\nAGUASCALIENTES\nASIENTOS\n51492\n\n\n1003\nAGUASCALIENTES\nCALVILLO\n58342\n\n\n1004\nAGUASCALIENTES\nCOSIO\n16968\n\n\n1010\nAGUASCALIENTES\nEL LLANO\n20746\n\n\n1005\nAGUASCALIENTES\nJESUS MARIA\n129421\n\n\n1006\nAGUASCALIENTES\nPABELLON DE ARTEAGA\n47641\n\n\n1007\nAGUASCALIENTES\nRINCON DE ROMOS\n56864\n\n\n1011\nAGUASCALIENTES\nSAN FRANCISCO DE LOS ROMO\n61992\n\n\n1008\nAGUASCALIENTES\nSAN JOSE DE GRACIA\n9514\n\n\n\n\n\n\nTable 2: Preview of population dataset"
  },
  {
    "objectID": "posts/python/clq.html#data-collection-for-2025",
    "href": "posts/python/clq.html#data-collection-for-2025",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Data Collection for 2025",
    "text": "Data Collection for 2025\n\n\n\n\n\n\nNoteCrime Incidence Data\n\n\n\nRequired Data: The number of victims of homicide disaggregated by municipality and the national total for the study period (January-October 2025).\nSource: SESNSP Open Crime Incidence Data.\n\n\n\n\n\n\n\n\nNotePopulation Data\n\n\n\nRequired Data: The Municipal Population projection for 2025.\nSource: CONAPO."
  },
  {
    "objectID": "posts/python/duckdb_example.html#beers-table",
    "href": "posts/python/duckdb_example.html#beers-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Beers table",
    "text": "Beers table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS beers (\n        CodC integer,\n        Package varchar(255),\n        Capacity float,\n        Stock integer\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO beers\n    VALUES (1, 'Botella', 0.2, 3600),\n        (2, 'Botella', 0.33, 1200),\n        (3, 'Lata', 0.33, 2400),\n        (4, 'Botella', 1, 288),\n        (5, 'Barril', 60, 30)\n''')\n\n\nRetrieve data\n\n\nShow code\n# Showing in polars dataframe\ndb.sql('SELECT * FROM beers').df()\n\n\n\n\n\n\n\n\n\nCodC\nPackage\nCapacity\nStock\n\n\n\n\n0\n1\nBotella\n0.20\n3600\n\n\n1\n2\nBotella\n0.33\n1200\n\n\n2\n3\nLata\n0.33\n2400\n\n\n3\n4\nBotella\n1.00\n288\n\n\n4\n5\nBarril\n60.00\n30"
  },
  {
    "objectID": "posts/python/duckdb_example.html#bars-table",
    "href": "posts/python/duckdb_example.html#bars-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Bars table",
    "text": "Bars table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS bars (\n        CodB integer,\n        Name varchar(255),\n        Cif varchar(255),\n        Location varchar(255)\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO bars\n    VALUES (1, 'Stop', '111111X', 'Villa Botijo'),\n        (2, 'Las Vegas', '222222Y', 'Villa Botijo'),\n        (3, 'Club Social', '', 'Las Ranas'),\n        (4, 'Otra Ronda', '333333Z', 'La Esponja')\n''')\n\n\nRetrieve data\n\n\nShow code\n# showing in pandas dataframe\ndb.sql('SELECT * FROM bars').df()\n\n\n\n\n\n\n\n\n\nCodB\nName\nCif\nLocation\n\n\n\n\n0\n1\nStop\n111111X\nVilla Botijo\n\n\n1\n2\nLas Vegas\n222222Y\nVilla Botijo\n\n\n2\n3\nClub Social\n\nLas Ranas\n\n\n3\n4\nOtra Ronda\n333333Z\nLa Esponja"
  },
  {
    "objectID": "posts/python/duckdb_example.html#employees-table",
    "href": "posts/python/duckdb_example.html#employees-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Employees table",
    "text": "Employees table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS employees (\n        CodE integer,\n        Name varchar(255),\n        Salary float\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO employees\n    VALUES (1, 'John Doe', 120000),\n        (2, 'Vicent Meren', 110000),\n        (3, 'Tom Simpson', 100000)\n''')\n\n\nRetrieve data\n\n\nShow code\ndb.sql('SELECT * FROM employees').df()\n\n\n\n\n\n\n\n\n\nCodE\nName\nSalary\n\n\n\n\n0\n1\nJohn Doe\n120000.0\n\n\n1\n2\nVicent Meren\n110000.0\n\n\n2\n3\nTom Simpson\n100000.0"
  },
  {
    "objectID": "posts/python/duckdb_example.html#delivery-table",
    "href": "posts/python/duckdb_example.html#delivery-table",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Delivery table",
    "text": "Delivery table\nCreate table\n\n\nShow code\ndb.sql('''\n    CREATE TABLE IF NOT EXISTS delivery (\n        CodE integer,\n        CodB integer,\n        CodC integer,\n        Delivery_date date,\n        Quantity integer\n    )\n''')\n\n\nInsert data\n\n\nShow code\ndb.sql('''\n    INSERT INTO delivery\n    VALUES (1, 1, 1, '2005-10-21', 240),\n        (1, 1, 2, '2005-10-21', 48),\n        (1, 2, 3, '2005-10-22', 60),\n        (1, 4, 5, '2005-10-22', 4),\n        (2, 2, 3, '2005-10-23', 48),\n        (2, 2, 5, '2005-10-23', 2),\n        (2, 4, 1, '2005-10-24', 480),\n        (2, 4, 2, '2005-10-24', 72),\n        (3, 3, 3, '2005-10-24', 48),\n        (3, 3, 4, '2005-10-25', 20)\n''')\n\n\nRetrieve data\n\n\nShow code\ndb.sql('SELECT * FROM delivery').df()\n\n\n\n\n\n\n\n\n\nCodE\nCodB\nCodC\nDelivery_date\nQuantity\n\n\n\n\n0\n1\n1\n1\n2005-10-21\n240\n\n\n1\n1\n1\n2\n2005-10-21\n48\n\n\n2\n1\n2\n3\n2005-10-22\n60\n\n\n3\n1\n4\n5\n2005-10-22\n4\n\n\n4\n2\n2\n3\n2005-10-23\n48\n\n\n5\n2\n2\n5\n2005-10-23\n2\n\n\n6\n2\n4\n1\n2005-10-24\n480\n\n\n7\n2\n4\n2\n2005-10-24\n72\n\n\n8\n3\n3\n3\n2005-10-24\n48\n\n\n9\n3\n3\n4\n2005-10-25\n20"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-1",
    "href": "posts/python/duckdb_example.html#query-1",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 1",
    "text": "Query 1\nObtain the name of the employees who delivered to the Stop bar during the week of October 17 to 23, 2005.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT r.Delivery_date\n        ,e.Name\n        ,b.Name\n    FROM delivery AS r\n    LEFT JOIN employees AS e ON r.CodE = e.CodE\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    WHERE b.Name = 'Stop'\n        AND r.Delivery_date BETWEEN '2005-10-17' AND '2005-10-23'\n    '''\n)\n\n\n┌───────────────┬──────────┬─────────┐\n│ Delivery_date │   Name   │  Name   │\n│     date      │ varchar  │ varchar │\n├───────────────┼──────────┼─────────┤\n│ 2005-10-21    │ John Doe │ Stop    │\n│ 2005-10-21    │ John Doe │ Stop    │\n└───────────────┴──────────┴─────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-2",
    "href": "posts/python/duckdb_example.html#query-2",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 2",
    "text": "Query 2\nObtain the CIF and name of the bars to which bottle-type beer with a capacity of less than 1 liter has been distributed, ordered by location.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Cif\n        ,b.Name\n        ,c.Package\n        ,c.Capacity\n        ,b.Location\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Package = 'Botella'\n        AND c.Capacity &lt; 1.0\n    ORDER BY b.Location\n    '''\n)\n\n\n┌─────────┬────────────┬─────────┬──────────┬──────────────┐\n│   Cif   │    Name    │ Package │ Capacity │   Location   │\n│ varchar │  varchar   │ varchar │  float   │   varchar    │\n├─────────┼────────────┼─────────┼──────────┼──────────────┤\n│ 333333Z │ Otra Ronda │ Botella │      0.2 │ La Esponja   │\n│ 333333Z │ Otra Ronda │ Botella │     0.33 │ La Esponja   │\n│ 111111X │ Stop       │ Botella │      0.2 │ Villa Botijo │\n│ 111111X │ Stop       │ Botella │     0.33 │ Villa Botijo │\n└─────────┴────────────┴─────────┴──────────┴──────────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-3",
    "href": "posts/python/duckdb_example.html#query-3",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 3",
    "text": "Query 3\nObtain the deliveries (name of the bar, container and capacity of the drink, date and quantity) made by Prudencio Caminero.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n        ,r.Delivery_date\n        ,r.Quantity\n        ,e.Name\n        ,e.CodE\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    LEFT JOIN employees AS e ON e.CodE = r.CodE\n    WHERE e.Name ILIKE '%doe%'\n    '''\n)\n\n\n┌────────────┬─────────┬──────────┬───────────────┬──────────┬──────────┬───────┐\n│    Name    │ Package │ Capacity │ Delivery_date │ Quantity │   Name   │ CodE  │\n│  varchar   │ varchar │  float   │     date      │  int32   │ varchar  │ int32 │\n├────────────┼─────────┼──────────┼───────────────┼──────────┼──────────┼───────┤\n│ Stop       │ Botella │      0.2 │ 2005-10-21    │      240 │ John Doe │     1 │\n│ Stop       │ Botella │     0.33 │ 2005-10-21    │       48 │ John Doe │     1 │\n│ Las Vegas  │ Lata    │     0.33 │ 2005-10-22    │       60 │ John Doe │     1 │\n│ Otra Ronda │ Barril  │     60.0 │ 2005-10-22    │        4 │ John Doe │     1 │\n└────────────┴─────────┴──────────┴───────────────┴──────────┴──────────┴───────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-4",
    "href": "posts/python/duckdb_example.html#query-4",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 4",
    "text": "Query 4\nObtain the bars to which bottle-type containers with a capacity of 0.2 or 0.33 have been distributed.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Package = 'Botella'\n        AND c.Capacity IN (0.2, 0.33)\n    '''\n)\n\n\n┌────────────┬─────────┬──────────┐\n│    Name    │ Package │ Capacity │\n│  varchar   │ varchar │  float   │\n├────────────┼─────────┼──────────┤\n│ Stop       │ Botella │      0.2 │\n│ Stop       │ Botella │     0.33 │\n│ Otra Ronda │ Botella │      0.2 │\n│ Otra Ronda │ Botella │     0.33 │\n└────────────┴─────────┴──────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-5",
    "href": "posts/python/duckdb_example.html#query-5",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 5",
    "text": "Query 5\nName of the employees who have distributed bottled beers to the “Stop” and “Las Vegas” bars.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT e.Name\n        ,b.Name\n        ,c.Package\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    LEFT JOIN employees AS e On e.CodE = r.CodE\n    WHERE b.Name IN ('Stop', 'Las Vegas')\n        AND c.Package = 'Botella'\n    '''\n)\n\n\n┌──────────┬─────────┬─────────┐\n│   Name   │  Name   │ Package │\n│ varchar  │ varchar │ varchar │\n├──────────┼─────────┼─────────┤\n│ John Doe │ Stop    │ Botella │\n│ John Doe │ Stop    │ Botella │\n└──────────┴─────────┴─────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-6",
    "href": "posts/python/duckdb_example.html#query-6",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 6",
    "text": "Query 6\nObtain the name and number of trips that each employee has made outside of Villa Botijo.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT e.Name\n        ,COUNT(b.Location) AS Travles\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN employees AS e On e.CodE = r.CodE\n    WHERE b.Location &lt;&gt; 'Villa Botijo'\n    GROUP BY 1\n    '''\n)\n\n\n┌──────────────┬─────────┐\n│     Name     │ Travles │\n│   varchar    │  int64  │\n├──────────────┼─────────┤\n│ John Doe     │       1 │\n│ Tom Simpson  │       2 │\n│ Vicent Meren │       2 │\n└──────────────┴─────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-7",
    "href": "posts/python/duckdb_example.html#query-7",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 7",
    "text": "Query 7\nObtain the name and location of the bar that has purchased the most liters of beer.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,b.Location\n        ,MAX(r.Quantity) AS Liters\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    GROUP BY 1, 2\n    ORDER BY 3 DESC\n    '''\n)\n\n\n┌─────────────┬──────────────┬────────┐\n│    Name     │   Location   │ Liters │\n│   varchar   │   varchar    │ int32  │\n├─────────────┼──────────────┼────────┤\n│ Otra Ronda  │ La Esponja   │    480 │\n│ Stop        │ Villa Botijo │    240 │\n│ Las Vegas   │ Villa Botijo │     60 │\n│ Club Social │ Las Ranas    │     48 │\n└─────────────┴──────────────┴────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-8",
    "href": "posts/python/duckdb_example.html#query-8",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 8",
    "text": "Query 8\nObtain bars that have purchased all types of beer with bottle packaging and a capacity less than 1 liter.\n\n\nShow code\ndb.sql(\n    '''\n    SELECT b.Name\n        ,c.Package\n        ,c.Capacity\n    FROM delivery AS r\n    LEFT JOIN bars AS b ON r.CodB = b.CodB\n    LEFT JOIN beers AS c ON r.CodC = c.CodC\n    WHERE c.Capacity IN (SELECT c.Capacity\n                            FROM beers AS c\n                            WHERE c.Package = 'Botella'\n                                AND c.Capacity &lt; 1.0)\n    '''\n)\n\n\n┌─────────────┬─────────┬──────────┐\n│    Name     │ Package │ Capacity │\n│   varchar   │ varchar │  float   │\n├─────────────┼─────────┼──────────┤\n│ Stop        │ Botella │      0.2 │\n│ Stop        │ Botella │     0.33 │\n│ Las Vegas   │ Lata    │     0.33 │\n│ Las Vegas   │ Lata    │     0.33 │\n│ Otra Ronda  │ Botella │      0.2 │\n│ Otra Ronda  │ Botella │     0.33 │\n│ Club Social │ Lata    │     0.33 │\n└─────────────┴─────────┴──────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-9",
    "href": "posts/python/duckdb_example.html#query-9",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 9",
    "text": "Query 9\nRaise the salary of the employee who has worked the most days by 5%.\n\n\nShow code\n# modify database records\ndb.sql(\n    '''\n    -- Raise 5% salary of workers\n    INSERT INTO employees (CodE, Name, Salary)\n    VALUES (1, 'John Doe', 120000*1.05),\n            (2, 'Vicent Meren', 110000*1.05)\n    '''\n)\n\n\n\n\nShow code\n# verify inserted record\ndb.sql(\n    '''\n    SELECT e.*\n    FROM employees AS e\n    '''\n)\n\n\n┌───────┬──────────────┬──────────┐\n│ CodE  │     Name     │  Salary  │\n│ int32 │   varchar    │  float   │\n├───────┼──────────────┼──────────┤\n│     1 │ John Doe     │ 120000.0 │\n│     2 │ Vicent Meren │ 110000.0 │\n│     3 │ Tom Simpson  │ 100000.0 │\n│     1 │ John Doe     │ 126000.0 │\n│     2 │ Vicent Meren │ 115500.0 │\n└───────┴──────────────┴──────────┘"
  },
  {
    "objectID": "posts/python/duckdb_example.html#query-10",
    "href": "posts/python/duckdb_example.html#query-10",
    "title": "DuckDB: A Compelling Solution for Data Analysis within Python Environment",
    "section": "Query 10",
    "text": "Query 10\nInsert a new distribution from the employee “Vicent Meren” to the “Stop” bar of 48 canned beers on 2005-10-26.\n\n\nShow code\n# Insert new record\ndb.sql(\n    '''\n    INSERT INTO delivery (CodE, CodB, CodC, Delivery_date, Quantity)\n    VALUES (2, 1, 3, '2005-10-26', 48)\n    '''\n)\n\n\n\n\nShow code\n# verify inserted record\ndb.sql(\n    '''\n    SELECT * \n    FROM delivery\n    WHERE Delivery_date = '2005-10-26'\n    '''\n)\n\n\n┌───────┬───────┬───────┬───────────────┬──────────┐\n│ CodE  │ CodB  │ CodC  │ Delivery_date │ Quantity │\n│ int32 │ int32 │ int32 │     date      │  int32   │\n├───────┼───────┼───────┼───────────────┼──────────┤\n│     2 │     1 │     3 │ 2005-10-26    │       48 │\n└───────┴───────┴───────┴───────────────┴──────────┘"
  },
  {
    "objectID": "posts/python/cdmx-subway.html",
    "href": "posts/python/cdmx-subway.html",
    "title": "Mexico City’s Underground",
    "section": "",
    "text": "Figure 1: Mexico City’s Subway Network"
  },
  {
    "objectID": "posts/python/cdmx-subway.html#early-beginnings-and-expansion",
    "href": "posts/python/cdmx-subway.html#early-beginnings-and-expansion",
    "title": "Mexico City’s Underground",
    "section": "Early Beginnings and Expansion",
    "text": "Early Beginnings and Expansion\nThe STC’s inception dates back to the late 1960s, a time when Mexico City was experiencing unprecedented urban growth. Recognizing the need for a modern transportation system, the government embarked on an ambitious project to construct an underground railway. The first line, Line 1, opened its doors in 1969, connecting the historic center of the city with the northern suburbs.\nOver the following decades, the STC expanded rapidly, adding new lines and stations to accommodate the increasing population. Today, the system comprises 12 lines and serves over 5 million passengers daily. It has become an integral part of the city’s infrastructure, connecting diverse neighborhoods and facilitating economic activity."
  },
  {
    "objectID": "posts/python/cdmx-subway.html#challenges-and-overcoming-adversity",
    "href": "posts/python/cdmx-subway.html#challenges-and-overcoming-adversity",
    "title": "Mexico City’s Underground",
    "section": "Challenges and Overcoming Adversity",
    "text": "Challenges and Overcoming Adversity\nDespite its success, the STC has faced numerous challenges throughout its history. One of the most significant issues has been overcrowding, particularly during peak hours. The system’s capacity has often been strained, leading to long wait times and uncomfortable conditions for commuters. To address this problem, the authorities have implemented various measures, including expanding the network and improving train frequency.\nAnother challenge has been the maintenance and upkeep of the system. The STC’s infrastructure is aging, and some sections require significant repairs and upgrades. Ensuring the safety and reliability of the system has been a constant priority for the authorities. In recent years, there have been efforts to modernize the infrastructure and improve maintenance practices.\n\n\n\n\n\n\nFigure 2: Inside the Subway"
  },
  {
    "objectID": "posts/python/cdmx-subway.html#the-future-of-the-stc",
    "href": "posts/python/cdmx-subway.html#the-future-of-the-stc",
    "title": "Mexico City’s Underground",
    "section": "The Future of the STC",
    "text": "The Future of the STC\nLooking ahead, the STC faces both opportunities and challenges. The government has ambitious plans to further expand the system, connecting new areas of the city and improving accessibility. However, these projects require substantial investment and careful planning. Additionally, the STC will need to adapt to changing demographics and transportation trends, such as the rise of electric vehicles and ride-sharing services.\nDespite the challenges, the STC remains a vital component of Mexico City’s urban landscape. Its history is a testament to the city’s resilience and its ability to adapt to changing circumstances. As Mexico City continues to grow and evolve, the STC will play a crucial role in shaping its future.\n\nCreating Beautiful Tables with Python and Great-Tables\n\n\nCode\n# import libraries\nimport polars as pl\nimport duckdb as db\nfrom great_tables import GT, md, html, loc, style\n\n\n\n\nCode\n# connect to dabase\nconn = db.connect('my_database.db')\n\n\n\n\nCode\n# retrieve data from table to dataframe\ndf = conn.sql('select * from cdmx_subway').pl()\n\n\n\n\nCode\n# close connection\nconn.close()\n\n\n\n\nCode\n# Change datatypes\ndf = df.select(pl.exclude('Line'))\n\n\n\n\nCode\n# Create table with great-tables\ncdmx = (\n    GT(df)\n    .tab_header(\n        title=md(\"### Mexico City's Subway Lines and Stations\"),\n        subtitle=html('''&lt;h4 align=\"left\"&gt;\n        Mexico City's metro system is a vital artery of the bustling metropolis.\n        It is the largest and busiest in Latin America, serving more than 5.5 million\n        passengers daily in its 195 stations and 12 lines.&lt;/h4&gt;''')\n    )\n    #.tab_options(table_width=\"100%\")\n    .cols_align(align='center', columns=['icon','Opening date','Stations'])\n    .cols_label(\n        icon=\"Line Sation\"\n    )\n    .fmt_date(columns='Opening date', date_style=\"m_day_year\")\n    .fmt_image(\"icon\", path=\"cdmx_metro_lines\")\n    .sub_missing(missing_text=\"\")\n    .tab_source_note(source_note=md('''**Jesus L. Monroy**&lt;br&gt;*Economist & Data Scientist*&lt;br&gt;&lt;br&gt;'''))\n    .tab_source_note(\n        source_note=md('''Source: [Wikipedia](https://en.wikipedia.org/wiki/Mexico_City_Metro&gt;Wikipedia)'''))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City's Subway Lines and Stations\n\n\nMexico City's metro system is a vital artery of the bustling metropolis. It is the largest and busiest in Latin America, serving more than 5.5 million passengers daily in its 195 stations and 12 lines.\n\n\nLine Sation\nOpening date\nStations\n\n\n\n\n\nAug 22, 1984\n20\n\n\n\nAug 22, 1984\n24\n\n\n\nDec 1, 1979\n21\n\n\n\nAug 29, 1981\n10\n\n\n\nDec 19, 1981\n13\n\n\n\nDec 21, 1983\n11\n\n\n\nNov 29, 1988\n14\n\n\n\nJul 20, 1994\n19\n\n\n\nAug 26, 1987\n12\n\n\n\nOct 30, 2012\n20\n\n\n\nAug 12, 1991\n10\n\n\n\nDec 15, 1999\n21\n\n\n\nJesus L. Monroy\nEconomist & Data Scientist\n\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "posts/python/cdmx-subway.html#contact",
    "href": "posts/python/cdmx-subway.html#contact",
    "title": "Mexico City’s Underground",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/datawrapper_api.html",
    "href": "posts/python/datawrapper_api.html",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "",
    "text": "Figure 1: Datawrapper Charts Catalogue"
  },
  {
    "objectID": "posts/python/datawrapper_api.html#key-features-of-datawrapper",
    "href": "posts/python/datawrapper_api.html#key-features-of-datawrapper",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Key Features of Datawrapper",
    "text": "Key Features of Datawrapper\n\nEasy-to-use interface\n\nDatawrapper’s intuitive design makes it accessible to users of all skill levels. Even those without a strong background in data visualization can create professional-looking charts and maps.\n\nDiverse chart types\n\nFrom simple bar charts and line graphs to more complex maps and scatter plots, Datawrapper offers a variety of chart types to suit different data sets and storytelling needs.\n\nCustomization\n\nUsers can customize their visualizations to match their brand or personal style. This includes options for changing colors, fonts, and layouts.\n\nInteractivity\n\nDatawrapper allows users to create interactive visualizations that respond to user input. For example, users can hover over data points to see more detailed information or filter data based on specific criteria.\n\nEmbedding and sharing\n\nOnce a visualization is created, it can be easily embedded into websites, blogs, or social media posts. Users can also share their visualizations directly with others.\n\nCollaboration\n\nDatawrapper supports collaboration, allowing multiple users to work on the same visualization simultaneously. This is particularly useful for teams or organizations that need to create data visualizations together."
  },
  {
    "objectID": "posts/python/datawrapper_api.html#use-cases-for-datawrapper",
    "href": "posts/python/datawrapper_api.html#use-cases-for-datawrapper",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Use Cases for Datawrapper",
    "text": "Use Cases for Datawrapper\n\nJournalism\n\nDatawrapper is a popular tool among journalists who want to present complex data in a clear and visually engaging way. It can be used to create interactive charts, maps, and infographics that enhance storytelling.\n\nResearch\n\nResearchers can use Datawrapper to visualize their findings and make them more accessible to a wider audience. By presenting data in a visual format, researchers can communicate their ideas more effectively and increase the impact of their work.\n\nBusiness\n\nBusinesses can use Datawrapper to create dashboards, reports, and presentations that help them make data-driven decisions. By visualizing key metrics and trends, businesses can gain valuable insights into their performance.\n\nEducation\n\nTeachers and students can use Datawrapper to create interactive visualizations that help them understand and learn from data. It can be used to teach data analysis, statistics, and other subjects.\n\nVisualization Types\nCollection of available Datawrapper chart types and their IDs\n\n\n\nTable 1: Datawrapper Chart Types\n\n\n\n\n\nChart\nTypeID\n\n\n\n\nBar Chart\nd3-bars\n\n\nSplit Bars\nd3-bars-split\n\n\nStacked Bars\nd3-bars-stacked\n\n\nBullet Bars\nd3-bars-bullet\n\n\nDot Plot\nd3-dot-plot\n\n\nRange Plot\nd3-range-plot\n\n\nArrow Plot\nd3-arrow-plot\n\n\nColumn Chart\ncolumn-chart\n\n\nGrouped Column Chart\ngrouped-column-chart\n\n\nStacked Column Chart\nstacked-column-chart\n\n\nArea Chart\nd3-area\n\n\nLine Chart\nd3-lines\n\n\nMultiple Lines Chart\nmultiple-lines\n\n\nPie Chart\nd3-pies\n\n\nDonut Chart\nd3-donuts\n\n\nMultiple Pies\nd3-multiple-pies\n\n\nMultiple Donuts\nd3-multiple-donuts\n\n\nScatter Plot\nd3-scatter-plot\n\n\nElection Donut\nelection-donut-chart\n\n\nTable\ntables\n\n\nChoropleth Map\nd3-maps-choropleth\n\n\nSymbol Map\nd3-maps-symbols\n\n\nLocator Map\nlocator-map\n\n\n\n\n\n\nMore information\n\n\nEnvironment settings\n\n\nShow code\n# Import libraries\nfrom datawrapper import Datawrapper\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow code\n# Token gotten from datawrapper\nfilename = 'credentials.json'\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n# read credentials\ntoken = keys['datawrapper_api']\n\n\n\n\nShow code\n# Access datawrapper api\ndw = Datawrapper(access_token = token)\n\n\n\n\nDatawrapper account\n\n\nShow code\n# Check your account details\ndw.get_my_account()\n\n\n\n\nDataset\n\n\nShow code\n# Read dataset courtesy of Sergio Sanchez\ndf = pd.read_csv(\n    'https://raw.githubusercontent.com/chekos/datasets/master/data/datawrapper_example.csv',\n    sep=\";\",\n)\n\n\n\n\nShow code\n# Select columns\ndf.columns = ['country','%pop in the capital','%pop in urban areas','%pop in rural areas']\n\n\n\n\nShow code\n# Show dataframe\ndf\n\n\n\n\n\n\n\n\n\ncountry\n%pop in the capital\n%pop in urban areas\n%pop in rural areas\n\n\n\n\n0\nIceland (Reykjavík)\n56.02\n38.0\n6.0\n\n\n1\nArgentina (Buenos Aires)\n34.95\n56.6\n8.4\n\n\n2\nJapan (Tokyo)\n29.52\n63.5\n7.0\n\n\n3\nUK (London)\n22.70\n59.6\n17.7\n\n\n4\nDenmark (Copenhagen)\n22.16\n65.3\n12.5\n\n\n5\nFrance (Paris)\n16.77\n62.5\n20.7\n\n\n6\nRussia (Moscow)\n8.39\n65.5\n26.1\n\n\n7\nNiger (Niamey)\n5.53\n12.9\n81.5\n\n\n8\nGermany (Berlin)\n4.35\n70.7\n24.9\n\n\n9\nIndia (Delhi)\n1.93\n30.4\n67.6\n\n\n10\nUSA (Washington, D.C.)\n1.54\n79.9\n18.6\n\n\n11\nChina (Beijing)\n1.40\n53.0\n45.6\n\n\n\n\n\n\n\n\n\nCreate stackbar chart\n\n\nShow code\n# Create Datawrapper bar chart\npop = dw.create_chart(\n    title='Where do people live?', chart_type='d3-bars-stacked', data=df\n)\n\n\n\n\nUpdate chart description\n\n\nShow code\ndw.update_description(\n    pop['id'],\n    source_name = 'UN Population Division',\n    source_url = 'https://population.un.org/wup/',\n    byline = 'Jesus L. Monroy&lt;br&gt;Economist & Data Scientist&lt;br&gt;&lt;br&gt;',\n    intro = 'Population percentage living in the capital by Country'\n)\n\n\n\n\nPublish chart\n\n\nShow code\ndw.publish_chart(chart_id = pop['id'])\n\n\n\n\nCustomize metadata\n\n\nShow code\ndw.update_chart(\n    chart_id = pop['id'],\n    metadata = {\n        'visualize': {\n            'sharing': {'enabled': True},\n            'thick': True,\n            'custom-colors': {\n                '%pop in rural areas': '#dadada',\n                '%pop in urban areas': '#1d81a2',\n                '%pop in the capital': '#15607a',\n               },\n           },\n        'publish': {\n            'blocks': {'get-the-data': False},\n           },\n    }\n)\n\n\n\n\nRepublish chart\n\n\nShow code\ndw.publish_chart(pop['id'])"
  },
  {
    "objectID": "posts/python/datawrapper_api.html#display-interactive-chart",
    "href": "posts/python/datawrapper_api.html#display-interactive-chart",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure 2: Datawrapper Interactive Chart\n\n\n\n\n\n\n\n\n\nNoteNote\n\n\n\nYou can hover over the bars on the above chart to get dynamic results."
  },
  {
    "objectID": "posts/python/datawrapper_api.html#conclusions",
    "href": "posts/python/datawrapper_api.html#conclusions",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Conclusions",
    "text": "Conclusions\nTo start using Datawrapper, simply create a free account on their website. Once you’re logged in, you can begin creating your first visualization.\nDatawrapper offers a variety of tutorials and resources to help you get started and make the most of the platform.\nBy leveraging the power of Datawrapper, you can create compelling data visualizations that help you tell your story more effectively.\nWhether you’re a seasoned data analyst or just starting out, Datawrapper is a valuable tool that can help you bring your data to life."
  },
  {
    "objectID": "posts/python/datawrapper_api.html#references",
    "href": "posts/python/datawrapper_api.html#references",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "References",
    "text": "References\n\nDatawrapper (2024) Barcharts in Datawrapper Academy\nKhanchandani E. (2020) How to use Datawrapper for journalists in Interhacktives\nSanchez, S. (2023) A lightweight Python wrapper for the Datawrapper API in Datawrapper API"
  },
  {
    "objectID": "posts/python/datawrapper_api.html#contact",
    "href": "posts/python/datawrapper_api.html#contact",
    "title": "Creating Stunning Charts with Datawrapper and Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/polynomial_regression.html",
    "href": "posts/python/polynomial_regression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Oftentimes we’ll encounter data where the relationship between the feature(s) and the response variable can’t be best described with a straight line. In these cases, we should use polynomial regression.\nAn example of a polynomial, coud be:\n3x4–7x3+2x2+113x^4 – 7x^3 + 2x^2 + 11\nTerminology\n\nDegree of a polynomial: The highest power in polynomial. In our example, 4\nCoefficient: Each constant (3, 7, 2, 11) in polynomial is a coefficient. In polynomial regression, these coefficients will be estimated\nLeading term: The term with the highest power (3x43x^4). It determines the polynomial’s graph behavior\nLeading coefficient: The coefficient of the leading term (3)\nConstant term: The y intercept, it never changes: no matter what the value of x is, the constant term remains the same\n\n\n\nLet’s return to $ 3x4 - 7x3 + 2x2 + 11 $, if we write a polynomial’s terms from the highest degree term to the lowest degree term, it’s called a polynomial’s standard form.\nIn the context of machine learning, you’ll often see it reversed:\ny=β0+β1x+β2x2+…+βnxny = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\dots + \\beta_nx^n\nwhere:\n\ny is the response variable we want to predict\nx is the feature\nβ0\\beta_0 is the y intercept\n\nThe other ßs are the coefficients/parameters we’d like to find when we train our model on the available x and y values\n\nn is the degree of the polynomial (the higher n is, the more complex curved lines you can create)\nThe above polynomial regression formula is very similar to the multiple linear regression formula:\n\ny=β0+β1x+β2x+…+βnxy = \\beta_0 + \\beta_1x + \\beta_2x + \\dots + \\beta_nx\nIt’s not a coincidence: polynomial regression is a linear model used for describing non-linear relationships\nHow is this possible? The magic lies in creating new features by raising the original features to a power\nLinear regression is just a first-degree polynomial. Polynomial regression uses higher-degree polynomials. Both of them are linear models, but the first results in a straight line, the latter gives you a curved line.\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\n\nCode\n# create dummy dataset\nx = np.arange(0, 30)\ny = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75,\n     88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]\n\n\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create an polynomial instance\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n\nDegree = 2 means that we want to work with a 2nd degree polynomial,\ny=β0+β1x+β2x2y = \\beta_0 + \\beta_1x + \\beta_2x^2\n\n\nCode\npoly_features = poly.fit_transform(x.reshape(-1, 1))\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n\nIt may seem confusing, why are we importing LinearRegression module? 😮\nWe just have to remind that polynomial regression is a linear model, that’s why we import LinearRegression. 🙂\n\n\nCode\n# Create a LinearRegression() instance\npoly_reg_model = LinearRegression()\n\n\n\n\nCode\n# Fit model to data\npoly_reg_model.fit(poly_features, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Predict responses\ny_predicted = poly_reg_model.predict(poly_features)\n\n\n\n\nCode\ny_predicted\n\n\narray([  1.70806452,   3.04187987,   4.70292388,   6.69119657,\n         9.00669792,  11.64942794,  14.61938662,  17.91657397,\n        21.54098999,  25.49263467,  29.77150802,  34.37761004,\n        39.31094073,  44.57150008,  50.1592881 ,  56.07430478,\n        62.31655014,  68.88602415,  75.78272684,  83.00665819,\n        90.55781821,  98.4362069 , 106.64182425, 115.17467027,\n       124.03474495, 133.22204831, 142.73658033, 152.57834101,\n       162.74733037, 173.24354839])\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"A Basic Polynomial Regression Example\", size=16)\nplt.scatter(x, y)\nplt.plot(x, y_predicted, c=\"green\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create data\nnp.random.seed(1)\nx_1 = np.absolute(np.random.randn(100, 1) * 10)\nx_2 = np.absolute(np.random.randn(100, 1) * 30)\ny = 2*x_1**2 + 3*x_1 + 2 + np.random.randn(100, 1)*20\n\n\n\n\nCode\n# Create visual\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes[0].scatter(x_1, y)\naxes[1].scatter(x_2, y)\naxes[0].set_title(\"x_1 plotted\")\naxes[1].set_title(\"x_2 plotted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create dataframe\ndf = pd.DataFrame(\n    {\"x_1\":x_1.reshape(100,),\n     \"x_2\":x_2.reshape(100,),\n     \"y\":y.reshape(100,)},\n        index=range(0,100))\ndf\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      y\n    \n  \n  \n    \n      0\n      16.243454\n      13.413857\n      570.412369\n    \n    \n      1\n      6.117564\n      36.735231\n      111.681987\n    \n    \n      2\n      5.281718\n      12.104749\n      62.392124\n    \n    \n      3\n      10.729686\n      17.807356\n      303.538953\n    \n    \n      4\n      8.654076\n      32.847355\n      151.109269\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.773401\n      48.823150\n      -0.430738\n    \n    \n      96\n      3.438537\n      18.069578\n      44.308720\n    \n    \n      97\n      0.435969\n      12.608466\n      19.383456\n    \n    \n      98\n      6.200008\n      24.328550\n      78.371729\n    \n    \n      99\n      6.980320\n      31.333263\n      132.108914\n    \n  \n\n100 rows × 3 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Define train and test sets\nX, y = df[[\"x_1\", \"x_2\"]], df[\"y\"]\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Create polynomial regression\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Test model\npoly_reg_y_predicted = poly_reg_model.predict(X_test)\n\n\nThe formula for Root Mean Square Error is:\nRMSE=1n∑i=1n(yi−ŷi)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\nThe smaller the RMSE metric the better the model\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\npoly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))\nprint(f'Polynomial regression\\nRMSE: {poly_reg_rmse:.2f}')\n\n\nPolynomial regression\nRMSE: 20.94\n\n\n\n\n\n\nCode\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Create linear regression instance\nlin_reg_model = LinearRegression()\n# Fit regression model\nlin_reg_model.fit(X_train, y_train)\n# Create predictions\nlin_reg_y_predicted = lin_reg_model.predict(X_test)\n# Calculate RMSE\nlin_reg_rmse = np.sqrt(mean_squared_error(y_test, lin_reg_y_predicted))\n# Print results\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:,.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nIn the train_test_split method we use X instead of poly_features, and it’s for a good reason.\nX contains our two original features (x_1 and x_2), so our linear regression model takes the form of:\ny=β0+β1x1+β2x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\n\nCode\nlin_reg_model.coef_\n\n\narray([43.73176255, -0.53140809])\n\n\n\n\nCode\nlin_reg_model.intercept_\n\n\nnp.float64(-117.07280081594811)\n\n\nOn the other hand, poly_features contains new features as well, created out of x_1 and x_2, so our polynomial regression model (based on a 2nd degree polynomial with two features) looks like this:\ny=β0+β1x1+β2x2+β3x12+β4x22+β5x1x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_2^2 + \\beta_5x_1x_2\nThis is because poly.fit_transform(X) added three new features to the original two (x1 (x1x_1) and x2 (x2x_2)): x12x_1^2, x22x_2^2 and x1x2x_1x_2\nx12x_1^2 and x22x_2^2 need no explanation, as we’ve already covered how they are created in the “Coding a polynomial regression model with scikit-learn” section.\nWhat’s more interesting is x1x2x_1x_2 – when two features are multiplied by each other, it’s called an interaction term. An interaction term accounts for the fact that one variable’s value may depend on another variable’s value (more on this here). poly.fit_transform() automatically created this interaction term for us, isn’t that cool? 🙂\n\n\nCode\npoly_reg_model.coef_\n\n\narray([ 3.61945509, -1.0859955 ,  1.89905813,  0.0207338 ,  0.01300394])\n\n\n\n\nCode\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nThe RMSE for the polynomial regression model is 20.94, while the RMSE for the linear regression model is 62.3. The polynomial regression model performs almost 3 times better than the linear regression model!\n\n\n\n\nIn this notebook, we have been able to expose that a basic understanding of polynomial regression. And we have shown RMSE metric for comparing the performance among different ML models.\nWe used a 2nd degree polynomial for ourthis example. Naturally, we should always test and trial before deploying a model to find what degree of polynomial performs best.\n\n\n\n\nUjhelyi T. Polynomial regression I mainly based on.\nHow to become a Data Scientist\n\n\n\n\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/polynomial_regression.html#environment-settings",
    "href": "posts/python/polynomial_regression.html#environment-settings",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n\n\nCode\n# create dummy dataset\nx = np.arange(0, 30)\ny = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75,\n     88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]\n\n\n\n\nCode\nplt.figure(figsize=(10,6))\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create an polynomial instance\npoly = PolynomialFeatures(degree=2, include_bias=False)\n\n\nDegree = 2 means that we want to work with a 2nd degree polynomial,\ny=β0+β1x+β2x2y = \\beta_0 + \\beta_1x + \\beta_2x^2\n\n\nCode\npoly_features = poly.fit_transform(x.reshape(-1, 1))\n\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n\nIt may seem confusing, why are we importing LinearRegression module? 😮\nWe just have to remind that polynomial regression is a linear model, that’s why we import LinearRegression. 🙂\n\n\nCode\n# Create a LinearRegression() instance\npoly_reg_model = LinearRegression()\n\n\n\n\nCode\n# Fit model to data\npoly_reg_model.fit(poly_features, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Predict responses\ny_predicted = poly_reg_model.predict(poly_features)\n\n\n\n\nCode\ny_predicted\n\n\narray([  1.70806452,   3.04187987,   4.70292388,   6.69119657,\n         9.00669792,  11.64942794,  14.61938662,  17.91657397,\n        21.54098999,  25.49263467,  29.77150802,  34.37761004,\n        39.31094073,  44.57150008,  50.1592881 ,  56.07430478,\n        62.31655014,  68.88602415,  75.78272684,  83.00665819,\n        90.55781821,  98.4362069 , 106.64182425, 115.17467027,\n       124.03474495, 133.22204831, 142.73658033, 152.57834101,\n       162.74733037, 173.24354839])\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"A Basic Polynomial Regression Example\", size=16)\nplt.scatter(x, y)\nplt.plot(x, y_predicted, c=\"green\")\nplt.show()"
  },
  {
    "objectID": "posts/python/polynomial_regression.html#polynomial-regression-with-multiple-features",
    "href": "posts/python/polynomial_regression.html#polynomial-regression-with-multiple-features",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Code\n# Create data\nnp.random.seed(1)\nx_1 = np.absolute(np.random.randn(100, 1) * 10)\nx_2 = np.absolute(np.random.randn(100, 1) * 30)\ny = 2*x_1**2 + 3*x_1 + 2 + np.random.randn(100, 1)*20\n\n\n\n\nCode\n# Create visual\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\naxes[0].scatter(x_1, y)\naxes[1].scatter(x_2, y)\naxes[0].set_title(\"x_1 plotted\")\naxes[1].set_title(\"x_2 plotted\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create dataframe\ndf = pd.DataFrame(\n    {\"x_1\":x_1.reshape(100,),\n     \"x_2\":x_2.reshape(100,),\n     \"y\":y.reshape(100,)},\n        index=range(0,100))\ndf\n\n\n\n\n\n  \n    \n      \n      x_1\n      x_2\n      y\n    \n  \n  \n    \n      0\n      16.243454\n      13.413857\n      570.412369\n    \n    \n      1\n      6.117564\n      36.735231\n      111.681987\n    \n    \n      2\n      5.281718\n      12.104749\n      62.392124\n    \n    \n      3\n      10.729686\n      17.807356\n      303.538953\n    \n    \n      4\n      8.654076\n      32.847355\n      151.109269\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.773401\n      48.823150\n      -0.430738\n    \n    \n      96\n      3.438537\n      18.069578\n      44.308720\n    \n    \n      97\n      0.435969\n      12.608466\n      19.383456\n    \n    \n      98\n      6.200008\n      24.328550\n      78.371729\n    \n    \n      99\n      6.980320\n      31.333263\n      132.108914\n    \n  \n\n100 rows × 3 columns\n\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\n# Define train and test sets\nX, y = df[[\"x_1\", \"x_2\"]], df[\"y\"]\npoly = PolynomialFeatures(degree=2, include_bias=False)\npoly_features = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)\n\n\n\n\nCode\n# Create polynomial regression\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n\nCode\n# Test model\npoly_reg_y_predicted = poly_reg_model.predict(X_test)\n\n\nThe formula for Root Mean Square Error is:\nRMSE=1n∑i=1n(yi−ŷi)2RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\nThe smaller the RMSE metric the better the model\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\npoly_reg_rmse = np.sqrt(mean_squared_error(y_test, poly_reg_y_predicted))\nprint(f'Polynomial regression\\nRMSE: {poly_reg_rmse:.2f}')\n\n\nPolynomial regression\nRMSE: 20.94\n\n\n\n\n\n\nCode\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Create linear regression instance\nlin_reg_model = LinearRegression()\n# Fit regression model\nlin_reg_model.fit(X_train, y_train)\n# Create predictions\nlin_reg_y_predicted = lin_reg_model.predict(X_test)\n# Calculate RMSE\nlin_reg_rmse = np.sqrt(mean_squared_error(y_test, lin_reg_y_predicted))\n# Print results\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:,.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nIn the train_test_split method we use X instead of poly_features, and it’s for a good reason.\nX contains our two original features (x_1 and x_2), so our linear regression model takes the form of:\ny=β0+β1x1+β2x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\n\nCode\nlin_reg_model.coef_\n\n\narray([43.73176255, -0.53140809])\n\n\n\n\nCode\nlin_reg_model.intercept_\n\n\nnp.float64(-117.07280081594811)\n\n\nOn the other hand, poly_features contains new features as well, created out of x_1 and x_2, so our polynomial regression model (based on a 2nd degree polynomial with two features) looks like this:\ny=β0+β1x1+β2x2+β3x12+β4x22+β5x1x2y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1^2 + \\beta_4x_2^2 + \\beta_5x_1x_2\nThis is because poly.fit_transform(X) added three new features to the original two (x1 (x1x_1) and x2 (x2x_2)): x12x_1^2, x22x_2^2 and x1x2x_1x_2\nx12x_1^2 and x22x_2^2 need no explanation, as we’ve already covered how they are created in the “Coding a polynomial regression model with scikit-learn” section.\nWhat’s more interesting is x1x2x_1x_2 – when two features are multiplied by each other, it’s called an interaction term. An interaction term accounts for the fact that one variable’s value may depend on another variable’s value (more on this here). poly.fit_transform() automatically created this interaction term for us, isn’t that cool? 🙂\n\n\nCode\npoly_reg_model.coef_\n\n\narray([ 3.61945509, -1.0859955 ,  1.89905813,  0.0207338 ,  0.01300394])\n\n\n\n\nCode\nprint(f'Linear regression\\nRMSE: {lin_reg_rmse:.2f}')\n\n\nLinear regression\nRMSE: 62.30\n\n\nThe RMSE for the polynomial regression model is 20.94, while the RMSE for the linear regression model is 62.3. The polynomial regression model performs almost 3 times better than the linear regression model!"
  },
  {
    "objectID": "posts/python/polynomial_regression.html#conclusion",
    "href": "posts/python/polynomial_regression.html#conclusion",
    "title": "Polynomial Regression",
    "section": "",
    "text": "In this notebook, we have been able to expose that a basic understanding of polynomial regression. And we have shown RMSE metric for comparing the performance among different ML models.\nWe used a 2nd degree polynomial for ourthis example. Naturally, we should always test and trial before deploying a model to find what degree of polynomial performs best."
  },
  {
    "objectID": "posts/python/polynomial_regression.html#references",
    "href": "posts/python/polynomial_regression.html#references",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Ujhelyi T. Polynomial regression I mainly based on.\nHow to become a Data Scientist"
  },
  {
    "objectID": "posts/python/polynomial_regression.html#contact",
    "href": "posts/python/polynomial_regression.html#contact",
    "title": "Polynomial Regression",
    "section": "",
    "text": "Jesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/tableau/missing-people.html",
    "href": "posts/tableau/missing-people.html",
    "title": "Missing People in Mexico",
    "section": "",
    "text": "This dashboard was designed to track missing people by location, last place seen and place type.\n\nData Sources: The data mainly come from government agency dedicated to attend requests about missing people.\nKey Metrics: The dashboard shows locations where missing people was seen for last time, location origin and missing people by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for government agency for public statistics and operational performance."
  },
  {
    "objectID": "posts/tableau/missing-people.html#overview",
    "href": "posts/tableau/missing-people.html#overview",
    "title": "Missing People in Mexico",
    "section": "",
    "text": "This dashboard was designed to track missing people by location, last place seen and place type.\n\nData Sources: The data mainly come from government agency dedicated to attend requests about missing people.\nKey Metrics: The dashboard shows locations where missing people was seen for last time, location origin and missing people by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for government agency for public statistics and operational performance."
  },
  {
    "objectID": "posts/tableau/missing-people.html#contact",
    "href": "posts/tableau/missing-people.html#contact",
    "title": "Missing People in Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/tableau/looker-studio.html",
    "href": "posts/tableau/looker-studio.html",
    "title": "Looker Studio",
    "section": "",
    "text": "We used Looker Studio to present results about customer behavior and online sales and conversions from Google Analytics.\nGoogle Analytics is a powerful tool for any business, but it becomes especially crucial for e-commerce businesses. It provides a wealth of data and insights into how customers interact with your online store, allowing you to optimize your website and marketing efforts to drive sales."
  },
  {
    "objectID": "posts/tableau/looker-studio.html#summary",
    "href": "posts/tableau/looker-studio.html#summary",
    "title": "Looker Studio",
    "section": "",
    "text": "We used Looker Studio to present results about customer behavior and online sales and conversions from Google Analytics.\nGoogle Analytics is a powerful tool for any business, but it becomes especially crucial for e-commerce businesses. It provides a wealth of data and insights into how customers interact with your online store, allowing you to optimize your website and marketing efforts to drive sales."
  },
  {
    "objectID": "posts/tableau/looker-studio.html#e-commerce-weekly-dashboard",
    "href": "posts/tableau/looker-studio.html#e-commerce-weekly-dashboard",
    "title": "Looker Studio",
    "section": "E-Commerce Weekly Dashboard",
    "text": "E-Commerce Weekly Dashboard\n\nProject Goal: This dashboard was designed to track the principal indicators about e-commerce from websites and mobile apps.\nData Sources: The data mainly come from Google Analytics tracking websites and mobile apps.\nKey Metrics: The dashboard shows users, new users, pageviews, sessions, orders, revenue and conversion rates.\nVisualizations: The types of charts used are time series for behavior, KPI cards, bar charts for categories and tables for detailed information.\nTarget Audience: The dashboard was designed for marketing managers and their teams."
  },
  {
    "objectID": "posts/tableau/looker-studio.html#e-commerce-weekly-report",
    "href": "posts/tableau/looker-studio.html#e-commerce-weekly-report",
    "title": "Looker Studio",
    "section": "E-Commerce Weekly Report",
    "text": "E-Commerce Weekly Report\n\nProject Goal: This report was designed to track the principal indicators about e-commerce from websites and mobile apps.\nData Sources: The data mainly come from Google Analytics tracking websites and mobile apps.\nKey Metrics: The report shows users, new users, pageviews, sessions, orders, revenue and conversion rates by year and month.\nVisualizations: The types of charts used are sparklines for behavior of the different metrics.\nTarget Audience: The report was designed for the marketing team for operational monitoring."
  },
  {
    "objectID": "posts/tableau/looker-studio.html#references",
    "href": "posts/tableau/looker-studio.html#references",
    "title": "Looker Studio",
    "section": "References",
    "text": "References\n\nAdvantages of Looker Studio"
  },
  {
    "objectID": "posts/tableau/looker-studio.html#contact",
    "href": "posts/tableau/looker-studio.html#contact",
    "title": "Looker Studio",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/tableau/sports.html",
    "href": "posts/tableau/sports.html",
    "title": "Sports Commerce",
    "section": "",
    "text": "This dashboard was designed to show sales by store and time intervals.\n\nData Sources: The data mainly come from sales department.\nKey Metrics: The dashboard shows sales by stores, quantities of items sold and sales by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories.\nTarget Audience: The dashboard was designed for sales department team with a sales target parameter to analyze which stores exceeds the target and which keeps below."
  },
  {
    "objectID": "posts/tableau/sports.html#overview",
    "href": "posts/tableau/sports.html#overview",
    "title": "Sports Commerce",
    "section": "",
    "text": "This dashboard was designed to show sales by store and time intervals.\n\nData Sources: The data mainly come from sales department.\nKey Metrics: The dashboard shows sales by stores, quantities of items sold and sales by time.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories.\nTarget Audience: The dashboard was designed for sales department team with a sales target parameter to analyze which stores exceeds the target and which keeps below."
  },
  {
    "objectID": "posts/tableau/sports.html#contact",
    "href": "posts/tableau/sports.html#contact",
    "title": "Sports Commerce",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/tableau/mexico-city-crimes.html",
    "href": "posts/tableau/mexico-city-crimes.html",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "",
    "text": "This dashboard was designed to track crime incidence in Mexico City during 2019-2024 by time, gender, age, mayorship and neighborhood.\n\nData Sources: The data comes from Mexico City Open Data Webpage.\nKey Metrics: The dashboard shows crime KPIs.\nVisualizations: The types of charts used are bar and map charts.\nTarget Audience: The dashboard was designed for crime incidence follow-up."
  },
  {
    "objectID": "posts/tableau/mexico-city-crimes.html#overview",
    "href": "posts/tableau/mexico-city-crimes.html#overview",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "",
    "text": "This dashboard was designed to track crime incidence in Mexico City during 2019-2024 by time, gender, age, mayorship and neighborhood.\n\nData Sources: The data comes from Mexico City Open Data Webpage.\nKey Metrics: The dashboard shows crime KPIs.\nVisualizations: The types of charts used are bar and map charts.\nTarget Audience: The dashboard was designed for crime incidence follow-up."
  },
  {
    "objectID": "posts/tableau/mexico-city-crimes.html#contact",
    "href": "posts/tableau/mexico-city-crimes.html#contact",
    "title": "Mexico City Crime Incidence 2019-2024",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/tableau/covid-cases.html",
    "href": "posts/tableau/covid-cases.html",
    "title": "Covid Cases in Americas",
    "section": "",
    "text": "This dashboard was designed to track cases and deaths by covid in Americas in 2024 by region and country.\n\nData Sources: The data comes from Worldometer information about cases and deaths by countries.\nKey Metrics: The dashboard shows cases and deaths KPIs.\nVisualizations: The types of charts used are bar and map charts by country.\nTarget Audience: The dashboard was designed for health follow-up."
  },
  {
    "objectID": "posts/tableau/covid-cases.html#overview",
    "href": "posts/tableau/covid-cases.html#overview",
    "title": "Covid Cases in Americas",
    "section": "",
    "text": "This dashboard was designed to track cases and deaths by covid in Americas in 2024 by region and country.\n\nData Sources: The data comes from Worldometer information about cases and deaths by countries.\nKey Metrics: The dashboard shows cases and deaths KPIs.\nVisualizations: The types of charts used are bar and map charts by country.\nTarget Audience: The dashboard was designed for health follow-up."
  },
  {
    "objectID": "posts/tableau/covid-cases.html#contact",
    "href": "posts/tableau/covid-cases.html#contact",
    "title": "Covid Cases in Americas",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/tableau/us-sales.html",
    "href": "posts/tableau/us-sales.html",
    "title": "US Sales Dashboard",
    "section": "",
    "text": "This dashboard was designed to track sales, profits and customers by categories, months and locations.\n\nData Sources: The data mainly come from the sales department with a detailed information about sales, customers and profits categories.\nKey Metrics: The dashboard shows sales, profit and customers KPIs.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for sales managers and their teams."
  },
  {
    "objectID": "posts/tableau/us-sales.html#overview",
    "href": "posts/tableau/us-sales.html#overview",
    "title": "US Sales Dashboard",
    "section": "",
    "text": "This dashboard was designed to track sales, profits and customers by categories, months and locations.\n\nData Sources: The data mainly come from the sales department with a detailed information about sales, customers and profits categories.\nKey Metrics: The dashboard shows sales, profit and customers KPIs.\nVisualizations: The types of charts used are time series for behavior, bar charts for categories and map for locations.\nTarget Audience: The dashboard was designed for sales managers and their teams."
  },
  {
    "objectID": "posts/tableau/us-sales.html#contact",
    "href": "posts/tableau/us-sales.html#contact",
    "title": "US Sales Dashboard",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/tableau/sundries.html",
    "href": "posts/tableau/sundries.html",
    "title": "Tableau Charts example",
    "section": "",
    "text": "In this section we present some examples of charts created with Tableau.\nKeep in mind that dashboards are instrinsically “exploratory”, i.e. you need to actively explore and interact by yourself with dashboards. There is no guided path."
  },
  {
    "objectID": "posts/tableau/sundries.html#summary",
    "href": "posts/tableau/sundries.html#summary",
    "title": "Tableau Charts example",
    "section": "",
    "text": "In this section we present some examples of charts created with Tableau.\nKeep in mind that dashboards are instrinsically “exploratory”, i.e. you need to actively explore and interact by yourself with dashboards. There is no guided path."
  },
  {
    "objectID": "posts/tableau/sundries.html#dynamics-of-personnel-deployed-in-mexico",
    "href": "posts/tableau/sundries.html#dynamics-of-personnel-deployed-in-mexico",
    "title": "Tableau Charts example",
    "section": "Dynamics of Personnel Deployed in Mexico",
    "text": "Dynamics of Personnel Deployed in Mexico\n\n\n\n\n\n\n\nNoteNote\n\n\n\nClick below to wath the video"
  },
  {
    "objectID": "posts/tableau/sundries.html#example-of-cloud-chart",
    "href": "posts/tableau/sundries.html#example-of-cloud-chart",
    "title": "Tableau Charts example",
    "section": "Example of cloud chart",
    "text": "Example of cloud chart"
  },
  {
    "objectID": "posts/tableau/sundries.html#profits-by-year-and-category",
    "href": "posts/tableau/sundries.html#profits-by-year-and-category",
    "title": "Tableau Charts example",
    "section": "Profits by year and category",
    "text": "Profits by year and category"
  },
  {
    "objectID": "posts/tableau/sundries.html#profitloss-by-sub-category",
    "href": "posts/tableau/sundries.html#profitloss-by-sub-category",
    "title": "Tableau Charts example",
    "section": "Profit/Loss by sub-category",
    "text": "Profit/Loss by sub-category"
  },
  {
    "objectID": "posts/tableau/sundries.html#profitloss-in-function-of-minimum-expected-level-by-category",
    "href": "posts/tableau/sundries.html#profitloss-in-function-of-minimum-expected-level-by-category",
    "title": "Tableau Charts example",
    "section": "Profit/Loss in function of minimum expected level by category",
    "text": "Profit/Loss in function of minimum expected level by category"
  },
  {
    "objectID": "posts/tableau/sundries.html#identification-of-products-by-profit-or-loss-contribution",
    "href": "posts/tableau/sundries.html#identification-of-products-by-profit-or-loss-contribution",
    "title": "Tableau Charts example",
    "section": "Identification of products by profit or loss contribution",
    "text": "Identification of products by profit or loss contribution"
  },
  {
    "objectID": "posts/tableau/sundries.html#contact",
    "href": "posts/tableau/sundries.html#contact",
    "title": "Tableau Charts example",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/mexico_peace_index.html#what-does-it-measue",
    "href": "posts/python/mexico_peace_index.html#what-does-it-measue",
    "title": "Mexico’s Peace index 2024",
    "section": "What does it measue?",
    "text": "What does it measue?\n\nMeasures peace across the entire country.\nAnalyzes recent violence and peace trends.\nEstimates the economic cost of violence in Mexico.\nIdentifies factors that drive peace and instability."
  },
  {
    "objectID": "posts/python/mexico_peace_index.html#how-its-measured",
    "href": "posts/python/mexico_peace_index.html#how-its-measured",
    "title": "Mexico’s Peace index 2024",
    "section": "How it’s measured",
    "text": "How it’s measured\nThe MPI isn’t just about crime rates. It considers 12 sub-indicators grouped into five major categories:\n\nOngoing Conflict\nSafety and Security\nDomesticized Conflict\nMilitarization\nIncarceration"
  },
  {
    "objectID": "posts/python/mexico_peace_index.html#display-interactive-chart",
    "href": "posts/python/mexico_peace_index.html#display-interactive-chart",
    "title": "Mexico’s Peace index 2024",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure 1: Mexico Peace Index\n\n\n\n\n\n\n\n\n\nNoteInfo\n\n\n\nYou can hover the mouse over the map to get additional information.\n\n\n\n\n\n\n\n\nFigure 2: Mexico Peace Index Variation"
  },
  {
    "objectID": "posts/python/mexico_peace_index.html#references",
    "href": "posts/python/mexico_peace_index.html#references",
    "title": "Mexico’s Peace index 2024",
    "section": "References",
    "text": "References\n\nMexico Peace Index"
  },
  {
    "objectID": "posts/python/mexico_peace_index.html#contact",
    "href": "posts/python/mexico_peace_index.html#contact",
    "title": "Mexico’s Peace index 2024",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/gdrive_project.html",
    "href": "posts/python/gdrive_project.html",
    "title": "Stop Building Dashboards",
    "section": "",
    "text": "Figure 1: ETL Phases"
  },
  {
    "objectID": "posts/python/gdrive_project.html#overview",
    "href": "posts/python/gdrive_project.html#overview",
    "title": "Stop Building Dashboards",
    "section": "Overview",
    "text": "Overview\nSome reasons why spreadsheets and slideshows persist in the office workflow, include:\n\nFamiliarity and Ease of Use\nFlexibility and Control\nStorytelling and Communication\nCollaboration and Sharing\nAd-hoc Analysis and Exploration\nCost and Accessibility\n\nWhile BI tools may be transforming the way we analyze data, it’s clear that spreadsheets and slideshows aren’t going anywhere anytime soon. They serve a different purpose, filling a gap that BI tools often miss.\nBuilding a Python-Powered Data Pipeline\nBusiness Intelligence (BI) tools are powerful, but they can also be expensive and complex. What if you could build a custom, flexible, and potentially more cost-effective solution using Python?\nThis post explores how you can leverage Python to collect, transform, and deliver data to Spreadsheets and Slides for compelling presentations. Why Python?\nPython has become a powerhouse in data science and automation. Its rich ecosystem of libraries makes it ideal for data manipulation, and seamless slides interaction. This combination offers a powerful alternative to traditional BI tools for certain use cases.\nIn this post we shall propose the use of Python (to collect, cleanse and transform data), Google Spreadsheets (to store transformed data) and Google Slides (to showcase visualizations). Proposed Workflow\nImagine you need to generate a weekly sales report and all you have to do is to run the next command:\n%%bash\njupyter-execute ./projects/weekly-report.ipynb\nAnd, voila! you have your weekly report updated and ready to present in Google Slides."
  },
  {
    "objectID": "posts/python/gdrive_project.html#environment-settings",
    "href": "posts/python/gdrive_project.html#environment-settings",
    "title": "Stop Building Dashboards",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nShow code\n# Import authenticator and gspread to manage g-sheets\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\n\n# Import other libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow code\n# get token\nfilename = 'credentials.json'\n\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n\n# read credentials\ntoken = keys['md_token']"
  },
  {
    "objectID": "posts/python/gdrive_project.html#extract-phase",
    "href": "posts/python/gdrive_project.html#extract-phase",
    "title": "Stop Building Dashboards",
    "section": "Extract Phase",
    "text": "Extract Phase\n\n\nShow code\n# connect to motherduck cloud\nconn = db.connect(f'md:?motherduck_token={token}')\n\n\n\n\nShow code\nconn.sql('show databases')\n\n\n\n\n\n┌───────────────────────┐\n│     database_name     │\n│        varchar        │\n├───────────────────────┤\n│ md_information_schema │\n│ my_db                 │\n│ my_portfolio          │\n│ sample_data           │\n└───────────────────────┘\n\n\n\nTable 1: Databases\n\n\n\n\n\n\nShow code\n# select specific database\nconn.sql('use my_portfolio')\n\n\n\n\nShow code\n# show tables in database\nconn.sql('show tables')\n\n\n\n\n\n┌──────────────────┐\n│       name       │\n│     varchar      │\n├──────────────────┤\n│ airports         │\n│ appl_stock       │\n│ cdmx_subway      │\n│ colors           │\n│ contains_null    │\n│ houses           │\n│ people           │\n│ prevalencia      │\n│ restaurants      │\n│ retail_sales     │\n│ sales            │\n│ sales_info       │\n│ sets             │\n│ water_collection │\n├──────────────────┤\n│     14 rows      │\n└──────────────────┘\n\n\n\nTable 2: Tables in database\n\n\n\n\n\n\nShow code\n# dataset\ndataset = conn.sql('select * from restaurants').df()\n\n(\n    dataset.head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nGreek Gyros\n770\n$68.39\nNew York\nFrench\n1\n\n\nTaste of Italy\n4,420\n$88.23\nAmsterdam\nChinese\n0\n\n\nMidnight Diner\n2,155\n$12.97\nLisbon\nMexican\n1\n\n\nTaste of Italy\n3,375\n$52.79\nSydney\nChinese\n1\n\n\n\n\n\n\nTable 3: Dataset Preview"
  },
  {
    "objectID": "posts/python/gdrive_project.html#transform-phase",
    "href": "posts/python/gdrive_project.html#transform-phase",
    "title": "Stop Building Dashboards",
    "section": "Transform Phase",
    "text": "Transform Phase\n\nWhich restaurant chain has the maximum number of restaurants?\n\n\nShow code\nchains = (\n    conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n    ''').df()\n)\nchains\n\n\n\n\n\n\n\n\n\n\n\n\nname\nno_of_chains\n\n\n\n\n0\nThe Burger Joint\n721\n\n\n1\nPizza Palace\n703\n\n\n2\nGreek Gyros\n696\n\n\n3\nCafe Delight\n692\n\n\n4\nFrench Delights\n681\n\n\n5\nThe BBQ Shack\n671\n\n\n6\nThe Golden Wok\n667\n\n\n7\nOcean Breeze\n665\n\n\n8\nSpice & Bloom\n665\n\n\n9\nMidnight Diner\n657\n\n\n\n\n\n\n\n\nTable 4: Data grouped by restaurant chains\n\n\n\n\n\n\nWhich restaurant chain has generated maximum revenue?\n\n\nShow code\nrevenue = (\n    conn.sql('''\n    select name, sum(rating_count * cost) as revenue\n    from restaurants\n    group by name\n    order by revenue DESC\n    limit 10\n    ''').df()\n)\n\n(\n    revenue\n        .style\n        .hide()    \n        .format({'revenue': '{:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrevenue\n\n\n\n\nThe Burger Joint\n108,820,424.64\n\n\nPizza Palace\n100,382,853.92\n\n\nCafe Delight\n98,037,063.93\n\n\nGreek Gyros\n96,403,445.25\n\n\nThe BBQ Shack\n96,286,414.02\n\n\nOcean Breeze\n95,664,612.77\n\n\nThe Golden Wok\n94,860,125.75\n\n\nSpice & Bloom\n91,824,854.56\n\n\nFrench Delights\n91,236,701.38\n\n\nMidnight Diner\n91,170,162.30\n\n\n\n\n\n\nTable 5: Data grouped by restaurant and revenue\n\n\n\n\n\n\nWhich city has generated maximum revenue?\n\n\nShow code\ncities = (\n    conn.sql('''\n    select city, sum(rating_count * cost) as revenue\n    from restaurants\n    group by city\n    order by revenue DESC\n    limit 10\n    ''').df()\n)\n\n(\n    cities\n        .style\n        .hide()    \n        .format({'revenue': '${:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nrevenue\n\n\n\n\nAmsterdam\n$148,839,878.62\n\n\nTokyo\n$148,035,421.32\n\n\nMadrid\n$141,487,618.97\n\n\nParis\n$141,219,374.56\n\n\nLondon\n$140,876,613.54\n\n\nRome\n$139,622,129.63\n\n\nNew York\n$138,621,609.28\n\n\nLisbon\n$136,814,247.27\n\n\nBerlin\n$136,434,163.25\n\n\nSydney\n$131,656,513.97\n\n\n\n\n\n\nTable 6: Data grouped by city and revenue"
  },
  {
    "objectID": "posts/python/gdrive_project.html#load-phase",
    "href": "posts/python/gdrive_project.html#load-phase",
    "title": "Stop Building Dashboards",
    "section": "Load Phase",
    "text": "Load Phase\n\n\nShow code\n# Create scope to authenticate\nSCOPES = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n\n# Read credentials\nGOOGLE_SHEETS_KEY_FILE = 'arkham-538.json'\ncredentials = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_SHEETS_KEY_FILE, SCOPES)\ngc = gspread.authorize(credentials)\n\n\n\n\nShow code\nimport pytz\nimport datetime\n\ntz = pytz.timezone('America/Mexico_City')\nupdate = datetime.datetime.now(tz).strftime('%b %d, %Y')\nperiod = update\n\n\n\n\nShow code\ndef save_to_gsheets(df, sheet_name, worksheet_name, period):\n    creds = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_SHEETS_KEY_FILE, SCOPES)\n    client = gspread.authorize(creds)\n    sheet = client.open(sheet_name)\n    worksheet = sheet.worksheet(worksheet_name)\n\n    # Convert datetimes to strings in advance\n    for column in df.columns[df.dtypes == 'datetime64[ns]']:\n        df[column] = df[column].astype(str)\n\n    # Prepare data for batch update\n    data = [df.columns.values.tolist()] + df.fillna('').values.tolist()\n\n    # Freeze rows and update cell values with a single batch update\n    worksheet.freeze(4)\n    worksheet.update('A4:M', data)\n\n    #fija fecha de consulta o actualizacion\n    update_data = {\n    'Last update': [\n        period,]\n    }\n\n    # convert to dataframe\n    update_data = pd.DataFrame(update_data, columns=['Last update'])\n\n    worksheet.update([update_data.columns.values.tolist()] + update_data.fillna('').values.tolist(),'A1:A2',)\n\n    print(f'DataFrame uploaded to: workbook: {sheet_name}, sheet: {worksheet_name}')\n\n\n\n\nShow code\nsave_to_gsheets(dataset, 'restaurants', 'data', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: data\n\n\n\n\nShow code\nsave_to_gsheets(chains, 'restaurants', 'chains', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: chains\n\n\n\n\nShow code\nsave_to_gsheets(revenue, 'restaurants', 'revenue', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: revenue\n\n\n\n\nShow code\nsave_to_gsheets(cities, 'restaurants', 'cities', period)\n\n\nDataFrame uploaded to: workbook: restaurants, sheet: cities"
  },
  {
    "objectID": "posts/python/gdrive_project.html#close-connection",
    "href": "posts/python/gdrive_project.html#close-connection",
    "title": "Stop Building Dashboards",
    "section": "Close connection",
    "text": "Close connection\n\n\nShow code\n# close connection\nconn.close()"
  },
  {
    "objectID": "posts/python/gdrive_project.html#retrieve-data-from-gsheets",
    "href": "posts/python/gdrive_project.html#retrieve-data-from-gsheets",
    "title": "Stop Building Dashboards",
    "section": "Retrieve data from gsheets",
    "text": "Retrieve data from gsheets\n\n\nShow code\n# Access worksheet id\ndf_id = '1JNAWb2QkFwh61v7QwEEVZnNhTPS0csbdMdll9y1csEg'\ndf_workbook = gc.open_by_key(df_id)\n# Access data by worksheet sheet\ndf = df_workbook.worksheet('data')\n# Save data to table\ndf = df.get_all_values()\n# Save accessed data from google sheets to dataframe\ndf = pd.DataFrame(df[1:], columns=df[0])\n\n\n\n\nShow code\ndf.head()\n\n\n\n\n\n\n\n\n\n\n\n\nLast update\n\n\n\n\n\n\n\n\n\n0\nFeb 24, 2025\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n3\nThe Golden Wok\n1477\n33.62048759\nBerlin\nAmerican\n5\n\n\n4\nGreek Gyros\n770\n68.38887409\nNew York\nFrench\n1\n\n\n\n\n\n\n\n\nTable 7: Data Saved on Gogle Sheets"
  },
  {
    "objectID": "posts/python/gdrive_project.html#google-sheets-report-data",
    "href": "posts/python/gdrive_project.html#google-sheets-report-data",
    "title": "Stop Building Dashboards",
    "section": "Google Sheets Report Data",
    "text": "Google Sheets Report Data\n\n\n\n\n\n\nFigure 2: Google Sheets Data for Presentation Report"
  },
  {
    "objectID": "posts/python/gdrive_project.html#sync-between-google-sheets-and-google-slides",
    "href": "posts/python/gdrive_project.html#sync-between-google-sheets-and-google-slides",
    "title": "Stop Building Dashboards",
    "section": "Sync between Google Sheets and Google Slides",
    "text": "Sync between Google Sheets and Google Slides\nSimply we copy and paste with sync for each table and chart and customize our slides.\n\n\n\n\n\n\nFigure 3: Synchronization between Google Sheets and Slides\n\n\n\n\n\n\n\n\n\nNoteGoogle Slides\n\n\n\nYou can see the final report on Google Slides"
  },
  {
    "objectID": "posts/python/gdrive_project.html#conclusions",
    "href": "posts/python/gdrive_project.html#conclusions",
    "title": "Stop Building Dashboards",
    "section": "Conclusions",
    "text": "Conclusions\nWhile BI tools are valuable, Python offers a compelling alternative for building custom data pipelines. By leveraging the power of Python using polars and duckdb libraries for data collection and transformation, and libraries like plotly for visualization you can create a flexible, cost-effective, and automated solution for delivering data to Google Spreadsheets, using gspread, and Google Slides for impactful presentations, by sync between these Google apps.\nThis approach empowers you to take control of your data and create highly tailored reporting solutions by replacing BI license costs."
  },
  {
    "objectID": "posts/python/gdrive_project.html#references",
    "href": "posts/python/gdrive_project.html#references",
    "title": "Stop Building Dashboards",
    "section": "References",
    "text": "References\n\nBusiness (2023) How to Design a Dashboard Presentation: A Step-by-Step Guide in slidemodel.com\nKarlson, P. (2022) Are Spreadsheets Secretly Running Your Business? in Forbes\nMonroy, Jesus (2024) Why BI Tools Fall Short: PowerPoint and Excel Still Rule the Business World in Medium\nMoore J. (2024) But, Can I Export it to Excel? in Do Mo(o)re with Data\nSchwab, P. (2021) Excel dominates the business world… and that’s not about to change in Into the Minds"
  },
  {
    "objectID": "posts/python/gdrive_project.html#contact",
    "href": "posts/python/gdrive_project.html#contact",
    "title": "Stop Building Dashboards",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/big-query.html#extract-phase",
    "href": "posts/python/big-query.html#extract-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Extract Phase",
    "text": "Extract Phase\nThe extraction phase entails retrieving data from your source. This may involve interacting with:\n\nFlat files\nDatabases\nXML files\nAPIs\nOther\n\n\nFlat files\n\n\nCode\nusers = (\n    pl.read_csv('users.csv', dtypes={'phone': pl.Utf8, 'id_atg':pl.Utf8})\n    .with_columns(\n        pl.col('entry_data').str.strptime(pl.Datetime, strict=False)\n    )\n)\n\n\n\n\nCode\nusers_profiling = (\n    # read csv file\n    pl.read_csv('profiles.csv', dtypes={'contact_phone': pl.Utf8,'post_code':pl.Utf8})\n        # change column dtypes\n    .with_columns(\n        pl.col('entry_data','entry_data_gep','update_date').str.strptime(pl.Datetime, strict=False),\n        pl.col('contact_phone').cast(pl.Utf8),\n    )\n)\n\n\n\n\nCode\norders = (\n    pl.read_csv('orders.csv').with_columns(\n        pl.col('entry_date','delivery_date').str.strptime(pl.Datetime, strict=False)\n    )\n)\n\n\n\n\nCode\norder_details = (\n    pl.read_csv('order-details.csv')\n)\n\n\n\n\nCode\npromotions = (\n    pl.read_csv('promotions.csv', dtypes={'short_description':pl.Utf8})\n    .with_columns(pl.col('key').cast(pl.Utf8))\n).unique(subset='key')\n\n\n\n\nCode\norder_status = pl.read_csv('order-status.csv')\n\n\n\n\nCode\nsocial_networks = (\n    pl.read_csv('social_networks.csv').select(\n        pl.col('id_social_network','social_network','description')\n    )\n)\n\n\n\n\nParquet files\n\n\nCode\ntypes = pl.read_parquet('types.parquet')\n\n\n\n\nCode\ngenre = pl.read_parquet('genre.parquet')\n\n\n\n\njson files\n\n\nCode\nwarehouses = pl.read_json('warehuse_catalog.json')\n\n\n\n\nDatabases\n\n\nCode\n# Connection to MS Access\nconn = pyodbc.connect(r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'r'DBQ=C:\\Users\\user\\folder\\file.accdb;')\n# Create cursor\ncursor = conn.cursor()\n\n\n\n\nCode\n# Write query\nquery = 'select * from sales where year=2022'\n# Convert to pandas dataframe\ndf = pd.read_sql(query, con=conn)\ndf.head()"
  },
  {
    "objectID": "posts/python/big-query.html#transform-phase",
    "href": "posts/python/big-query.html#transform-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Transform Phase",
    "text": "Transform Phase\nCleanse, validate, and manipulate the extracted data based on your analysis requirements. This might include:\n\nData Cleaning\n\nHandle missing values, inconsistent formatting, or errors.\n\nData Type Conversion\n\nEnsure consistent data types for columns based on their intended use in BigQuery.\n\nFiltering/Aggregation\n\nSelect or aggregate specific data subsets for targeted analysis.\n\nEnrichment\n\nMerge extracted data with additional sources to enhance its value.\n\n\nCode\n# join types, orders, order_details, promotions and warehouses\nsheet = (\n    types.join(orders, on='id_type', how='left')\n    .join(order_details, on='id_order', how='left')\n    .join(promotions, on='key', how='left')\n    .join(warehouses, on='id_warehouse', how='left')\n).rename({'id_warehouse':'id_warehouse_promo', 'active':'promo_active'})"
  },
  {
    "objectID": "posts/python/big-query.html#load-phase",
    "href": "posts/python/big-query.html#load-phase",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Load Phase",
    "text": "Load Phase\nThere are two primary options for loading data:\n\nStaging Table\n\nCreate a staging table and load the transformed data into it for temporary storage before validating and potentially modifying it:\n\nDirect Load\n\nLoad the data directly into your target table, bypassing the staging step. However, this approach can be less flexible for complex transformations:\n\n\nCode\n# create dataset\nclient.create_dataset('database')\n\n\nDataset(DatasetReference('gepp-538', 'database'))\n\n\n\n\nCode\n# convert to pandas\nsheet = sheet.to_pandas()\n# upload to big query\nsheet.to_gbq('dw.transformation.catalog',\n                    project_id='repository-538',\n                    if_exists='replace',\n                    credentials=bq_credentials)\n\n\n100%|███████████████████████████████████████████| 1/1 [00:00&lt;00:00, 7626.01it/s]"
  },
  {
    "objectID": "posts/python/big-query.html#execute-queries-from-big-query",
    "href": "posts/python/big-query.html#execute-queries-from-big-query",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Execute queries from Big Query",
    "text": "Execute queries from Big Query\n\n\nCode\n# create sql query\nquery = '''\n    SELECT *\n    FROM `dw.transformation.catalog`\n'''\n# convert query to pandas dataframe\ncatalog = pd.read_gbq(query, credentials=bq_credentials)"
  },
  {
    "objectID": "posts/python/big-query.html#contact",
    "href": "posts/python/big-query.html#contact",
    "title": "Building Data Pipelines with BigQuery and Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/mexico_pop1950-2023.html",
    "href": "posts/python/mexico_pop1950-2023.html",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "",
    "text": "In this article we shall see Mexico’s Population evolution over years.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n\n\nCode\n# Read tsv file\nmex = pd.read_csv('datasets/mexico_pop.tsv', sep='\\t')\n# Drop spaces\nmex = mex.rename(columns={'Year ':'Year','Population ':'Population','Growth Rate':'Growth rate'})\n\n\n\n\nCode\nmex.sample(5)\n\n\n\n\n\n\n\n\n\nYear\nPopulation\nGrowth rate\n\n\n\n\n41\n1982\n70,656,783\n2.06%\n\n\n54\n1969\n48,714,394\n3.27%\n\n\n69\n1954\n30,616,896\n2.67%\n\n\n42\n1981\n69,233,769\n2.26%\n\n\n47\n1976\n60,452,543\n3.00%\n\n\n\n\n\n\n\n\n\nCode\nmex['Population'] = pd.to_numeric(mex['Population'].str.replace(',',''))\n\n\n\n\nCode\nmex['Growth rate'] = pd.to_numeric(mex['Growth rate'].str.replace('%','', regex=True))\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Population'))\n    + geom_line(color='#b30000', size=1.5, linetype='solid')\n    + labs(x='Year', y='Population', \n           title='Total Population in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 150_000_000))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw() # Removes \"&lt;Figure Size: (640 x 480)&gt;\" below chart\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Growth rate'))\n    + geom_line(color='#767600', size=1.5, linetype='solid')\n    + labs(x='Year', y='Grwoth rate %', \n           title='Population Growth in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 5))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw()"
  },
  {
    "objectID": "posts/python/mexico_pop1950-2023.html#introduction",
    "href": "posts/python/mexico_pop1950-2023.html#introduction",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "",
    "text": "In this article we shall see Mexico’s Population evolution over years.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n\n\nCode\n# Read tsv file\nmex = pd.read_csv('datasets/mexico_pop.tsv', sep='\\t')\n# Drop spaces\nmex = mex.rename(columns={'Year ':'Year','Population ':'Population','Growth Rate':'Growth rate'})\n\n\n\n\nCode\nmex.sample(5)\n\n\n\n\n\n\n\n\n\nYear\nPopulation\nGrowth rate\n\n\n\n\n41\n1982\n70,656,783\n2.06%\n\n\n54\n1969\n48,714,394\n3.27%\n\n\n69\n1954\n30,616,896\n2.67%\n\n\n42\n1981\n69,233,769\n2.26%\n\n\n47\n1976\n60,452,543\n3.00%\n\n\n\n\n\n\n\n\n\nCode\nmex['Population'] = pd.to_numeric(mex['Population'].str.replace(',',''))\n\n\n\n\nCode\nmex['Growth rate'] = pd.to_numeric(mex['Growth rate'].str.replace('%','', regex=True))\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Population'))\n    + geom_line(color='#b30000', size=1.5, linetype='solid')\n    + labs(x='Year', y='Population', \n           title='Total Population in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 150_000_000))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw() # Removes \"&lt;Figure Size: (640 x 480)&gt;\" below chart\n\n\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(mex, aes(x='Year', y='Growth rate'))\n    + geom_line(color='#767600', size=1.5, linetype='solid')\n    + labs(x='Year', y='Grwoth rate %', \n           title='Population Growth in Mexico 1950-2023', \n           subtitle='\\n Description:')\n    + scale_y_continuous(expand=(0,0), limits=(0, 5))\n    + scale_x_continuous(expand=(0,0), limits=(1950, 2023))\n).draw()"
  },
  {
    "objectID": "posts/python/mexico_pop1950-2023.html#conclusions",
    "href": "posts/python/mexico_pop1950-2023.html#conclusions",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "Conclusions",
    "text": "Conclusions\nMexico boasts a population of nearly 130 million, ranking it as the 10th most populous country globally.\nThe population is steadily growing, with estimates suggesting an annual increase of around 1.2%.\nMexico is a highly urbanized nation, with 88% of the population residing in cities. This trend is expected to continue, driven by economic opportunities and infrastructure development.\nThe overall population density is moderate, at around 66 individuals per square kilometer, but varies significantly across regions.\nThe population is relatively young, with a median age of 29.8 years. This indicates a large segment in their working years, potentially driving economic growth.\nHowever, an aging population is expected in the coming decades, requiring adjustments to social security and healthcare systems.\nMexico is a multicultural nation with a rich indigenous heritage. While mestizos (mixed Indigenous and European ancestry) form the majority, Indigenous groups still comprise a significant portion of the population, contributing to cultural diversity.\nRapid urbanization brings challenges like managing infrastructure, providing essential services, and tackling inequality.\nInvesting in education, healthcare, and job creation can harness the demographic dividend presented by the young population.\nAddressing the potential impact of an aging population and promoting sustainable development are crucial for long-term prosperity."
  },
  {
    "objectID": "posts/python/mexico_pop1950-2023.html#references",
    "href": "posts/python/mexico_pop1950-2023.html#references",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "References",
    "text": "References\n\nWorldometer\nINEGI - National Institute of Statistics and Geography\nCIA World Factbook"
  },
  {
    "objectID": "posts/python/mexico_pop1950-2023.html#contact",
    "href": "posts/python/mexico_pop1950-2023.html#contact",
    "title": "Mexico: A Populous Nation with a Dynamic Demographic Landscape",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/ceferesos.html",
    "href": "posts/python/ceferesos.html",
    "title": "Federal Prisons in Mexico",
    "section": "",
    "text": "Figure 1: Cefereso No. 12 Vehicule Gate"
  },
  {
    "objectID": "posts/python/ceferesos.html#main-functions",
    "href": "posts/python/ceferesos.html#main-functions",
    "title": "Federal Prisons in Mexico",
    "section": "Main Functions",
    "text": "Main Functions\n\nAdministration of the Federal Penitentiary System\nResponsible for the management and operation of federal social rehabilitation centers (CEFERESOS) throughout the country.\nImplementation of social reintegration policies\nDevelops and implements programs and strategies to facilitate the reintegration of inmates into society through educational, employment, sports, and health activities.\nGuarantee of Human Rights\nEnsures respect for the human rights of persons deprived of their liberty, ensuring decent living conditions in prisons.\nCrime Prevention\nContributes to crime prevention through the implementation of effective social reintegration programs and collaboration with other public security institutions."
  },
  {
    "objectID": "posts/python/ceferesos.html#importance",
    "href": "posts/python/ceferesos.html#importance",
    "title": "Federal Prisons in Mexico",
    "section": "Importance",
    "text": "Importance\nThe OADPRS plays a fundamental role in the criminal justice system, as social rehabilitation is a key element in reducing recidivism and strengthening public security. Its work is essential to ensuring that persons deprived of their liberty have the opportunity to reintegrate into society in a productive and law-abiding manner. The work of the OADPRS is monitored by the National Human Rights Commission to ensure that inmates’ rights are respected."
  },
  {
    "objectID": "posts/python/ceferesos.html#challenges",
    "href": "posts/python/ceferesos.html#challenges",
    "title": "Federal Prisons in Mexico",
    "section": "Challenges",
    "text": "Challenges\nThe OADPRS faces significant challenges, such as overcrowding in some prisons, the need to improve inmates’ living conditions, and the fight against corruption. In addition, the CNDH has issued various recommendations stemming from the lack of timely medical care within penitentiary centers.\nIn short, the OADPRS is a crucial institution for public safety and criminal justice in Mexico, responsible for administering the federal penitentiary system and promoting the social reintegration of inmates."
  },
  {
    "objectID": "posts/python/crimes-web.html",
    "href": "posts/python/crimes-web.html",
    "title": "Mexico City Crime According to the Statistics",
    "section": "",
    "text": "Figure 1: Mexico City Crimes"
  },
  {
    "objectID": "posts/python/crimes-web.html#import-libraries",
    "href": "posts/python/crimes-web.html#import-libraries",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Import libraries",
    "text": "Import libraries\n\n\nShow code\n#import libraries\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport duckdb as db\nimport datetime as datetime\nimport json\nimport plotly.express as px\nimport folium\nfrom folium.plugins import Fullscreen"
  },
  {
    "objectID": "posts/python/crimes-web.html#database-connection",
    "href": "posts/python/crimes-web.html#database-connection",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Database connection",
    "text": "Database connection\n\n\nShow code\n# get token\nfilename = 'credentials.json'\n\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n\n# read credentials\ntoken = keys['md_token']\n\n\nDuckdb is a powerful tool for data analysts and developers who need to perform fast and efficient analytical queries on large datasets, especially in environments where simplicity and portability are crucial.\n\n\nShow code\n# connect to motherduck cloud\nconn = db.connect(f'md:?motherduck_token={token}')\n\n\n\n\nShow code\n# retrieve dataframe\ndf = conn.execute('select * from projects.cdmx_fgj_uncleaned').pl()\n\n\n\n\nShow code\n# close connection\nconn.close()\n\n\n\n\nShow code\nprint(f'The dataset contains {df.shape[0]:,.0f} rows')\n\n\nThe dataset contains 1,415,763 rows"
  },
  {
    "objectID": "posts/python/crimes-web.html#missing-values",
    "href": "posts/python/crimes-web.html#missing-values",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Missing values",
    "text": "Missing values\n\n\nShow code\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1415763 entries, 0 to 1415762\nData columns (total 22 columns):\n #   Column             Non-Null Count    Dtype  \n---  ------             --------------    -----  \n 0   anio_inicio        1415748 non-null  float64\n 1   mes_inicio         1415748 non-null  object \n 2   fecha_inicio       1415748 non-null  object \n 3   hora_inicio        1415748 non-null  object \n 4   anio_hecho         1415343 non-null  float64\n 5   mes_hecho          1415343 non-null  object \n 6   fecha_hecho        1415342 non-null  object \n 7   hora_hecho         1415353 non-null  object \n 8   delito             1415749 non-null  object \n 9   categoria_delito   1415749 non-null  object \n 10  sexo               1168059 non-null  object \n 11  edad               931053 non-null   float64\n 12  tipo_persona       1408196 non-null  object \n 13  calidad_juridica   1415748 non-null  object \n 14  competencia        1415749 non-null  object \n 15  colonia_hecho      1340796 non-null  object \n 16  colonia_catalogo   1323317 non-null  object \n 17  alcaldia_hecho     1413284 non-null  object \n 18  alcaldia_catalogo  1372214 non-null  object \n 19  municipio_hecho    1413284 non-null  object \n 20  latitud            1340994 non-null  float64\n 21  longitud           1340994 non-null  float64\ndtypes: float64(5), object(17)\nmemory usage: 237.6+ MB\n\n\n\n\nShow code\ndf = df.dropna(subset=['latitud'])\n\n\n\n\nShow code\nprint(f'The dataset now has {df.shape[0]:,.0f} rows')\n\n\nThe dataset now has 1,340,994 rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that the original dataset includes 1,415,763 rows.\nHowever, there are many null values in different fields for considerable gaps.\nAs the dataset can be considered large, we decided to drop all null latitude rows, obtaining 1,340,994 rows.\n\n\n\n\nDue to the missing rows in many fields, we kept around with 94.72% of the original dataset"
  },
  {
    "objectID": "posts/python/crimes-web.html#polars-dataframe",
    "href": "posts/python/crimes-web.html#polars-dataframe",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Polars dataframe",
    "text": "Polars dataframe\nPolars is a modern DataFrame library that prioritizes performance and efficiency. Its Rust-based architecture, combined with features like lazy evaluation and parallel processing, makes it a powerful tool for data professionals.\n\n\nShow code\n# convert to polars dataframe\ndf = pl.from_pandas(df)\n\n\n\n\nShow code\ndf.schema\n\n\nSchema([('anio_inicio', Float64),\n        ('mes_inicio', String),\n        ('fecha_inicio', String),\n        ('hora_inicio', String),\n        ('anio_hecho', Float64),\n        ('mes_hecho', String),\n        ('fecha_hecho', String),\n        ('hora_hecho', String),\n        ('delito', String),\n        ('categoria_delito', String),\n        ('sexo', String),\n        ('edad', Float64),\n        ('tipo_persona', String),\n        ('calidad_juridica', String),\n        ('competencia', String),\n        ('colonia_hecho', String),\n        ('colonia_catalogo', String),\n        ('alcaldia_hecho', String),\n        ('alcaldia_catalogo', String),\n        ('municipio_hecho', String),\n        ('latitud', Float64),\n        ('longitud', Float64)])"
  },
  {
    "objectID": "posts/python/crimes-web.html#data-cleansing",
    "href": "posts/python/crimes-web.html#data-cleansing",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Data cleansing",
    "text": "Data cleansing\n\n\nShow code\ndf = (\n    df.filter(\n    # drop null and 2018 years\n        (pl.col('anio_inicio')!=2018)\n    ).with_columns(\n    # create datetime field\n        (pl.col('fecha_inicio').cast(pl.String) + ' ' + pl.col('hora_inicio')\n             .cast(pl.String)).alias('fecha_inicio')\n    ).select(\n    # exclude columns\n        pl.exclude('anio_inicio','mes_inicio','hora_inicio',\n                   'anio_hecho','mes_hecho','fecha_hecho','hora_hecho')\n    ).with_columns(\n        fecha_inicio=pl.col('fecha_inicio').str.to_datetime()\n    ).drop_nulls(subset='fecha_inicio')\n)\n\n\n\n\nShow code\ndf.select(pl.all().is_null().sum()).to_dicts()\n\n\n[{'fecha_inicio': 0,\n  'delito': 0,\n  'categoria_delito': 0,\n  'sexo': 237070,\n  'edad': 449780,\n  'tipo_persona': 6858,\n  'calidad_juridica': 1,\n  'competencia': 0,\n  'colonia_hecho': 382,\n  'colonia_catalogo': 17677,\n  'alcaldia_hecho': 3,\n  'alcaldia_catalogo': 953,\n  'municipio_hecho': 3,\n  'latitud': 0,\n  'longitud': 0}]\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that even if we dropped around 75,000 rows, there continue to be many fields with empty rows, such as sex, age, neighborhood, mayorship and municipality."
  },
  {
    "objectID": "posts/python/crimes-web.html#data-cleaning-for-age",
    "href": "posts/python/crimes-web.html#data-cleaning-for-age",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Data cleaning for age",
    "text": "Data cleaning for age\n\n\nAge goes from 0.0 up to 369.0 years old!\n\n\n\n\nShow code\n# case for age\ndf = df.with_columns(\n    pl.when(pl.col('edad') &lt; 18).then(18)\n    .when(pl.col('edad')&gt;99).then(99)\n    .otherwise('edad')\n    .alias('edad')\n)\n\n\nWe have cleaned age values by setting age less than 18 to 18, and age values gretar than 99 to 99."
  },
  {
    "objectID": "posts/python/crimes-web.html#accrued-crimes-by-year",
    "href": "posts/python/crimes-web.html#accrued-crimes-by-year",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Accrued crimes by year",
    "text": "Accrued crimes by year\n\n\nShow code\nyears = (\n    df.sort('fecha_inicio')\n        .group_by_dynamic('fecha_inicio', every='1y')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n(\n    years\n        .to_pandas()\n        .style.format(\n            {\n            'fecha_inicio':'{:%Y}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nfecha_inicio\ncrimes\n\n\n\n\n2019\n256,827\n\n\n2020\n204,659\n\n\n2021\n228,627\n\n\n2022\n237,659\n\n\n2023\n239,402\n\n\n2024\n173,803\n\n\n\n\n\n\nTable 1: Accrued Crimes in Mexico City by Year\n\n\n\n\n\n\nShow code\nfig = px.bar(years, \n             x='fecha_inicio',\n             y='crimes',\n             orientation='v',\n             hover_data=['fecha_inicio','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Year',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Crimes')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Crimes in Mexico City by Year"
  },
  {
    "objectID": "posts/python/crimes-web.html#time-behavior-of-crimes",
    "href": "posts/python/crimes-web.html#time-behavior-of-crimes",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Time behavior of crimes",
    "text": "Time behavior of crimes\n\n\nShow code\nmonths = (\n    df\n        .sort('fecha_inicio', descending=False)\n        .group_by_dynamic('fecha_inicio', every='1mo')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n(\n    months\n        .to_pandas()\n        .tail(10)\n        .style.format(\n            {\n            'fecha_inicio':'{:%b, %Y}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nfecha_inicio\ncrimes\n\n\n\n\nDec, 2023\n17,358\n\n\nJan, 2024\n18,354\n\n\nFeb, 2024\n18,750\n\n\nMar, 2024\n19,797\n\n\nApr, 2024\n19,923\n\n\nMay, 2024\n20,879\n\n\nJun, 2024\n19,209\n\n\nJul, 2024\n19,429\n\n\nAug, 2024\n19,149\n\n\nSep, 2024\n18,313\n\n\n\n\n\n\nTable 2: Accrued Crimes in Mexico City by Month\n\n\n\n\n\n\nShow code\nfig = px.line(months, \n             x='fecha_inicio',\n             y='crimes',\n             hover_data=['fecha_inicio','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City 2019-2024',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Crimes')),\n    )\n\nfig.update_traces(line_color='#7f0000',\n                  line={'width':3},\n                  )\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 3: Time Behavior of Crimes in Mexico City"
  },
  {
    "objectID": "posts/python/crimes-web.html#crimes-by-sex",
    "href": "posts/python/crimes-web.html#crimes-by-sex",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by sex",
    "text": "Crimes by sex\n\n\nShow code\ndf_sex = (\n    df.group_by('sexo')\n        .agg(pl.len().alias('crimes'))\n)\n\n\n\n\nShow code\n# convert to pandas\ndf_sex = df_sex.to_pandas()\n# rename row\ndf_sex['sexo'] = df_sex['sexo'].replace({None:'NA'})\n\n\n\n\nShow code\n(\n    df_sex\n        .sort_values('crimes')\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nsexo\ncrimes\n\n\n\n\nNA\n237,070\n\n\nFemenino\n535,960\n\n\nMasculino\n567,947\n\n\n\n\n\n\nTable 3: Accrued Crimes in Mexico City by Sex\n\n\n\n\n\n\nShow code\nfig = px.bar(df_sex.sort_values('crimes'),\n             y='sexo',\n             x='crimes',\n             orientation='h',\n             hover_data=['sexo','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Sex',\n             template='ggplot2',\n             text='crimes',\n             )\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Sex')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 4: Accrued Crimes in Mexico City by Sex"
  },
  {
    "objectID": "posts/python/crimes-web.html#crimes-by-age",
    "href": "posts/python/crimes-web.html#crimes-by-age",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by age",
    "text": "Crimes by age\n\n\nShow code\ndf_edad = (\n    df.group_by('edad')\n        .agg(pl.len().alias('crimes'))\n        .sort('edad')\n        .drop_nulls()\n)\n\n\n\n\nShow code\n(\n    df_edad\n        .to_pandas()\n        .sample(10)\n        .style.format(\n            {\n            'edad':'{:,.0f}',\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nedad\ncrimes\n\n\n\n\n56\n10,789\n\n\n47\n16,414\n\n\n44\n16,818\n\n\n90\n313\n\n\n85\n789\n\n\n37\n21,022\n\n\n82\n1,181\n\n\n88\n493\n\n\n50\n16,098\n\n\n73\n3,296\n\n\n\n\n\n\nTable 4: Accrued Crimes in Mexico City by Age\n\n\n\n\n\n\nShow code\nfig = px.bar(df_edad,\n             x='edad',\n             y='crimes',\n             orientation='v',\n             hover_data=['edad','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Age',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Age')),\n    )\n\nfig.update_traces(marker_color='#7f0000',)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 5: Crimes in Mexico City by Age Distribution"
  },
  {
    "objectID": "posts/python/crimes-web.html#crimes-by-neighborhood",
    "href": "posts/python/crimes-web.html#crimes-by-neighborhood",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by neighborhood",
    "text": "Crimes by neighborhood\n\n\nShow code\ndf_colonia = (\n    df.group_by('colonia_hecho')\n        .agg(pl.len().alias('crimes'))\n        .top_k(10, by='crimes')\n        .sort('crimes', descending=False)\n)\n\n\n\n\nShow code\n(\n    df_colonia\n        .to_pandas()\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\ncolonia_hecho\ncrimes\n\n\n\n\nPEDREGAL DE SANTO DOMINGO\n10,073\n\n\nJUÁREZ\n11,423\n\n\nBUENAVISTA\n11,749\n\n\nNARVARTE\n11,906\n\n\nAGRÍCOLA ORIENTAL\n12,024\n\n\nMORELOS\n12,518\n\n\nROMA NORTE\n14,878\n\n\nDEL VALLE CENTRO\n17,178\n\n\nDOCTORES\n24,711\n\n\nCENTRO\n40,066\n\n\n\n\n\n\nTable 5: Accrued Crimes in Mexico City by Neighborhood\n\n\n\n\n\n\nShow code\nfig = px.bar(df_colonia,\n             y='colonia_hecho',\n             x='crimes',\n             orientation='h',\n             hover_data=['colonia_hecho','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City - Top 10 Neighborhoods',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Alcaldia')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 6: Crimes in Mexico City by Neighborhood"
  },
  {
    "objectID": "posts/python/crimes-web.html#crimes-by-mayorship",
    "href": "posts/python/crimes-web.html#crimes-by-mayorship",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Crimes by mayorship",
    "text": "Crimes by mayorship\n\n\nShow code\ndf_alcaldia = (\n    df.group_by('alcaldia_hecho')\n        .agg(pl.len().alias('crimes'))\n        .sort('crimes')\n        .filter(pl.col('alcaldia_hecho')!='FUERA DE CDMX')\n)\n\n\n\n\nShow code\n(\n    df_alcaldia\n        .to_pandas()\n        .style.format(\n            {\n            'crimes':'{:,.0f}',\n            }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nalcaldia_hecho\ncrimes\n\n\n\n\nMILPA ALTA\n13,184\n\n\nCUAJIMALPA DE MORELOS\n23,329\n\n\nLA MAGDALENA CONTRERAS\n27,285\n\n\nTLAHUAC\n41,547\n\n\nXOCHIMILCO\n45,924\n\n\nIZTACALCO\n60,427\n\n\nAZCAPOTZALCO\n64,561\n\n\nVENUSTIANO CARRANZA\n78,446\n\n\nMIGUEL HIDALGO\n84,242\n\n\nTLALPAN\n84,382\n\n\nCOYOACAN\n93,953\n\n\nALVARO OBREGON\n94,083\n\n\nBENITO JUAREZ\n101,759\n\n\nGUSTAVO A. MADERO\n139,294\n\n\nCUAUHTEMOC\n188,964\n\n\nIZTAPALAPA\n199,591\n\n\n\n\n\n\nTable 6: Accrued Crimes in Mexico City by Mayorship\n\n\n\n\n\n\nShow code\nfig = px.bar(df_alcaldia,\n             y='alcaldia_hecho',\n             x='crimes',\n             orientation='h',\n             hover_data=['alcaldia_hecho','crimes',],\n             height=500,\n             width=830,\n             title='Crimes in Mexico City by Mayorship',\n             template='ggplot2',)\n\nfig.update_layout(\n    xaxis=dict(title=dict(text='')),\n    yaxis=dict(title=dict(text='Alcaldia')),\n    )\n\nfig.update_traces(marker_color='#7f0000',\n                 texttemplate = \"%{value:,.0f}\",)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 7: Crimes in Mexico City by Mayorship"
  },
  {
    "objectID": "posts/python/crimes-web.html#heat-map-of-crimes",
    "href": "posts/python/crimes-web.html#heat-map-of-crimes",
    "title": "Mexico City Crime According to the Statistics",
    "section": "Heat map of crimes",
    "text": "Heat map of crimes\n\n\nShow code\ndf_map = (\n    df.with_columns(\n        (pl.col('colonia_hecho') + ', ' + pl.col('alcaldia_hecho')).alias('neighborhood')\n    )\n        .filter(pl.col('alcaldia_hecho')!='FUERA DE CDMX')\n        .group_by('neighborhood', maintain_order=True)\n        .agg(latitude=pl.col('latitud').mean(),\n             longitude=pl.col('longitud').mean(),\n             crimes=pl.col('delito').len()\n            )\n)\n\n\n\n\nShow code\nheat_map = df_map.to_pandas()\n\n\n\n\nShow code\n(\n    heat_map.sort_values('crimes', ascending=False)\n        .head(10)\n        .style.format(\n        {\n            'latitude':'{:,.4f}',\n            'longitude':'{:,.4f}',\n            'crimes':'{:,.0f}',\n        }\n        ).hide(axis='index')\n)\n\n\n\n\n\n\n\n\n\n\nneighborhood\nlatitude\nlongitude\ncrimes\n\n\n\n\nCENTRO, CUAUHTEMOC\n19.4327\n-99.1375\n40,042\n\n\nDOCTORES, CUAUHTEMOC\n19.4200\n-99.1486\n24,711\n\n\nDEL VALLE CENTRO, BENITO JUAREZ\n19.3831\n-99.1682\n17,178\n\n\nROMA NORTE, CUAUHTEMOC\n19.4184\n-99.1627\n14,878\n\n\nAGRÍCOLA ORIENTAL, IZTACALCO\n19.3947\n-99.0708\n12,008\n\n\nNARVARTE, BENITO JUAREZ\n19.3930\n-99.1542\n11,906\n\n\nJUÁREZ, CUAUHTEMOC\n19.4268\n-99.1628\n11,408\n\n\nPEDREGAL DE SANTO DOMINGO, COYOACAN\n19.3275\n-99.1677\n10,073\n\n\nPOLANCO, MIGUEL HIDALGO\n19.4335\n-99.1956\n10,019\n\n\nAGRÍCOLA PANTITLAN, IZTACALCO\n19.4104\n-99.0649\n9,662\n\n\n\n\n\n\nTable 7: Crimes in Mexico City Heatmap\n\n\n\n\n\n\nShow code\nfrom folium.plugins import HeatMap\n\nbasemap = folium.Map(location=[19.35, -99.12], zoom_start=10.4)\n\n\n\n\nShow code\nHeatMap(heat_map[['latitude', 'longitude', 'crimes']]).add_to(basemap)\n\n\n&lt;folium.plugins.heat_map.HeatMap at 0x310ecc530&gt;\n\n\n\n\nShow code\nfolium.plugins.Fullscreen().add_to(basemap)\nbasemap\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 8: Mexico City Crime Heatmap by Mayorship\n\n\n\n\n\n\nShow code\n# mexico city crime map\nm = folium.Map(\n    location=[19.35, -99.12],\n    zoom_start=10,\n    control_scale=False,\n)\n# Layers\nCrime = folium.FeatureGroup(name='&lt;u&gt;&lt;b&gt;Place&lt;/b&gt;&lt;/u&gt;', show=True)\nm.add_child(Crime)\n#draw marker with symbol you want at base\nmy_symbol_css_class= \"\"\" &lt;style&gt;\n.fa-mysymbol3:before {\n    font-family: Gill Sans; \n    font-weight: bold;\n    font-size: 11px;\n    color: white;\n    background-color:'';\n    border-radius: 10px; \n    white-space: pre;\n    content: 'P';\n    }\n&lt;/style&gt;    \n        \"\"\"\n# the below is just add above  CSS class to folium root map      \nm.get_root().html.add_child(folium.Element(my_symbol_css_class))\n# then we just create marker and specific your css class in icon like below\nfor i in heat_map.index:\n   html=f\"\"\"\n        &lt;p style=\"font-size: 14px;\"&gt;{heat_map.iloc[i]['neighborhood']}&lt;/font&gt;&lt;/p&gt;\n        &lt;p style=\"font-size: 14px;\"&gt;Total crimes: {heat_map.iloc[i]['crimes']}&lt;/font&gt;&lt;/p&gt;\n        \"\"\"\n   iframe = folium.IFrame(html=html, width=220, height=90)\n   popup = folium.Popup(iframe, max_width=250)\n   folium.Marker(\n        location = [heat_map.iloc[i]['latitude'], heat_map.iloc[i]['longitude']],\n        icon = folium.Icon(color='darkred', prefix='fa', icon='fa-mysymbol3'),\n        popup = popup,\n        tooltip = heat_map.iloc[i]['neighborhood']\n    ).add_to(Crime)\nfolium.plugins.Fullscreen().add_to(m)\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 9: Mexico City Crime Hubs by Neighborhood"
  },
  {
    "objectID": "posts/python/regression_analysis_overview.html",
    "href": "posts/python/regression_analysis_overview.html",
    "title": "Regression Analysis Overview",
    "section": "",
    "text": "Regression searches for relationships among variables. For example, you can want to understand how salaries depend on diverse features among employees of a company, such as experience, education level, role, city of employment, etc.\nData related to each employee represents one observation. The presumption is that salary depends on experience, education, role, and city.\nBy other hand, you may want to establish the relationship among housing prices on a particular area and number of bedrooms, distance to the city center, among other features.\nIn regression analysis, we consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that at least one of the features depends on the others, you’ll need to establish a relationship among them. In scientific jargon, you need to find a function that maps some features to others sufficiently well.\nCommonly, we call the dependent feature dependent variable or response (generally denoted as y), and the independent features are called independent variables, inputs, or features.\nRegression problems usually works with continuous variables. Features, however,can be continuous, discrete, or even categorical.\nFinally, regression analysis is very important when you want to forecast a response using a new set of features. For example, you may want to predict gasoline consumption of a household for the next period given its price, number of residents in that household, car model, etc."
  },
  {
    "objectID": "posts/python/regression_analysis_overview.html#contact",
    "href": "posts/python/regression_analysis_overview.html#contact",
    "title": "Regression Analysis Overview",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/water collection system.html",
    "href": "posts/python/water collection system.html",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "",
    "text": "Rainwater harvesting systems are a prominent water collection method in Mexico, especially in areas facing water scarcity.\nThese systems, called “Sistemas de Captación de Agua de Lluvia” (SCALL) offer several benefits:"
  },
  {
    "objectID": "posts/python/water collection system.html#contact",
    "href": "posts/python/water collection system.html#contact",
    "title": "Water Collection System in Mexico City - 2022",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/plotting-with-holoviews.html#overview",
    "href": "posts/python/plotting-with-holoviews.html#overview",
    "title": "Creating Charts with HoloViews",
    "section": "Overview",
    "text": "Overview\nFor data analytics, the magic lies in transforming raw numbers into visuals that reveal hidden patterns and trends. But the process of creating these visualizations can often be cumbersome, requiring extensive coding and customization. Here’s where HoloViews steps in, offering a refreshing approach to data visualization in Python."
  },
  {
    "objectID": "posts/python/plotting-with-holoviews.html#what-is-holoviews",
    "href": "posts/python/plotting-with-holoviews.html#what-is-holoviews",
    "title": "Creating Charts with HoloViews",
    "section": "What is HoloViews?",
    "text": "What is HoloViews?\nHoloViews is an open-source Python library designed to streamline data analysis and visualization. It departs from the traditional method of meticulously crafting plots line by line. Instead, HoloViews focuses on a declarative approach, where you describe your data and desired visualization, and it takes care of the intricate details. This allows you to express your ideas with concise code, freeing you to delve deeper into your story-telling."
  },
  {
    "objectID": "posts/python/plotting-with-holoviews.html#why-choose-holoviews",
    "href": "posts/python/plotting-with-holoviews.html#why-choose-holoviews",
    "title": "Creating Charts with HoloViews",
    "section": "Why Choose HoloViews?",
    "text": "Why Choose HoloViews?\nSeveral factors make HoloViews an attractive option for data visualization:\n\nSimplicity\n\nHoloViews boasts a user-friendly syntax, enabling you to create complex visualizations with minimal code. This focus on brevity empowers you to iterate quickly and explore your data efficiently.\n\nFlexibility\n\nHoloViews integrates seamlessly with popular data structures like NumPy and Pandas, effortlessly handling your data. Additionally, it plays well with different plotting backends like Bokeh, Plotly and Matplotlib, giving you control over the final look and feel of your visualizations.\n\nInteractivity\n\nHoloViews visualizations are not static images. They can be interactive, allowing users to zoom, pan, and explore the data from various angles. This interactivity fosters deeper engagement and a richer understanding of the information.\n\nComprehensiveness\n\nThe HoloViews ecosystem extends beyond the core library. It encompasses projects like hvPlot for quick visualizations and GeoViews for crafting geographical visualizations. This suite of tools caters to a wide range of data exploration needs."
  },
  {
    "objectID": "posts/python/plotting-with-holoviews.html#environment-settings",
    "href": "posts/python/plotting-with-holoviews.html#environment-settings",
    "title": "Creating Charts with HoloViews",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nCode\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom itables import init_notebook_mode\ninit_notebook_mode(all_interactive=True)\nimport holoviews as hv\nhv.extension('bokeh')\n\n\n\n\n\n\n\n\n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nThis is the init_notebook_mode cell from ITables v2.2.3\n(you should not see this message - is your notebook trusted?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\nCode\n# Get data\nurl = 'https://raw.githubusercontent.com/shoukewei/data/main/data-pydm/gdp_china_outlier_treated.csv'\ndf = pd.read_csv(url)\n\n\n\n\nCode\n# Dataset preview\nfrom itables import show\n\nshow(df, lengthMenu=[10, 25, 50, 100,])\n\n\n\n\n    \n      \n      prov\n      gdpr\n      year\n      gdp\n      pop\n      finv\n      trade\n      fexpen\n      uinc\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.2.3 from the init_notebook_mode cell...\n(need help?)\n\n\n\n\n\n\n\n\nCode\n# Change column name\ndf = df.rename(columns={'prov':'Province',})\n\n\n\n\nCode\n# Convert to holviews dataset\nhd = hv.Dataset(df)"
  },
  {
    "objectID": "posts/python/plotting-with-holoviews.html#creating-charts",
    "href": "posts/python/plotting-with-holoviews.html#creating-charts",
    "title": "Creating Charts with HoloViews",
    "section": "Creating Charts",
    "text": "Creating Charts\n\n\nCode\n# Customization specs\ngrid_style = {'grid_line_color': 'white',\n              'grid_line_width': 2.,\n             }\n\n# Create chart\ncurves_app = hd.to(hv.Bars, kdims=['year'], vdims=['gdp'], groupby='Province',)\n\n# Chart options\ncurves_app.opts(height=500,\n                width=650,\n                xlabel='Year',\n                tools=['hover'],\n                ylabel='GDP',\n                xrotation=45,\n                toolbar=None, #above, below, left, right\n                fill_color='#1c2841',\n                line_color='black',\n                bgcolor='#f6f6f6',\n                show_grid=True,\n                gridstyle=grid_style,\n               )\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteNote\n\n\n\nYou can select a Province from the filter to get its correspondent chart."
  },
  {
    "objectID": "posts/python/plotting-with-holoviews.html#conclusions",
    "href": "posts/python/plotting-with-holoviews.html#conclusions",
    "title": "Creating Charts with HoloViews",
    "section": "Conclusions",
    "text": "Conclusions\nAs you gain experience with HoloViews, you can delve into its more advanced features.\nHoloViews allows for extensive customization, enabling you to tailor your visualizations to perfectly suit your needs and branding.\nFurthermore, HoloViews integrates well with other data science libraries within the Python ecosystem, fostering a powerful and cohesive environment for data exploration and analysis."
  },
  {
    "objectID": "posts/python/plotting-with-holoviews.html#contact",
    "href": "posts/python/plotting-with-holoviews.html#contact",
    "title": "Creating Charts with HoloViews",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/lego_history.html",
    "href": "posts/python/lego_history.html",
    "title": "Exploring Lego Toys with Polars",
    "section": "",
    "text": "Figure 1: Lego Toys"
  },
  {
    "objectID": "posts/python/lego_history.html#reading-datasets",
    "href": "posts/python/lego_history.html#reading-datasets",
    "title": "Exploring Lego Toys with Polars",
    "section": "Reading datasets",
    "text": "Reading datasets\nA comprehensive database of lego blocks is provided by Rebrickable.\nThe data is available as csv file and the schema is shown below\n\n\n\n\nDatabase schema\n\nLet us start by reading in the colors data to get a sense of the diversity of lego sets!"
  },
  {
    "objectID": "posts/python/restaurants_sql.html",
    "href": "posts/python/restaurants_sql.html",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "",
    "text": "Figure 1: Restaurants Report"
  },
  {
    "objectID": "posts/python/restaurants_sql.html#project-phases",
    "href": "posts/python/restaurants_sql.html#project-phases",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Project phases",
    "text": "Project phases\n\nData Loading\n\nDuckDB: Load large datasets directly into DuckDB for efficient in-memory operations and query execution.\nPolars: Utilize Polars’ fast data loading capabilities, especially for CSV and Parquet files, to quickly ingest data into its DataFrame structure.\n\nData Transformation\n\nPolars: Perform data cleaning, filtering, aggregation, and other transformations within the Polars DataFrame efficiently.\nDuckDB: Execute complex SQL queries directly on the in-memory data within DuckDB for advanced data manipulation.\n\nData Visualization\n\nPolars: Use Polars’ built-in plotting capabilities for quick exploratory visualizations.\nPlotly: Leverage Plotly’s extensive library of interactive and customizable plots for in-depth analysis and presentation.\n\nPerformance Optimization\n\nMinimize Data Transfers: Avoid unnecessary data transfers between tools. For example, if possible, perform data transformations within DuckDB and then directly visualize results using Plotly.\nUtilize Parallel Processing: Leverage the parallel processing capabilities of both DuckDB and Polars to speed up data processing tasks.\nOptimize Queries: Write efficient SQL queries and use appropriate data types to maximize DuckDB’s performance.\nCaching: Cache intermediate results to avoid redundant computations."
  },
  {
    "objectID": "posts/python/restaurants_sql.html#environment-settings",
    "href": "posts/python/restaurants_sql.html#environment-settings",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Environment settings",
    "text": "Environment settings\n\n\nCode\nimport numpy as np\nimport polars as pl\nimport random\nimport duckdb as db\nimport plotly.express as px"
  },
  {
    "objectID": "posts/python/restaurants_sql.html#create-dummy-dataset",
    "href": "posts/python/restaurants_sql.html#create-dummy-dataset",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Create dummy dataset",
    "text": "Create dummy dataset\n\n\nCode\ndef generate_dummy_data(num_rows=10_000):\n  \"\"\"\n  Generates dummy data for restaurants with name, rating_count, cost, city, and cuisine.\n  Args:\n    num_rows: Number of rows to generate.\n  Returns:\n    A list of dictionaries, where each dictionary represents a restaurant with the specified fields.\n  \"\"\"\n\n  data = []\n  names = [\"The Cozy Nook\", \"Spice & Bloom\", \"The Golden Wok\", \"Midnight Diner\", \"Ocean Breeze\", \n           \"Cafe Delight\", \"The Burger Joint\", \"Pizza Palace\", \"Taste of Italy\", \"French Delights\",\n           \"The Curry House\", \"Sushi Corner\", \"Greek Gyros\", \"Taco Town\", \"The BBQ Shack\"]\n  cities = [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Rome\", \n            \"Berlin\", \"Sydney\", \"Madrid\", \"Amsterdam\", \"Lisbon\"]\n  cuisines = [\"Italian\", \"Mexican\", \"Indian\", \"Chinese\", \"Japanese\", \n              \"American\", \"French\", \"Thai\", \"Greek\", \"Spanish\"]\n\n  for _ in range(num_rows):\n    restaurant = {\n        'name': random.choice(names),\n        'rating_count': random.randint(100, 5000),\n        'cost': random.uniform(10.0, 100.0),\n        'city': random.choice(cities),\n        'cuisine': random.choice(cuisines),\n        'rating': random.randint(0, 5)\n    }\n    data.append(restaurant)\n\n  return data\n\n\n\n\nCode\n# Generate rows of dummy data\ndummy_restaurants = generate_dummy_data()"
  },
  {
    "objectID": "posts/python/restaurants_sql.html#create-dataframe",
    "href": "posts/python/restaurants_sql.html#create-dataframe",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Create dataframe",
    "text": "Create dataframe\n\n\nCode\n# Create dataframe\nrestaurants = pl.DataFrame(dummy_restaurants)\n\n\n\n\nCode\n# Show dataframe\n(\n    restaurants\n        .to_pandas()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n3,408\n$37.75\nLisbon\nFrench\n2\n\n\nGreek Gyros\n1,484\n$93.44\nParis\nItalian\n0\n\n\nOcean Breeze\n2,792\n$32.26\nBerlin\nMexican\n5\n\n\nGreek Gyros\n2,038\n$20.78\nLondon\nSpanish\n5\n\n\nThe Curry House\n2,321\n$94.79\nLisbon\nFrench\n1\n\n\n\n\n\n\nTable 1: Restaurants dataset"
  },
  {
    "objectID": "posts/python/restaurants_sql.html#connect-to-database",
    "href": "posts/python/restaurants_sql.html#connect-to-database",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Connect to database",
    "text": "Connect to database\n\n\nCode\n# connect to database\nconn = db.connect('my_database.db')\n#conn = db.connect('restaurants.db')\n\n\n\n\nCode\n# retrieve data from table\nres = conn.sql('select * from restaurants limit 5')\n\n\n\n\nCode\n# Show table\n(\n    res.df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nGreek Gyros\n770\n$68.39\nNew York\nFrench\n1\n\n\nTaste of Italy\n4,420\n$88.23\nAmsterdam\nChinese\n0\n\n\nMidnight Diner\n2,155\n$12.97\nLisbon\nMexican\n1\n\n\nTaste of Italy\n3,375\n$52.79\nSydney\nChinese\n1\n\n\n\n\n\n\nTable 2: Restaurants table from database"
  },
  {
    "objectID": "posts/python/restaurants_sql.html#queries",
    "href": "posts/python/restaurants_sql.html#queries",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Queries",
    "text": "Queries\n\nWhich restaurant of London is visited by the least number of people?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where city = 'London' and rating_count = (select min(rating_count)\n                                            from restaurants \n                                            where city = 'London')\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nGreek Gyros\n101\n$13.82\nLondon\nIndian\n2\n\n\n\n\n\n\nTable 3: Query 1 result table\n\n\n\n\n\n\nWhich restaurant has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where cost*rating_count = (select max(cost*rating_count)\n                                from restaurants)\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nOcean Breeze\n4,969\n$99.92\nMadrid\nChinese\n4\n\n\n\n\n\n\nTable 4: Query 2 result table\n\n\n\n\n\n\nHow many restaurants are having a rating more than the average rating?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where rating &gt; (select avg(rating)\n                    from restaurants)\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Golden Wok\n1,477\n$33.62\nBerlin\nAmerican\n5\n\n\nThe Burger Joint\n3,631\n$25.36\nLondon\nAmerican\n4\n\n\nThe Curry House\n3,033\n$56.05\nAmsterdam\nItalian\n3\n\n\nThe Burger Joint\n4,656\n$19.38\nAmsterdam\nAmerican\n3\n\n\nPizza Palace\n1,862\n$71.95\nParis\nJapanese\n5\n\n\n\n\n\n\nTable 5: Query 3 result table\n\n\n\n\n\n\nWhich restaurant of New York has generated the most revenue?\n\n\nCode\n(\n    conn.sql('''\n    select * from restaurants\n    where city = 'New York' and cost*rating_count = (select max(cost*rating_count)\n                                                    from restaurants \n                                                    where city = 'New York')\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrating_count\ncost\ncity\ncuisine\nrating\n\n\n\n\nThe Curry House\n4,955\n$97.92\nNew York\nJapanese\n5\n\n\n\n\n\n\nTable 6: Query 4 result table\n\n\n\n\n\n\nWhich restaurant chain has the maximum number of restaurants?\n\n\nCode\n(\n    conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'rating_count': '{:,.0f}', 'cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nno_of_chains\n\n\n\n\nThe Burger Joint\n721\n\n\nPizza Palace\n703\n\n\nGreek Gyros\n696\n\n\nCafe Delight\n692\n\n\nFrench Delights\n681\n\n\nThe BBQ Shack\n671\n\n\nThe Golden Wok\n667\n\n\nOcean Breeze\n665\n\n\nSpice & Bloom\n665\n\n\nMidnight Diner\n657\n\n\n\n\n\n\nTable 7: Query 5 result table\n\n\n\n\n\n\nCode\nres_chains = conn.sql('''\n    select name, count(name) as no_of_chains\n    from restaurants\n    group by name\n    order by no_of_chains DESC\n    limit 10\n''').pl()\n\nfig = px.bar(res_chains, \n             x='no_of_chains',\n             y='name',\n             orientation='h',\n             hover_data=['no_of_chains', 'name',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Chains by Restaurant',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='outside',\n                  marker_color='rgb(55, 83, 109)',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,)\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Chains', yaxis_title='Restaurant')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 2: Restaurant Chains\n\n\n\n\n\n\nWhich restaurant chain has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select name, sum(rating_count * cost) as revenue\n    from restaurants\n    group by name\n    order by revenue DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'revenue': '{:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\nname\nrevenue\n\n\n\n\nThe Burger Joint\n108,820,424.64\n\n\nPizza Palace\n100,382,853.92\n\n\nCafe Delight\n98,037,063.93\n\n\nGreek Gyros\n96,403,445.25\n\n\nThe BBQ Shack\n96,286,414.02\n\n\nOcean Breeze\n95,664,612.77\n\n\nThe Golden Wok\n94,860,125.75\n\n\nSpice & Bloom\n91,824,854.56\n\n\nFrench Delights\n91,236,701.38\n\n\nMidnight Diner\n91,170,162.30\n\n\n\n\n\n\nTable 8: Query 6 result table\n\n\n\n\n\n\nCode\nrev = conn.sql('''\n            select name, sum(rating_count * cost) as revenue\n            from restaurants\n            group by name\n            order by revenue DESC\n            limit 10\n            ''').pl()\n\nfig = px.bar(rev, \n             x='revenue',\n             y='name',\n             orientation='h',\n             hover_data=['revenue', 'name',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Revenue by Restaurant',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='inside',\n                  marker_color='#004700',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,\n                  hovertemplate='&lt;br&gt;'.join([\n                      'Restaurant: %{y}',\n                      'Revenue: %{x}',\n                  ]),\n                 )\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Revenue', yaxis_title='Restaurant',\n                 xaxis_tickprefix='$', xaxis_tickformat=',.2f')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 3: Revenue by Restaurant\n\n\n\n\n\n\nWhich city has the maximum number of restaurants?\n\n\nCode\n(\n    conn.sql('''\n    select city, count(*) as no_of_restaurants\n    from restaurants\n    group by city\n    order by no_of_restaurants DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'no_of_restaurants': '{:,.0f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nno_of_restaurants\n\n\n\n\nTokyo\n1,071\n\n\nAmsterdam\n1,038\n\n\nLisbon\n1,005\n\n\nMadrid\n1,003\n\n\nLondon\n993\n\n\nParis\n992\n\n\nRome\n979\n\n\nBerlin\n977\n\n\nNew York\n971\n\n\nSydney\n971\n\n\n\n\n\n\nTable 9: Query 7 result table\n\n\n\n\n\n\nWhich city has generated maximum revenue?\n\n\nCode\n(\n    conn.sql('''\n    select city, sum(rating_count * cost) as revenue\n    from restaurants\n    group by city\n    order by revenue DESC\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'revenue': '${:,.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\nrevenue\n\n\n\n\nAmsterdam\n$148,839,878.62\n\n\nTokyo\n$148,035,421.32\n\n\nMadrid\n$141,487,618.97\n\n\nParis\n$141,219,374.56\n\n\nLondon\n$140,876,613.54\n\n\nRome\n$139,622,129.63\n\n\nNew York\n$138,621,609.28\n\n\nLisbon\n$136,814,247.27\n\n\nBerlin\n$136,434,163.25\n\n\nSydney\n$131,656,513.97\n\n\n\n\n\n\nTable 10: Query 8 result table\n\n\n\n\n\n\nCode\ncities = conn.sql('''\n                select city, sum(rating_count * cost) as revenue\n                from restaurants\n                group by city\n                order by revenue DESC\n                limit 10\n                ''').pl()\n\nfig = px.bar(cities, \n             x='revenue',\n             y='city',\n             orientation='h',\n             hover_data=['revenue', 'city',],\n             height=600,\n             width=940,\n             text_auto=True,\n             title='Revenue by City',)\nfig.update_traces(textfont_size=12, \n                  textangle=0,\n                  textposition='inside',\n                  marker_color='#880808',\n                  marker_line_color='#000000',\n                  marker_line_width=1.5,\n                  opacity=0.8,\n                  hovertemplate='&lt;br&gt;'.join([\n                      'City: %{y}',\n                      'Revenue: %{x}',\n                  ]),\n                 )\nfig.update_layout(yaxis=dict(autorange='reversed'), xaxis_title='Revenue', yaxis_title='City',\n                 xaxis_tickprefix='$', xaxis_tickformat=',.2f')\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 4: Revenue by City\n\n\n\n\n\n\nList 10 least expensive cuisines\n\n\nCode\n(\n    conn.sql('''\n    select cuisine, avg(cost) as avg_cost\n    from restaurants\n    group by cuisine\n    order by avg_cost asc\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncuisine\navg_cost\n\n\n\n\nIndian\n$53.92\n\n\nAmerican\n$53.96\n\n\nItalian\n$54.61\n\n\nThai\n$54.64\n\n\nFrench\n$54.65\n\n\nMexican\n$55.47\n\n\nSpanish\n$55.54\n\n\nJapanese\n$55.55\n\n\nGreek\n$55.77\n\n\nChinese\n$56.26\n\n\n\n\n\n\nTable 11: Query 9 result table\n\n\n\n\n\n\nList 10 most expensive cuisines\n\n\nCode\n(\n    conn.sql('''\n    select cuisine, avg(cost) as avg_cost\n    from restaurants\n    group by cuisine\n    order by avg_cost desc\n    limit 10\n    ''')\n        .df()\n        .head(10)\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncuisine\navg_cost\n\n\n\n\nChinese\n$56.26\n\n\nGreek\n$55.77\n\n\nJapanese\n$55.55\n\n\nSpanish\n$55.54\n\n\nMexican\n$55.47\n\n\nFrench\n$54.65\n\n\nThai\n$54.64\n\n\nItalian\n$54.61\n\n\nAmerican\n$53.96\n\n\nIndian\n$53.92\n\n\n\n\n\n\nTable 12: Query 10 result table\n\n\n\n\n\n\nWhat city has Mexican food as the most popular cuisine?\n\n\nCode\n(\n    conn.sql('''\n    select city, avg(cost) avg_cost, count(*) as restaurants\n    from restaurants\n    where cuisine = 'Mexican'\n    group by city\n    order by restaurants desc\n    ''')\n        .df()\n        .head()\n        .style\n        .hide()    \n        .format({'avg_cost': '${:.2f}'})\n)\n\n\n\n\n\n\n\n\n\n\ncity\navg_cost\nrestaurants\n\n\n\n\nLondon\n$52.30\n112\n\n\nTokyo\n$48.83\n111\n\n\nRome\n$57.11\n108\n\n\nAmsterdam\n$60.67\n108\n\n\nLisbon\n$56.35\n101\n\n\n\n\n\n\nTable 13: Query 11 result table\n\n\n\n\n\n\nCode\n# close connection\nconn.close()"
  },
  {
    "objectID": "posts/python/restaurants_sql.html#conclusions",
    "href": "posts/python/restaurants_sql.html#conclusions",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Conclusions",
    "text": "Conclusions\nThis project highlights the value of combining the strengths of SQL and Python for data-intensive tasks, providing a robust and efficient solution for a wide range of data analysis challenges.\nBy carefully considering the interaction between DuckDB, Polars, and Plotly, you can create a powerful and efficient data analysis and visualization pipeline.\nThis approach can lead organizations to unlock valuable insights from their data more quickly and effectively, driving data-driven decision-making."
  },
  {
    "objectID": "posts/python/restaurants_sql.html#contact",
    "href": "posts/python/restaurants_sql.html#contact",
    "title": "Data Alchemy, SQL Analysis within Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/working_with_json_files.html#overview",
    "href": "posts/python/working_with_json_files.html#overview",
    "title": "Manipulating JSON Files with Python",
    "section": "Overview",
    "text": "Overview\nDiving into the world of data science and machine learning, one of the fundamental skills you’ll encounter is the art of reading data.\nIf you have already some experience with it, you’re probably familiar with JSON (JavaScript Object Notation) files.\nThink of how NoSQL databases like MongoDB love to store data in JSON, or how REST APIs often respond in the same format.\nHowever, JSON, while perfect for storage and exchange, isn’t quite ready for in-depth analysis in its raw form.\nThis is where we transform it into something more analytically friendly – a tabular format.\n\nEnvironment settings\n\n\nShow code\n# Create a json file\n\nsimple_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n}\n\nsimple_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n}\n\nsimple_json_list = [\n    simple_json, \n    simple_json_2\n\n]\n\n\n\n\nShow code\nimport pandas as pd\n\n\n\n\nManipulating json files\n\n\nShow code\npd.json_normalize(simple_json)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nDavid\nLondon\n80000\n\n\n\n\n\n\n\n\n\nShow code\npd.json_normalize(simple_json_2)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nTaylor\nChicago\n120000\n\n\n\n\n\n\n\n\n\nShow code\npd.json_normalize(simple_json_list)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\n\n\n\n\n0\nDavid\nLondon\n80000\n\n\n1\nTaylor\nChicago\n120000\n\n\n\n\n\n\n\nIn case we just want to transform some specific fields into a tabular pandas DataFrame, the json_normalize() command does not allow us to choose what fields to transform.\nTherefore, a small preprocessing of the JSON should be performed where we filter just those columns of interest.\n\n\nShow code\n# Fields to include\nfields = ['name', 'city']\n\n# Filter the JSON data\nfiltered_json_list = [{key: value for key, value in item.items() if key in fields}\n                      for item in simple_json_list]\n\npd.json_normalize(filtered_json_list)\n\n\n\n\n\n\n\n\n\nname\ncity\n\n\n\n\n0\nDavid\nLondon\n\n\n1\nTaylor\nChicago\n\n\n\n\n\n\n\nWhen dealing with multiple-leveled JSONs we find ourselves with nested JSONs within different levels.\nThe procedure is the same as before, but in this case, we can choose how many levels we want to transform.\nBy default, the command will always expand all levels and generate new columns containing the concatenated name of all the nested levels.\n\n\nShow code\n# Create a nested json file\n\nmultiple_levels_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n    'skills': {\n        'python': 'advanced',\n        'SQL': 'advanced',\n        'GCP': 'mid'\n    },\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":True,\n        \"data analyst\":False,\n        }\n    }\n\nmultiple_levels_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n    'skills': {\n        'python': 'mid',\n        'SQL': 'advanced',\n        'GCP': 'beginner'\n    },\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":False,\n        \"data analyst\":True\n        }\n    }\n\nmultiple_level_json_list = [\n    multiple_levels_json, \n    multiple_levels_json_2\n\n]\n\n\nWe would get the following table with 3 columns under the field skills:\n\nskills.python\nskills.SQL\nskills.GCP\n\nand 4 columns under the field roles\n\nroles.project manager\nroles.data engineer\nroles.data scientist\nroles.data analyst\n\nHowever, imagine we just want to transform our top level.\nWe can do so by specifically defining the parameter max_level to 0\n\n\nShow code\npd.json_normalize(multiple_level_json_list, max_level = 0)\n\n\n\n\n\n\n\n\n\nname\ncity\nincome\nskills\nroles\n\n\n\n\n0\nDavid\nLondon\n80000\n{'python': 'advanced', 'SQL': 'advanced', 'GCP...\n{'project manager': False, 'data engineer': Fa...\n\n\n1\nTaylor\nChicago\n120000\n{'python': 'mid', 'SQL': 'advanced', 'GCP': 'b...\n{'project manager': False, 'data engineer': Fa...\n\n\n\n\n\n\n\n\n\nShow code\nnested_json = {\n    'name': 'David',\n    'city': 'London',\n    'income': 80000,\n    'skills': [\"python\", \"SQL\",\"GCP\"],\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":True,\n        \"data analyst\":False,\n        }\n    }\n\nnested_json_2 = {\n    'name': 'Taylor',\n    'city': 'Chicago',\n    'income': 120000,\n    'skills': [\"python\", \"SQL\",\"PowerBI\",\"Looker\"],\n    'roles': {\n        \"project manager\":False,\n        \"data engineer\":False,\n        \"data scientist\":False,\n        \"data analyst\":True\n        }\n    }\n\nnested_json_list = [\n    nested_json, \n    nested_json_2\n\n]\n\n\n\n\nShow code\npd.json_normalize(nested_json_list, record_path=['skills'], meta=['name','city'])\n\n\n\n\n\n\n\n\n\n0\nname\ncity\n\n\n\n\n0\npython\nDavid\nLondon\n\n\n1\nSQL\nDavid\nLondon\n\n\n2\nGCP\nDavid\nLondon\n\n\n3\npython\nTaylor\nChicago\n\n\n4\nSQL\nTaylor\nChicago\n\n\n5\nPowerBI\nTaylor\nChicago\n\n\n6\nLooker\nTaylor\nChicago"
  },
  {
    "objectID": "posts/python/working_with_json_files.html#conclusion",
    "href": "posts/python/working_with_json_files.html#conclusion",
    "title": "Manipulating JSON Files with Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, the transformation of JSON data into CSV files using Python’s Pandas library is easy and effective.\nJSON is still the most common format in modern data storage and exchange, notably in NoSQL databases and REST APIs. However, it presents some important analytic challenges when dealing with data in its raw format."
  },
  {
    "objectID": "posts/python/working_with_json_files.html#references",
    "href": "posts/python/working_with_json_files.html#references",
    "title": "Manipulating JSON Files with Python",
    "section": "References",
    "text": "References\n\nFerrer, J. (2024) Converting JSONs to Pandas DataFrames: Parsing Them the Right Way in Data Science"
  },
  {
    "objectID": "posts/python/working_with_json_files.html#contact",
    "href": "posts/python/working_with_json_files.html#contact",
    "title": "Manipulating JSON Files with Python",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/spotipy.html#what-is-spotipy",
    "href": "posts/python/spotipy.html#what-is-spotipy",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "What is Spotipy?",
    "text": "What is Spotipy?\nSpotipy is a lightweight, easy-to-use Python library that simplifies interacting with the Spotify API. It handles the nitty-gritty details of authentication, making requests, and parsing responses, letting you focus on the fun part: exploring and manipulating music data."
  },
  {
    "objectID": "posts/python/spotipy.html#why-use-spotipy",
    "href": "posts/python/spotipy.html#why-use-spotipy",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Why Use Spotipy?",
    "text": "Why Use Spotipy?\n\nAccess a Universe of Music Data\nRetrieve information about artists, albums, tracks, playlists, audio features (like danceability and energy), and much more.\nAutomate Tasks\nCreate scripts to manage your playlists, discover new music based on your taste, analyze your listening habits, or even build your own music recommendation system.\nIntegrate with Other Tools\nCombine Spotipy with other Python libraries like Pandas for data analysis, Matplotlib for visualization, or Flask for building web applications.\nBuild Music-Focused Apps\nDevelop custom applications that leverage Spotify’s vast music catalog and user data. Think of creating personalized radio stations, music visualizations, or tools for music discovery.\nEasy to Learn and Use\nSpotipy’s well-documented API and intuitive design make it accessible to both beginners and experienced Python developers."
  },
  {
    "objectID": "posts/python/spotipy.html#key-features-and-capabilities",
    "href": "posts/python/spotipy.html#key-features-and-capabilities",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Key Features and Capabilities",
    "text": "Key Features and Capabilities\n\nAuthentication\nHandles the OAuth 2.0 flow for authenticating users and obtaining access tokens, allowing you to access both public and private data.\nSearching\nSearch for artists, tracks, albums, and playlists using keywords.\nRetrieving Information\nFetch detailed information about specific artists, tracks, albums, and playlists, including metadata, audio features, and related artists.\nPlaylist Management\nCreate, modify, and manage playlists, including adding and removing tracks.\nUser Profile Access\nAccess user profile information, including listening history, followed artists, and saved tracks.\nAudio Features Analysis\nRetrieve audio features for tracks, such as danceability, energy, tempo, valence, and more. This data can be used to analyze music and build interesting applications.\n\n\nEnvironment settings\n\n\nShow code\nimport json\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\nimport pandas as pd\n\n\n\n\nShow code\n# Token gotten from spotify api\nfilename = '../APIs/credentials.json'\n# read json file\nwith open(filename) as f:\n    keys = json.load(f)\n# read credentials\nclientID = keys['spotipy_client_id']\nclientSecret = keys['spotipy_client_secret']\n\n\n\n\nShow code\nclient_credential_manager = SpotifyClientCredentials(client_id=clientID, client_secret=clientSecret)\nsp = spotipy.Spotify(client_credentials_manager=client_credential_manager)\n\n\n\n\nArtist\n\n\nShow code\nresults = sp.artist_albums('spotify:artist:06HL4z0CvFAxyc27GXpf02', album_type='album')\nalbums = results['items']\nwhile results['next']:\n    results = sp.next(results)\n    albums.extend(results['items'])\n\nfor album in albums:\n    print(album['name'])\n\n\n1989 (Taylor's Version) [Deluxe]\n1989 (Taylor's Version)\nSpeak Now (Taylor's Version)\nMidnights (The Til Dawn Edition)\nMidnights (3am Edition)\nMidnights\nRed (Taylor's Version)\nFearless (Taylor's Version)\nevermore (deluxe version)\nevermore\nfolklore: the long pond studio sessions (from the Disney+ special) [deluxe edition]\nfolklore (deluxe version)\nfolklore\nLover\nreputation\nreputation Stadium Tour Surprise Song Playlist\n1989 (Deluxe)\n1989\nRed (Deluxe Edition)\nSpeak Now World Tour Live\nSpeak Now\nSpeak Now (Deluxe Package)\nFearless (Platinum Edition)\nFearless (International Version)\nLive From Clear Channel Stripped 2008\nTaylor Swift\n\n\n\n\nAlbum\n\n\nShow code\nalbum = sp.album_tracks('spotify:album:6MeLjaERUK6fJ58YZpPlyC')\nlista_canciones = album['items']\n\nfor cancion in lista_canciones:\n    print(cancion['name'])\n\n\nY Nos Dieron las Diez\nConductores Suicidas\nYo Quiero Ser una Chica Almodovar\nA la Orilla de la Chimenea\nTodos Menos Tú\nLa del Pirata Cojo\nLa Canción de las Noches Perdidas\nLos Cuentos Que Yo Cuento\nPeor para el Sol\nAmor Se Llama el Juego\nPastillas para No Soñar"
  },
  {
    "objectID": "posts/python/spotipy.html#search",
    "href": "posts/python/spotipy.html#search",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Search",
    "text": "Search\n\n\nShow code\nartist_name = []\ntrack_name = []\npopularity = []\ntrack_id = []\n\nfor i in range(0,1_000,50):\n    track_results = sp.search(q='year:2020', type='track', limit=50, offset=i)\n    for i, t in enumerate(track_results['tracks']['items']):\n        artist_name.append(t['artists'][0]['name'])\n        track_name.append(t['name'])\n        track_id.append(t['id'])\n        popularity.append(t['popularity'])\n\n\n\n\nShow code\ntrack_dataframe = pd.DataFrame(\n        {'artist_name':artist_name,\n         'track_name':track_name,\n         'track_id':track_id,\n         'popularity':popularity}\n)\n\n\n\n\nShow code\ntrack_dataframe\n\n\n\n\n\n\n\n\n\nartist_name\ntrack_name\ntrack_id\npopularity\n\n\n\n\n0\nDream Supplier\nClean Baby Sleep White Noise (Loopable)\n0zirWZTcXBBwGsevrsIpvT\n94\n\n\n1\nHotel Ugly\nShut up My Moms Calling\n3hxIUxnT27p5WcmjGUXNwx\n90\n\n\n2\nBrent Faiyaz\nClouded\n2J6OF7CkpdQGSfm1wdclqn\n86\n\n\n3\n21 Savage\nGlock In My Lap\n6pcywuOeGGWeOQzdUyti6k\n87\n\n\n4\nSteve Lacy\nInfrunami\n0f8eRy9A0n6zXpKSHSCAEp\n86\n\n\n...\n...\n...\n...\n...\n\n\n995\nEdith Whiskers\nHome\n18V1UiYRvWYwn01CRDbbuR\n73\n\n\n996\nDuke Dumont\nOcean Drive\n4b93D55xv3YCH5mT4p6HPn\n74\n\n\n997\nBad Bunny\nTE DESEO LO MEJOR\n23XjN1s3DZC8Q9ZwuorYY4\n73\n\n\n998\nJunior H\nNo Me Pesa\n4YU704KDCv4tyE6qQxliY3\n69\n\n\n999\nDestroy Lonely\nIn The Air\n2eJBpNlTTPatjec4SDQvuo\n64\n\n\n\n\n1000 rows × 4 columns"
  },
  {
    "objectID": "posts/python/spotipy.html#download-music-songs-from-spotify",
    "href": "posts/python/spotipy.html#download-music-songs-from-spotify",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Download music songs from Spotify",
    "text": "Download music songs from Spotify\n\n\nShow code\nimport spotdl\nimport spotify_dl\n\n\n\n\nShow code\n%%bash\ncd ~/Downloads;\nspotify_dl -l 'https://open.spotify.com/track/3D0bXrSv7O73vOaGOG8J9c?si=2eb4c15e2d7c4adf' \\\n&gt; /dev/null;\n\n\n\n\nShow code\n%%bash\ncd ~/Downloads;\nspotdl 'https://open.spotify.com/track/3D0bXrSv7O73vOaGOG8J9c?si=2eb4c15e2d7c4adf' \\\n&gt; /dev/null;"
  },
  {
    "objectID": "posts/python/spotipy.html#conclusions",
    "href": "posts/python/spotipy.html#conclusions",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Conclusions",
    "text": "Conclusions\nSpotipy is a valuable tool for anyone interested in working with Spotify’s music data. It bridges the gap between the Spotify API and the Python programming language, enabling developers to create innovative and data-driven music experiences. While there are considerations related to API limitations and authentication, the benefits of using Spotipy generally outweigh the challenges.\nConsiderations and Limitations\n\nAPI Rate Limits: Spotify’s API has rate limits, which can restrict the number of requests you can make within a given time period. This necessitates careful planning and optimization of API calls, especially for large-scale data retrieval.\nAuthentication Complexity: While Spotipy simplifies authentication, understanding OAuth 2.0 and managing access tokens can still be a hurdle for some users.\nData Structure Awareness: Effective use of Spotipy requires a good understanding of the structure of Spotify’s data, including the various object types (artists, tracks, playlists) and their attributes.\nDependence on Spotify API: Spotipy’s functionality is entirely dependent on the Spotify Web API. Any changes or limitations to the API will directly affect the library’s capabilities.\nMaintaining Token Refreshing: Applications that use spotipy and run for long periods of time, need to implement robust token refreshing, or the application will cease to function."
  },
  {
    "objectID": "posts/python/spotipy.html#contact",
    "href": "posts/python/spotipy.html#contact",
    "title": "Spotipy, A Python Library for Spotify Music Lovers",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/mexico_gn.html",
    "href": "posts/python/mexico_gn.html",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "",
    "text": "GN personnel"
  },
  {
    "objectID": "posts/python/mexico_gn.html#national-guard",
    "href": "posts/python/mexico_gn.html#national-guard",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "National Guard",
    "text": "National Guard\nMexico’s National Guard is a relatively new security force established in 2019. It was created to address the country’s high crime rates and complement traditional law enforcement.\n\nFormation\n\nIn 2019, it emerged by merging elements of the Federal Police, Military Police, and Naval Police.\n\nMission\n\nThe National Guard was intended to be a gendarmerie, focusing on territorial defense and public security tasks like crime prevention and patrolling.\n\nStructure\n\nInitially envisioned under civilian control, a 2022 reform transferred command to the Ministry of National Defense, sparking controversy.\n\nCurrent Status\n\nThe National Guard’s role is evolving. It still tackles crime, but also handles tasks like border control and disaster relief."
  },
  {
    "objectID": "posts/python/mexico_gn.html#display-interactive-chart",
    "href": "posts/python/mexico_gn.html#display-interactive-chart",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure 1: Datawrapper Interactive Chart\n\n\n\n\n\n\n\n\n\nNoteInfo\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "posts/python/mexico_gn.html#references",
    "href": "posts/python/mexico_gn.html#references",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "References",
    "text": "References\n\nWhat is Guardia Nacional?\nGobierno de Mexico"
  },
  {
    "objectID": "posts/python/mexico_gn.html#contact",
    "href": "posts/python/mexico_gn.html#contact",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/sql_duckdb.html",
    "href": "posts/python/sql_duckdb.html",
    "title": "Supercharge Your SQL Analysis with Python and DuckDB",
    "section": "",
    "text": "Overview\nIn this post, we’ll delve into the seamless integration of DuckDB with popular Python libraries, enabling efficient data ingestion, transformation, and analysis. Through practical examples, we demonstrate how to harness the full potential of DuckDB for complex SQL queries, real-time data processing, and exploratory data analysis. By the end of this post, readers will gain the knowledge and skills to supercharge their SQL analysis projects with Python and DuckDB.\n\n\nDatabase Creation\nDatabase: retail_db\nTable: retail_sales\n\n\nShow code\n# Import libraries\nimport polars as pl\nimport duckdb as db\nimport plotly.express as px\n\n\n\n\nShow code\n# Create database\nconn = db.connect('datasets/retail_db.db')\n\n\n\n\nShow code\n# Create table\nconn.sql('''\n    create table if not exists retail_sales (\n        id INT,\n        sale_date DATE,\n        sale_time TIME,\n        customer_id INT,\n        gender VARCHAR(10),\n        age INT,\n        category VARCHAR(35),\n        quantity INT,\n        price_per_unit FLOAT,\n        cogs FLOAT,\n        total_sale FLOAT\n        )\n''')\n\n\n\n\nData Ingestion\n\n\nShow code\n# Insert data into table from csv file\nconn.sql('''\n    INSERT INTO retail_sales\n    SELECT * FROM read_csv('datasets/sales.csv')\n''')\n\n\n\n\nData Exploration and Cleaning\n\nRecord Count: Determine the total number of records in the dataset.\nCustomer Count: Find out how many unique customers are in the dataset.\nCategory Count: Identify all unique product categories in the dataset.\nNull Value Check: Check for any null values in the dataset and delete records with missing data\n\n\n\nShow code\n# Show first 10 records \nconn.sql('select * exclude(cogs) from retail_sales limit 10').pl()\n\n\n\nshape: (10, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n10\n2023-11-22\n12:30:00\n110\n\"Female\"\n37\n\"Clothing\"\n1\n99.989998\n99.989998\n\n\n\n\n\n\nRecord count\n\n\nShow code\nconn.sql('select count(*) as records from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\nrecords\n\n\ni64\n\n\n\n\n448\n\n\n\n\n\n\nCustomer count\n\n\nShow code\nconn.sql('select count(distinct customer_id) customers from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\ncustomers\n\n\ni64\n\n\n\n\n55\n\n\n\n\n\n\nCategory count\n\n\nShow code\nconn.sql('select distinct category from retail_sales').pl()\n\n\n\nshape: (9, 1)\n\n\n\ncategory\n\n\nstr\n\n\n\n\n\"Toys\"\n\n\n\"Electronics\"\n\n\n\"Books\"\n\n\n\"Home Appliances\"\n\n\n\"Groceries\"\n\n\n\"Sports Equipment\"\n\n\n\"Clothing\"\n\n\n\"Home Goods\"\n\n\n\"Beauty Products\"\n\n\n\n\n\n\nNull value check\n\n\nShow code\nconn.table('retail_sales').pl().null_count()\n\n\n\nshape: (1, 11)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ncogs\ntotal_sale\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nData Analysis\nWrite a SQL query to retrieve all columns for sales made on ‘2023-11-23’\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where sale_date = '2023-11-23'\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n\n\n\n\nWrite a SQL query to retrieve all transactions where the category is ‘Clothing’ and the quantity sold is more than 4 in the month of Nov-2022\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where category = 'Clothing'\n        and extract('month' from sale_date) = '11'\n        and quantity &gt;= 2\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n14\n2023-11-22\n13:30:00\n114\n\"Female\"\n27\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n22\n2023-11-20\n15:30:00\n122\n\"Female\"\n28\n\"Clothing\"\n2\n49.990002\n99.980003\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n30\n2023-11-17\n17:30:00\n130\n\"Female\"\n42\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n32\n2023-11-16\n14:15:00\n456\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n40\n2023-11-14\n18:30:00\n2859\n\"Female\"\n27\n\"Clothing\"\n4\n39.990002\n159.960007\n\n\n48\n2023-11-06\n19:00:00\n5443\n\"Female\"\n35\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n\n\n\n\nWrite a SQL query to calculate the total sales for each category\n\n\nShow code\nsales = conn.sql('''\n    select\n        category\n        , round(sum(total_sale),2) as net_sale\n        , count(*) as total_orders\n    from retail_sales\n    group by 1\n    order by total_orders desc\n''').pl()\n\n\n\n\nShow code\nsales\n\n\n\nshape: (9, 3)\n\n\n\ncategory\nnet_sale\ntotal_orders\n\n\nstr\nf64\ni64\n\n\n\n\n\"Electronics\"\n68878.32\n96\n\n\n\"Clothing\"\n12557.76\n96\n\n\n\"Books\"\n4133.04\n80\n\n\n\"Home Goods\"\n8437.84\n56\n\n\n\"Toys\"\n1639.04\n24\n\n\n\"Beauty Products\"\n1359.44\n24\n\n\n\"Home Appliances\"\n9599.76\n24\n\n\n\"Groceries\"\n3069.84\n24\n\n\n\"Sports Equipment\"\n3039.68\n24\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(sales,\n             x=\"net_sale\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','net_sale',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Sales by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to find the average age of customers who purchased items from the ‘Clothing’ category\n\n\nShow code\nconn.sql('''\n    select\n        round(avg(age), 2) as avg_age\n    from retail_sales\n    where category = 'Clothing'\n''').pl()\n\n\n\nshape: (1, 1)\n\n\n\navg_age\n\n\nf64\n\n\n\n\n33.25\n\n\n\n\n\n\nWrite a SQL query to find all transactions where the total_sale is greater than 1,000\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where total_sale &gt; 999\n''').pl()\n\n\n\nshape: (24, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n\n\n\n\nWrite a SQL query to find the total number of transactions made by each gender in each category\n\n\nShow code\nconn.sql('''\n    select\n        category\n        , gender\n        , count(*) as total_trans\n    FROM retail_sales\n    group by category\n            , gender\n    order by 2\n''').pl()\n\n\n\nshape: (9, 3)\n\n\n\ncategory\ngender\ntotal_trans\n\n\nstr\nstr\ni64\n\n\n\n\n\"Clothing\"\n\"Female\"\n96\n\n\n\"Home Goods\"\n\"Female\"\n56\n\n\n\"Toys\"\n\"Female\"\n24\n\n\n\"Home Appliances\"\n\"Female\"\n24\n\n\n\"Beauty Products\"\n\"Female\"\n24\n\n\n\"Groceries\"\n\"Male\"\n24\n\n\n\"Electronics\"\n\"Male\"\n96\n\n\n\"Sports Equipment\"\n\"Male\"\n24\n\n\n\"Books\"\n\"Male\"\n80\n\n\n\n\n\n\nWrite a SQL query to calculate the average sale for each month\n\n\nShow code\nconn.sql('''\n    select\n        year\n        , month\n        , avg_sale\n    from\n        (\n        select\n            extract(year from sale_date) as year\n            , EXTRACT(month from sale_date) as month\n            , round(avg(total_sale),2) as avg_sale\n            , rank() over(partition by extract(year from sale_date)\n            order by avg(total_sale) desc) as rank\n        from retail_sales\n        group by 1, 2\n        ) as t1\n''').pl()\n\n\n\nshape: (2, 3)\n\n\n\nyear\nmonth\navg_sale\n\n\ni64\ni64\nf64\n\n\n\n\n2023\n11\n254.61\n\n\n2023\n10\n198.3\n\n\n\n\n\n\nWrite a SQL query to find the top 5 customers based on the highest total sales\n\n\nShow code\nconn.sql('''\n    select customer_id\n            , round(sum(total_sale),2) as total_sales\n    from retail_sales\n    group by 1\n    order by 2 desc\n    limit 5\n''').pl()\n\n\n\nshape: (5, 2)\n\n\n\ncustomer_id\ntotal_sales\n\n\ni32\nf64\n\n\n\n\n129\n7999.92\n\n\n105\n7999.92\n\n\n109\n7999.84\n\n\n113\n6399.92\n\n\n117\n6399.84\n\n\n\n\n\n\nWrite a SQL query to find the number of unique customers who purchased items from each category\n\n\nShow code\ncustomers = conn.sql('''\n    select category\n            , count(distinct customer_id) as count_unique\n    from retail_sales\n    group by category\n    order by 2 desc\n''').pl()\n\n\n\n\nShow code\ncustomers\n\n\n\nshape: (9, 2)\n\n\n\ncategory\ncount_unique\n\n\nstr\ni64\n\n\n\n\n\"Clothing\"\n12\n\n\n\"Electronics\"\n12\n\n\n\"Books\"\n10\n\n\n\"Home Goods\"\n7\n\n\n\"Beauty Products\"\n3\n\n\n\"Toys\"\n3\n\n\n\"Groceries\"\n3\n\n\n\"Home Appliances\"\n3\n\n\n\"Sports Equipment\"\n3\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(customers,\n             x=\"count_unique\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','count_unique',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Unique Customers Purchases by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to create each shift and number of orders (Example Morning &lt;12, Afternoon Between 12 & 17, Evening &gt;17)\n\n\nShow code\nconn.sql('''\n    with hourly_sale as\n        (\n        select *\n            , case \n                when extract(hour from sale_time) &lt;12 then 'Morning'\n                when extract(hour from sale_time) between 12 and 17 then 'Afternoon'\n            else 'Evening'\n            end as shift\n        from retail_sales\n        )\n    select\n        shift\n        , count(*) as total_orders\n    from hourly_sale\n    group by shift\n''').pl()\n\n\n\nshape: (3, 2)\n\n\n\nshift\ntotal_orders\n\n\nstr\ni64\n\n\n\n\n\"Evening\"\n32\n\n\n\"Morning\"\n136\n\n\n\"Afternoon\"\n280\n\n\n\n\n\n\n\n\nClose connection\n\n\nShow code\n# Close connection\nconn.close()\n\n\n\n\nConclusions\nThis project demonstrates the power of combining Python and DuckDB for efficient and insightful SQL analysis. By mastering these tools, data analysts can streamline their workflows, uncover valuable insights, and make data-driven decisions that drive business success.\nWe’ve explored the fundamental concepts of SQL and its practical applications in data analysis. By leveraging the capabilities of Python and DuckDB, we’ve been able to efficiently query, clean, and analyze data. This knowledge and skillset can be applied to a wide range of data-driven tasks, from simple data exploration to complex predictive modeling. As we continue to delve deeper into the world of data, the combination of Python and DuckDB promises to be a powerful tool for data analysts.\nThese insights can be used to optimize marketing strategies, improve customer satisfaction, and drive revenue growth. As data continues to proliferate, the ability to harness its power through SQL analysis will become increasingly important for businesses to stay competitive.\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/why_bi_tools_fall_short.html",
    "href": "posts/python/why_bi_tools_fall_short.html",
    "title": "Why BI Tools Fall Short, A Failure to Capture the Office Workflow",
    "section": "",
    "text": "Overview\nCompanies use data warehouses or data lakes for centralized data storage, consistency, data quality, scalability and easy access. Business Intelligence (BI) solutions in conjunction with data warehouses are used to make more informed, data-driven decisions by means of dahsboards for stakeholders.\nIn this fashion, a dashboard is created using a BI application, connected to a data warehouse with the aim to be consumed by end users for their business activities.\nIt is an open secret, nonetheless, that staff steadily use spreadsheets to store information and manipulate data sets coming from data warehouses, other information systems and dashboards.\nIn a similar fashion, staff steadily use slide presentations to showcase insights and reports to managers and other stake holders. This means there are countless presentations and data analyses stored in local Excel and PowerPoint files.\n\n\nBeyond the Dashboard: Reporting with Slideshows\nDashboards can be used to gather and analyze data, while slideshows can be used to present the findings in a clear and concise manner. Besides, it enables you to use your existing data insights in the tools you’re most familiar with, without having to switch to more complex ones.\n\nYou can just do your data analysis in Excel and then present it in PowerPoint. This provides you with just the flexibility you need for.\n\nWhile a dashboard is a centralized section that displays your data visually typically by using a license BI tool (Tableau, Power BI), staff prefers to present data insights to potential customers or coworkers in Excel or PowerPoint by copying and pasting charts and tables from dashboards.\n\n\n\n\n\n\nIndeed, BI tools like Tableau or Power BI offer options to download data to Excel or csv files, PowerPoint and images.\n\n\n\nAutomating Spreadsheet Data with Python\nI propose the process of creating an ETL from the data warehouse to a spreadsheet using Python, and synchronizing tables and charts from Excel to PowerPoint to get an automated reporting in a local file with the needs of the end user.\nETL (Extract, Transform, Load) is a data integration process that involves 03 main steps:\n\nExtract Phase. Retrieving data from a source system (in this case, a data warehouse).\nTransform Phase. Manipulating, cleaning, and aggregating the extracted data.\nLoad Phase. Storing the transformed data into a target system (in this case, an Excel file).\n\n\n\n\n\n\n\nFigure 1: Image by El Mehdi Ettaki\n\n\n\n\n\nCase Study\nI’ll show an example using Snowflake as data warehouse, Python for ETL process, and Excel as destination. Finally, PowerPoint will present the data insights.\n\nBy foregoing BI tools, we can substantially reduce project expenses.\n\n\n\n\n\n\n\nFigure 2: Image by author\n\n\n\n\nImport libraries\n\n\nimport pandas as pd\nfrom snowflake.snowpark import Session, Window\nimport snowflake.snowpark.functions as F\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nRead credentials\n\n\n# Credentials\nfile = 'credentials.json'\n# read file\nwith open(file) as f:\n    keys = json.load(f)\n\n\nConnect to Data Warehouse using Snowpark\n\n\n# Snowflake's Snowpark Connection\nconnection = {\n    \"account\": keys['account'],\n    \"user\": keys['user'],\n    \"role\": keys['role'],\n    \"authenticator\": keys['authenticator'],\n    \"warehouse\": keys['warehouse'],\n    \"database\": keys['database'],\n    \"schema\": keys['schema'],\n}\n\ndef snowflake_connection():\n    try:\n        session = Session.builder.configs(connection).create()\n        print(\"Connection successful!\")\n    except:\n        raise ValueError(\"Connection failed!\")\n    return session\n\nsession = snowflake_connection()\n\n\nExtract data using Snowpark\n\n\n# Extract sales data\nsales = conn.sql('''\n  SELECT\n      store_id,\n      SUM(sales_amount) AS total_sales\n  FROM\n      dm_sales\n  WHERE\n      YEAR(dm_sales) = YEAR(CURDATE())\n  GROUP BY\n      store_id\n''')\n# Extract stores data\nstores = (\n  conn.table('dm_stores')\n      .select('location','id','responsible')\n)\n# Label stores region\nstores = stores.with_column(\n        'region',\n        F.when(F.col('region_abv')=='AS', 'ASPAC')\n          .when(F.col('region_abv')=='LA', 'LATAM')\n          .when(F.col('region_abv')=='EU', 'EMEA')\n          .when(F.col('region_abv')=='NA', 'NA')\n          .otherwise('null')\n    )\n# Combine tables\ndata = (sales.join(stores, sales.store_id == stores.id))\n# Save to pandas\ndata = data.to_pandas()\n\n\nTransform data using Pandas\n\n\n# Calculate top 10 products\ntop_10 = data.nlargest(10, 'total_sales')\n\n\nLoad data using Pandas\n\n\n# Write to Excel\ntop_10.to_excel('top_10_products.xlsx', index=False)\n\n\n\nFrom Excel to PowerPoint: Automating Report Creation\nNow, you can customize your tables and charts in Excel with the data saved from Python. \nTo automate your customized tables and charts created in Excel onto PowerPoint, you just need to follow the next steps:\n\n\n\n\n\n\n\n\n\n\nFinally, after customizing your slideshow, you will get a PowerPoint like the following:\n\n\n\n\n\n\nFigure 3: Image by author\n\n\n\n\n\nConclusions\nThe use of spreadsheets and slideshows in businesses is not going to disappear soon. Hence, even if BI tools like Tableau or Power BI are used in businesses, Excel and PowerPoint are going to be used by staff to reporting and presentations to coworkers and managers.\nBy following the above steps and leveraging the power of Python, you can efficiently extract, transform, and load data from your data warehouse into Excel for further analysis and insights that fulfills end user’s requirements.\nMaybe is it time to recalculate the cost-benefit implications for companies to abandon expensive BI licenses in favor of flexible, cost-effective open-source solutions like Python.\n\n\nReferences\n\nBusiness (2023) How to Design a Dashboard Presentation: A Step-by-Step Guide in slidemodel.com\nKarlson, P. (2022) Are Spreadsheets Secretly Running Your Business? in Forbes\nMoore J. (2024) But, Can I Export it to Excel? in Do Mo(o)re with Data\nSchwab, P. (2021) Excel dominates the business world.. and that’s not about to change in Into the Minds\n\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Economist & Data Scientist\n\nI’ve applied data analysis and visualization to drive strategic value for 10+ years across industries like public security, e-commerce, and healthcare.\nI use Python1, SQL, and Tableau to transform raw data into clear, actionable data products (webpages, slideshows, interactive reports) that support decision-making, occasionally incorporating regression and classification analyses.\nCommitted to fostering data literacy, I share data posts on my Blog and also on Learning Data and T3CH via Medium.\nBackground in Economics (BA), Information Technology (MA) and Data Science and Machine Learning (Certification).\nVegan.\n© 2025"
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "About me",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIncluding libraries such as numpy, pandas, polars, duckdb, matplotlib, seaborn, plotly, folium, sckitlearn.↩︎"
  }
]