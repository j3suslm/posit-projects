[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "An Application of Location Quotients to High-Impact Offenses\n\n\nCrime Localization in Mexico 2025\n\n\n\npython\n\n\n\nTraditional crime rate metrics often mask geographic risk disparities by presenting aggregate averages.  In contrast, the LQ provides a robust analytical tool for determining whether a specific geographic area experiences a disproportionate share of a given crime type relative to a larger reference area, thereby highlighting true criminal hotspots.\n\n\n\n\n\nDec 8, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City Crime According to the Statistics\n\n\nIssues and Figures, 2019-2024\n\n\n\npython\n\n\n\nCrime in Mexico City presents a complex and evolving challenge. The city, a sprawling metropolis, grapples with a range of criminal activities, from petty theft and street-level drug offenses to organized crime and violent acts. Factors contributing to this multifaceted issue include socioeconomic disparities, corruption, and the influence of transnational criminal organizations.\n\n\n\n\n\nJun 29, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City Crime Incidence 2019-2024\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nStop Building Dashboards\n\n\nStart Understanding the White Collar Workflow\n\n\n\npython\n\nsql\n\n\n\nIn today’s data-driven world, Business Intelligence (BI) tools promise powerful insights and streamlined reporting. Yet, the humble spreadsheet and slideshow persist in the white-collar world. While BI tools manages complex analysis and visualization, spreadsheets and slideshows offer unique advantages that keep them firmly entrenched in our workflows.\n\n\n\n\n\nFeb 24, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGasoline Prices in Mexico\n\n\nFuel Price Fluctuations, A Constant Concern\n\n\n\npython\n\n\n\nGasoline prices in Mexico are a complex issue influenced by various factors. In this article we examine the gasoline prices in the 32 States that constitute Mexico and also their differences in cost, sale price and profits. Challenges remain in stabilizing prices and mitigating the impact on consumers and the economy.\n\n\n\n\n\nFeb 3, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Database System with DuckDB for Local Processing and MotherDuck for Scalable Cloud Storage\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nThis project explores how to leverage the strengths of DuckDB and MotherDuck to build a robust data processing and storage solution. DuckDB excels at fast in-memory analytics, while MotherDuck provides a scalable and cost-effective cloud data warehouse. By combining these technologies, you can achieve optimal performance for both local and cloud-based data operations.\n\n\n\n\n\nJan 13, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nData Alchemy, SQL Analysis within Python\n\n\nA Case Study using Duckdb, Polars and Plotly\n\n\n\npython\n\nsql\n\n\n\nThis project focuses on leveraging the strengths of DuckDB, Polars, and Plotly for efficient data analysis and visualization. DuckDB is used for fast in-memory data processing, Polars provides a user-friendly and high-performance DataFrame library, and Plotly offers interactive and customizable visualizations.\n\n\n\n\n\nJan 6, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nGeospatial Analysis in Python\n\n\nA Case Study using Duckdb, Polars and Folium\n\n\n\npython\n\n\n\nGeospatial analysis involves the application of spatial concepts and techniques to data that has geographic coordinates. With the rise of big data and the increasing availability of geospatial information, the demand for effective geospatial analysis tools has grown significantly. Python, with its rich ecosystem of libraries, has emerged as a powerful and popular choice for geospatial data scientists.\n\n\n\n\n\nJan 2, 2025\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSports Commerce\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy BI Tools Fall Short, A Failure to Capture the Office Workflow\n\n\nSlideshows and Spreadsheets Still Rule the Business World\n\n\n\npython\n\n\n\nDespite BI solutions, spreadsheets & slideshows persist in data-driven decision making.\n\n\n\n\n\nDec 5, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge Your SQL Analysis with Python and DuckDB\n\n\nRethinking Data Analysis: A New Paradigm\n\n\n\npython\n\nsql\n\n\n\nThis article explores the synergy between Python and DuckDB, a powerful in-memory database, to revolutionize SQL-based data analysis. By leveraging Python’s extensive data science ecosystem and DuckDB’s lightning-fast query execution, data professionals can significantly accelerate their workflows.\n\n\n\n\n\nNov 23, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCovid Cases in Americas\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nSpotipy, A Python Library for Spotify Music Lovers\n\n\nDiscover your Music Habits with Python\n\n\n\npython\n\n\n\nEver wondered about the intricacies of your listening habits on Spotify? Or perhaps you’re a developer looking to build a music-related application? Enter Spotipy, a fantastic Python library that acts as a bridge between your code and the Spotify Web API.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFlying into Mexico\n\n\nA Bird’s-Eye View of Major Airports\n\n\n\npython\n\n\n\nAs a popular tourist destination, Mexico offers visitors a diverse range of experiences, from ancient ruins to pristine beaches. The country’s well-connected airport system ensures seamless travel to these captivating destinations.\n\n\n\n\n\nOct 30, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Stunning Charts with Datawrapper and Python\n\n\nAn Overview\n\n\n\npython\n\n\n\nDatawrapper is a powerful online tool designed to help you create engaging and informative data visualizations. Whether you’re a journalist, researcher, or simply someone who wants to present data in a more visually appealing way, Datawrapper can make the process quick and easy.\n\n\n\n\n\nSep 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico City’s Underground\n\n\nA History of Challenges and Triumphs\n\n\n\npython\n\n\n\nMexico City’s metro is the largest and busiest in Latin America, serving more than 5.5 million passengers daily in its 195 stations and 12 lines\n\n\n\n\n\nSep 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico’s Peace index 2024\n\n\n\n\n\n\npython\n\n\n\nThe Mexico Peace Index (MPI), created by the Institute for Economics and Peace (IEP), is a valuable tool for understanding peacefulness in Mexico. According to IEP, Mexico’s peacefulness has improved for 04 years in a row. However, the situation isn’t uniform. While 15 states showed improvement, 17 got worse. Drug cartel activity and political violence remain challenges.\n\n\n\n\n\nMay 17, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Data Pipelines with BigQuery and Python\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn the realm of data analytics, Extract, Transform, Load (ETL) processes play a pivotal role. They streamline the integration of data from various sources, enabling its cleaning, manipulation, and loading into target systems like BigQuery, Google’s cloud-based data warehouse. By leveraging Python’s versatility and BigQuery’s scalability, you can construct powerful ETL pipelines to prepare your data for insightful analysis.\n\n\n\n\n\nMay 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Charts with HoloViews\n\n\nUnveiling Insights with Less Effort by Using HoloViews\n\n\n\npython\n\n\n\nHoloViews offers a compelling alternative for data visualization in Python. With its emphasis on simplicity, flexibility, and interactivity, HoloViews empowers you to create insightful visualizations that effectively communicate your data’s story.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nNational Guard’s Deployed Personnel in Mexico\n\n\n\n\n\n\npython\n\n\n\nMexico’s National Guard is a relatively new security force established in 2019. It was created to address the country’s high crime rates and complement traditional law enforcement.\n\n\n\n\n\nApr 19, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Dashboards with Python\n\n\nUnleash the Power of Python\n\n\n\npython\n\n\n\nPython, a versatile programming language, empowers you to create interactive dashboards that unlock the hidden potential of your data.\n\n\n\n\n\nApr 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Use Mermaid to Create Diagrams as a Data Scientist\n\n\nUse Python code as your Secret Weapon\n\n\n\npython\n\n\n\nIn this article, we will show how you can create diagrams with code within Jupyter and stop using external diagramming graphical user interfaces (GUIs) like draw.io or Lucidchart.\n\n\n\n\n\nMar 29, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Slides with Python and Quarto\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico’s Political Landscape\n\n\n\n\n\n\npython\n\n\n\nIn this article, we will show an overview of Mexico’s Political Scenario and its correlation with Mexico City’s.\n\n\n\n\n\nMar 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDuckDB: A Compelling Solution for Data Analysis within Python Environment\n\n\nA valuable tool for data scientists working with Python due to its speed, ease of use, and tight integration.\n\n\n\npython\n\nsql\n\n\n\nDuckDB is a fast, embedded analytical database that outstands in in-memory operations. It provides a SQL interface, making it easy for users with database querying experience.\n\n\n\n\n\nMar 18, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMexico: A Populous Nation with a Dynamic Demographic Landscape\n\n\nAn Overview of Mexico’s Population Dynamics since 1950\n\n\n\npython\n\n\n\nMexico boasts a population of nearly 130 million, ranking it as the 10th most populous country globally. Mexico is a highly urbanized nation, with 88% of the population residing in cities. This trend is expected to continue, driven by economic opportunities and infrastructure development.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nWater Collection System in Mexico City - 2022\n\n\nRainwater harvesting systems are a prominent water collection method in Mexico\n\n\n\npython\n\n\n\nRainwater harvesting is a sustainable and effective way to manage water resources in Mexico. By promoting water conservation and increasing water security, these systems offer a path towards a more water-resilient future.\n\n\n\n\n\nMar 16, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nManipulating JSON Files with Python\n\n\nAn Overview Using Pandas\n\n\n\npython\n\n\n\nThis article presents a straightforward and efficient method for reading JSON files using Python Pandas library. We will show how to import JSON data into DataFrames, enabling comprehensive analysis and exploration. This approach significantly simplifies the process of working with JSON data, making it accessible for data analysis.\n\n\n\n\n\nMar 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Lego Toys with Polars\n\n\nA quick summary about Lego bricks\n\n\n\npython\n\n\n\nThe Lego brick was invented in 1949 by Ole Kirk Christiansen, and the company has since grown to become one of the world’s leading toy manufacturers. Lego products are sold in over 140 countries, and the company has over 40,000 employees worldwide.\n\n\n\n\n\nMar 11, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nFederal Prisons in Mexico\n\n\nA Geographical Overview of Mexico’s Federal Penal System\n\n\n\npython\n\n\n\nMexico’s federal prisons, known as Ceferesos (Centros Federales de Readaptación Social), are a complex system facing challenges. While intended for rehabilitation, reports often highlight overcrowding, violence, and inadequate living conditions. In this article we will show the location of federal prisons in Mexico, including the famous Islas Marías, which closed in 2019. We wil use Folium, a powerful Python library to enhance data analysis and geographic visualization.\n\n\n\n\n\nMar 6, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression\n\n\nAn Extension for Linear Regression Models\n\n\n\npython\n\n\n\nIn this article, we shall show where linear regression falls short and we should use polynomial regression instead.\n\n\n\n\n\nMar 2, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Analysis Overview\n\n\nA quick summary about linear and nonlinear regression\n\n\n\npython\n\n\n\nWhether you want to do statistics, machine learning, or economic analysis, it’s likely that you will have to use regression analysis. Regression analysis is one of the most important fields in statistics, economics and machine learning. We shall briefly explore the different techniques about regression.\n\n\n\n\n\nFeb 28, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nCohort Analysis\n\n\nUnderstanding Customer Behavior\n\n\n\npython\n\n\n\nCohort analysis is an extremely helpful tool that can be used to improve business practices and can effectively increase user retention if businesses implement necessary changes according to the test results. In today’s world where data is everywhere, cohort analysis is effective in extracting useful information by analyzing the behavioral patterns of customers in order to predict the future of the business. Observing cohorts over time gives insight into user experience and helps in developing better tactics.\n\n\n\n\n\nFeb 9, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the Power of Big Data with PySpark\n\n\nAn Overview\n\n\n\npython\n\nsql\n\n\n\nIn today’s data-driven world, we’re constantly bombarded with massive amounts of information. Analyzing this data efficiently and effectively is crucial for businesses and researchers alike. That’s where PySpark comes in. It’s a powerful tool that brings the distributed computing capabilities of Apache Spark to the familiar and versatile Python ecosystem.\n\n\n\n\n\nJan 12, 2024\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nMissing People in Mexico\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nUS Sales Dashboard\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nLooker Studio\n\n\n\n\n\n\nlooker-studio\n\n\n\n\n\n\n\n\n\nFeb 25, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nTableau Charts example\n\n\n\n\n\n\ntableau\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nJesus L. Monroy\n\n\n\n\n\n\n\n\n\n\n\n\nDiving into the World of SQL\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\nDec 14, 2022\n\n\nJesus L. Monroy\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/sql/sql.html#what-is-sql",
    "href": "posts/sql/sql.html#what-is-sql",
    "title": "Diving into the World of SQL",
    "section": "What is SQL?",
    "text": "What is SQL?\nSQL stands for Structured Query Language. It is a standardized language for managing data in relational databases. Think of a relational database as a highly organized digital filing cabinet, where information is stored in tables with rows and columns. SQL provides the tools to not only retrieve specific data from these tables but also to create, modify, and delete tables and their contents."
  },
  {
    "objectID": "posts/sql/sql.html#why-learn-sql",
    "href": "posts/sql/sql.html#why-learn-sql",
    "title": "Diving into the World of SQL",
    "section": "Why Learn SQL?",
    "text": "Why Learn SQL?\nThe widespread adoption of relational databases like MySQL, PostgreSQL, Microsoft SQL Server, and Oracle Database makes SQL a highly sought-after skill. Learning SQL opens doors to a variety of opportunities, including:\n\nData Analysis\n\nSQL allows you to extract meaningful insights from vast datasets, identify trends, and generate reports that drive business decisions.\n\nDatabase Administration\n\nManaging and maintaining databases, ensuring data integrity, and optimizing performance often requires a deep understanding of SQL.\n\nWeb Development\n\nMany web applications rely on databases to store and retrieve information. SQL is essential for building dynamic websites and applications.\n\nData Science\n\nSQL is a fundamental tool for data scientists, enabling them to clean, prepare, and explore data before applying more advanced statistical and machine learning techniques."
  },
  {
    "objectID": "posts/sql/sql.html#key-sql-concepts",
    "href": "posts/sql/sql.html#key-sql-concepts",
    "title": "Diving into the World of SQL",
    "section": "Key SQL Concepts:",
    "text": "Key SQL Concepts:\nSQL is built around a set of commands that allow you to perform various operations. Here are some of the most important concepts:\n\nSELECT\n\nThis command is used to retrieve data from one or more tables based on specified criteria. You can select specific columns, filter data using conditions, and sort the results. For example: SELECT name, age FROM users WHERE city = ‘New York’;\n\nINSERT\n\nThis command adds new rows of data into a table. For example: INSERT INTO users (name, age, city) VALUES (‘John Doe’, 30, ‘London’);\n\nUPDATE\n\nThis command modifies existing data in a table. For example: UPDATE users SET age = 31 WHERE name = ‘John Doe’;\n\nDELETE\n\nThis command removes rows from a table. For example: DELETE FROM users WHERE age = 65;\n\nCREATE\n\nThis command is used to create new database objects, such as tables, views, and indexes.\n\nDROP: This command deletes existing database objects."
  },
  {
    "objectID": "posts/sql/sql.html#tasks-performed-with-sql",
    "href": "posts/sql/sql.html#tasks-performed-with-sql",
    "title": "Diving into the World of SQL",
    "section": "Tasks performed with SQL",
    "text": "Tasks performed with SQL\n\nCreate and drop tables\nInsert and update records\nRetrieve data\nDelete data\nManage permissions\nCreate views and procedures"
  },
  {
    "objectID": "posts/sql/sql.html#advanced-tasks-in-sql",
    "href": "posts/sql/sql.html#advanced-tasks-in-sql",
    "title": "Diving into the World of SQL",
    "section": "Advanced tasks in SQL",
    "text": "Advanced tasks in SQL\n\nJOINs: Combining data from multiple tables based on related columns.\nSubqueries: Embedding queries within other queries to perform more sophisticated data retrieval.\n\nAggregate Functions: Performing calculations on groups of data, such as calculating averages, sums, and counts.\n\nWindow functions:"
  },
  {
    "objectID": "posts/sql/sql.html#example-queries",
    "href": "posts/sql/sql.html#example-queries",
    "title": "Diving into the World of SQL",
    "section": "Example queries",
    "text": "Example queries\n-- COUNT CUSTOMERS\nSELECT COUNT(1) AS customers\nFROM addresses\n;\n-- GET RETAIL SALES DATASET\nSELECT p.id_purchase\n    , d.priceInfo\n    , d.quantity\n    , s.id_shopper\n    , s.id_region\n    , r.description AS region\n    , s.id_territory\n    , t.description AS territory\n    , s.id_store\n    , b.description AS store\n    , s.NUD\n    , s.shop\n    , s.route\n    , s.purchase_date\n    , p.cancel\nFROM purchases AS p\nINNER JOIN purchase_details AS d ON p.id_purchase = d.id_purchase \nINNER JOIN shops AS s ON p.id_shopper = s.id_shopper\nINNER JOIN territories AS t ON s.id_territory = t.id_territory\nINNER JOIN stores AS b ON s.id_store = b.id_store\nINNER JOIN regions AS r ON s.id_region = r.id_region\n;\n-- GET PURCHASES BY CUSTOMER\nSELECT o.no_order\n    , o.id_order_status\n    , e.description\n    , o.id_social_network\n    , CASE\n        WHEN o.id_social_network = 1 THEN 'I'\n        WHEN o.id_social_network = 2 THEN 'F'\n        ELSE 'W'\n    END AS origin\n    , o.quantity \n    , o.total AS total_order\n    , o.purchase_date\n    , d.route AS delivery_route\n    , d.id_store \n    , d.nud\n    , d.email \n    , d.phone \nFROM orders o\nLEFT JOIN orderStatus e ON o.id_order_status = e.id_order_status\nLEFT JOIN addresses d ON o.id_address = d.id_address\nWHERE 1 = 1\n-- AND DATE(fs_ConvertDateToMexico(o.purchase_date))\n-- BETWEEN '2023-04-01' AND '2023-04-30'\n;\n-- GET UNIQUE BRANDS\nSELECT DISTINCT g.brand_code as Brand_Code\n, g.brand_name as Brand_Name\n, g.style_num_offset as Style_Num_Offset\n, g.active as Active\n, g.parent_brand as Parent_Brand\n, g.reporting_brand as Reporting_Brand\nFROM\ntp_brand g\n;\n-- GET FACTORY DATA WITH NULL COUNTRY OR CITY\nSELECT CONVERT(e.id,char) as Factory_Id\n, e.factory_name  as PO_FTY\n, ifnull(e.factory_city,'No City') as City\n, ifnull(e.factory_country,'No Country') as Country\nFROM\ntp_factories e\n;\n-- INSER DATA INTO TABLE\ninsert into CatAlumnos\nvalues(‘Juan’, ‘Perez’, ‘Huerta’, 1990-02-25, 2, ‘a’)\n;\n/* GET STUDENT AGE POSTGRES FUNCTION */\nWITH cte_ages AS (\n    SELECT name, age(CURRENT_DATE, bod) AS student_age\n    FROM students\n)\nSELECT name, age\nFROM ages\nWHERE age &lt; 18\n;\n-- CREATE VIEW IN SQL SERVER\nCREATE VIEW student_profiles AS\n    SELECT concat(name, ' ' surname) AS student_name,\n/* Using SQL Server datediff function to reckon age */\n    DATEDIFF(YY, getdate(), bod) as age\n    FROM students\n    ORDER BY name\n;\n/* CONCAT AND CAST DATA TYPES */\nSELECT \n  concat('gsn','_', cast(campaign_id as text)) AS id_gsn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_2\nTOP 5\n;\n/* CREATE A UNION OF TABLES FROM 02 TEMP TABLES (CTEs) */\nWITH table_2_temp AS (\nSELECT\n  concat('gsn', '_', cast(campaign_id as text)) AS id_gsn\n, start_date \n, end_date\n, clicks\n, views\nFROM table_2\n)\n, table_3_temp AS (\nSELECT \nconcat('fbn', '_', cast(campaign_id as text)) AS id_fbn\n, start_date\n, end_date \n, clicks\n, views\nFROM table_3\nWHERE cliks &gt; 0\n)\nSELECT \n  t2.id_gsn\nFROM table_2_temp t2\nUNION ALL\nSELECT \n  t3.id_fbn\nFROM table_3_temp t3\n;\n/* Window functions\nAdd row numbers to the placings table */\nSELECT\n    'year'\n    , host_country\n    , first_place\n    , total_goals\n    , row_number() OVER() AS row_num\nFROM world_cup_placings\n;\n\n/* Using SUM() within our window function */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , SUM(total_goals) OVER() AS all_goals\nFROM world_cup_placings\n;\n\n/* Computing the average number of goals */\nSELECT\n    \"year\"\n    , host_country\n    , first_place\n    , total_goals\n    , round(avg(total_goals) OVER(), 0) AS mean_goals\nFROM world_cup_placings\n;\n-- Big Query platform\n-- covid cases in North America 2020-2023\nwith years as (\n    select country_region\n        , extract(year from date) as year\n        , sum(confirmed) as total_confirmed\n        , sum(deaths) as total_deaths\n        , sum(cast(recovered as integer)) as total_recovered\n        , sum(active) as total_active\n    from `big-query.public-data.covid19_jhu_csse.summary`\n    where country_region in ('Mexico', 'US', 'Canada')\n    group by 1, 2\n    order by 1, 2\n)\nselect *\nfrom (\n    select \n        year\n        , country_region\n        , total_confirmed\n    from years\n)\npivot(sum(total_confirmed) as confirmed for year in (2020, 2021, 2022, 2023))\n;"
  },
  {
    "objectID": "posts/sql/sql.html#contact",
    "href": "posts/sql/sql.html#contact",
    "title": "Diving into the World of SQL",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy\nEconomist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/airports.html",
    "href": "posts/python/airports.html",
    "title": "Flying into Mexico",
    "section": "",
    "text": "Figure 1: Mexico City’s International Airport"
  },
  {
    "objectID": "posts/python/airports.html#contact",
    "href": "posts/python/airports.html#contact",
    "title": "Flying into Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html",
    "href": "posts/python/mermaid-diagrams.html",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "",
    "text": "Figure 1: Mermaid for diagramming"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "href": "posts/python/mermaid-diagrams.html#data-science-cycle-diagram-using-mermaid",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Data Science Cycle Diagram using Mermaid",
    "text": "Data Science Cycle Diagram using Mermaid\n\n\nCode\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D\n\n\n\n\n\n---\ntitle: Data Science Flow chart\n---\n\ngraph LR\n%% Create a flow chart about data science process\n\nA[Define Problem] --&gt; B[(Data Collection)]\nB --&gt; C([Exploratory Data Analysis])\nC --&gt; D{Data Cleaning & Preprocessing}\nD --&gt; E[Feature Engineering]\nE --&gt; F[Model Building & Training]\nF --&gt; G([Model Evaluation])\nG --&gt; H[Deployment]\nG --&gt; I[Refine Model]\nI --&gt; D"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#flow-process",
    "href": "posts/python/mermaid-diagrams.html#flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Flow Process",
    "text": "Flow Process\n\n\nCode\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\n\n\nAdding colors\n\n\nCode\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC\n\n\n\n\n\nflowchart LR\n    style ItemA fill:#C4F7A1, stroke:#7FC6A4\n    style ItemB fill:#FFEC51, stroke:#FFEC51  \n    style ItemC fill:#FFC4EB, stroke:#FFC4EB \n    ItemA--&gt;ItemB--&gt;ItemC"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#complex-flow-process",
    "href": "posts/python/mermaid-diagrams.html#complex-flow-process",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Complex Flow Process",
    "text": "Complex Flow Process\ngraph\n    A[ItemA]--&gt; ItemB\n    A--&gt; ItemC\n    subgraph ItemC\n        D[ItemD]--&gt;E[ItemE]\n        E--&gt;D\n    end\n\n    X[ItemX]==&gt;Decision\n    click X \"https://mermaid.js.org/\"\n    Decision{Item Y?}==&gt;|Yes| Y[ItemY]\n    Decision==&gt;|No| Z[ItemZ]"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#pie-charts",
    "href": "posts/python/mermaid-diagrams.html#pie-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Pie charts",
    "text": "Pie charts\npie title Pie chart 2\n    \"Category A\" : 2000\n    \"Category B\" : 500\n    \"Category C\" : 1000"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#gantt-chart",
    "href": "posts/python/mermaid-diagrams.html#gantt-chart",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Gantt Chart",
    "text": "Gantt Chart\ngantt \n    title Gantt chart 1\n    dateFormat YYYY-MM-DD\n    axisFormat %b %d\n    section Section A\n        Task A1: a1, 2024-04-15, 7d\n        Task A2: after a1, 5d\n    section Section B\n        Task B1: 2024-04-22, 2024-05-02\n        Task B2: 2024-04-29, 5d"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#journey-charts",
    "href": "posts/python/mermaid-diagrams.html#journey-charts",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Journey Charts",
    "text": "Journey Charts\njourney\n    title Journey 1\n    section Section A\n        Activity 1A: 5: Person1\n        Activity 2A: 3: Person2\n        Activity 3A: 2: Person1, Person2\n    section Section B\n        Activity 1B: 4: Person2\n        Activity 1B: 5: Person1"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#state-diagram",
    "href": "posts/python/mermaid-diagrams.html#state-diagram",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "State Diagram",
    "text": "State Diagram\nstateDiagram\n    [*] --&gt; StateA\n    StateA --&gt; [*]\n    StateA --&gt; StateB\n    StateB --&gt; StateA\n    StateB --&gt; StateD\n    StateD --&gt; [*]"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#items-diagrams",
    "href": "posts/python/mermaid-diagrams.html#items-diagrams",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Items diagrams",
    "text": "Items diagrams\nerDiagram\n    Item1 ||--o{ Item2: text1\n    Item2 ||--|{ Item3: text2\n    Item1 }|..|{ Item4: text3\n    Item4 }o--o{ Item4: text3"
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#conclusions",
    "href": "posts/python/mermaid-diagrams.html#conclusions",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Conclusions",
    "text": "Conclusions\nMermaid is a popular markdown-like syntax for generating diagrams and charts using Markdown. Think of it as a way to describe your diagrams in a human-readable format, which Mermaid then renders into a visual representation.\nMermaid Python acts as a bridge, allowing you to generate these Mermaid diagrams within your Python environment and even embed them in your Jupyter notebooks or web applications.\nMermaid Python also allows you to customize the appearance of your diagrams, add styling, and even integrate with other Python libraries. You can dynamically generate diagrams based on your data, making it a powerful tool for data visualization and exploration."
  },
  {
    "objectID": "posts/python/mermaid-diagrams.html#contact",
    "href": "posts/python/mermaid-diagrams.html#contact",
    "title": "Why You Should Use Mermaid to Create Diagrams as a Data Scientist",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/clq.html",
    "href": "posts/python/clq.html",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "",
    "text": "Figure 1: Several violent episodes in Mexico suggest a worrying trend\nSource: The Economist"
  },
  {
    "objectID": "posts/python/clq.html#spatial-concentration-of-high-impact-crime-in-mexico",
    "href": "posts/python/clq.html#spatial-concentration-of-high-impact-crime-in-mexico",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Spatial Concentration of High Impact Crime in Mexico",
    "text": "Spatial Concentration of High Impact Crime in Mexico\nThis study utilizes the Location Quotient (LQ) methodology to systematically identify and quantify the spatial concentration of High-Impact Crimes across Mexican municipalities, January through October 2025. We have reduced the research to analyze homicides, in order to establish baseline LQs for the target year.\nFindings are anticipated to reveal significant spatial segregation of criminal risk, with certain key urban and border jurisdictions demonstrating LQs significantly greater than 1.0, indicating a severe concentration of high-impact offenses.\nThe primary objective is to move beyond generalized security strategies by providing policymakers and public security agencies with granular, evidence-based intelligence for targeted resource allocation.\nThe resulting maps and risk profiles offer a critical foundation for designing preventative interventions, optimizing police deployment, and implementing precision policing strategies to effectively combat concentrated insecurity in Mexico."
  },
  {
    "objectID": "posts/python/clq.html#key-elements",
    "href": "posts/python/clq.html#key-elements",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Key Elements",
    "text": "Key Elements\n\nProblem: Traditional crime rates mask disparity.\nMethodology: Location Quotient (LQ) analysis.\nFocus: High-Impact Homicides in Mexico.\nTimeframe: January through October 2025.\nKey Findings: Significant spatial segregation/concentration (LQ &gt; 1.0) will be revealed.\nImpact: Provides intelligence for targeted resource allocation and precision policing."
  },
  {
    "objectID": "posts/python/clq.html#foundations-of-location-quotients",
    "href": "posts/python/clq.html#foundations-of-location-quotients",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Foundations of Location Quotients",
    "text": "Foundations of Location Quotients\nThe Location Quotient, primarily developed by economists to study regional specialization and industrial concentration, provides a measure of relative importance.\nHoover (1936) and Isard (1951) are credited with pioneering the concept. In economics, the LQ for an industry in a region is the ratio of that industry’s share of regional employment to its share of national employment.\nAn LQ greater than 1.0 signifies that the region is specialized in that industry compared to the nation. This concept is directly translated to crime analysis, replacing ‘industry’ with ‘crime type’ and ‘employment’ with ‘crime counts’ or ‘population.’"
  },
  {
    "objectID": "posts/python/clq.html#datasets",
    "href": "posts/python/clq.html#datasets",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Datasets",
    "text": "Datasets\n\nHomicides\n\nhomicides = Path('homicides.parquet')\n\ndf_homicides = pl.read_parquet(homicides)\n\ndf2 = df_homicides.to_pandas()\n\n\n\n\n(\n    df2 \n        .head(10)\n        .style.hide(axis='index')\n)\n\n\nTable 1: Preview of homicides dataset\n\n\n\n\n\nPopulation\n\npop = Path('population.parquet')\n\ndf_pop = pl.read_parquet(pop)\n\ndf3 = df_pop.to_pandas()\n\n\n\n\n(\n  df3\n      .head(10)\n      .style.hide(axis='index')\n)\n\n\nTable 2: Preview of population dataset"
  },
  {
    "objectID": "posts/python/clq.html#data-collection-for-2025",
    "href": "posts/python/clq.html#data-collection-for-2025",
    "title": "An Application of Location Quotients to High-Impact Offenses",
    "section": "Data Collection for 2025",
    "text": "Data Collection for 2025\n\n\n\n\n\n\nNoteCrime Incidence Data\n\n\n\nRequired Data: The number of victims of homicide disaggregated by municipality and the national total for the study period (January-October 2025).\nSource: SESNSP Open Crime Incidence Data.\n\n\n\n\n\n\n\n\nNotePopulation Data\n\n\n\nRequired Data: The Municipal Population projection for 2025.\nSource: CONAPO."
  },
  {
    "objectID": "posts/python/mexico_gn.html",
    "href": "posts/python/mexico_gn.html",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "",
    "text": "GN personnel"
  },
  {
    "objectID": "posts/python/mexico_gn.html#national-guard",
    "href": "posts/python/mexico_gn.html#national-guard",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "National Guard",
    "text": "National Guard\nMexico’s National Guard is a relatively new security force established in 2019. It was created to address the country’s high crime rates and complement traditional law enforcement.\n\nFormation\n\nIn 2019, it emerged by merging elements of the Federal Police, Military Police, and Naval Police.\n\nMission\n\nThe National Guard was intended to be a gendarmerie, focusing on territorial defense and public security tasks like crime prevention and patrolling.\n\nStructure\n\nInitially envisioned under civilian control, a 2022 reform transferred command to the Ministry of National Defense, sparking controversy.\n\nCurrent Status\n\nThe National Guard’s role is evolving. It still tackles crime, but also handles tasks like border control and disaster relief."
  },
  {
    "objectID": "posts/python/mexico_gn.html#display-interactive-chart",
    "href": "posts/python/mexico_gn.html#display-interactive-chart",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "Display interactive chart",
    "text": "Display interactive chart\n\n\n\n\n\n\nFigure 1: Datawrapper Interactive Chart\n\n\n\n\n\n\n\n\n\nNoteInfo\n\n\n\nYou can hover the mouse over the map to get additional information."
  },
  {
    "objectID": "posts/python/mexico_gn.html#references",
    "href": "posts/python/mexico_gn.html#references",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "References",
    "text": "References\n\nWhat is Guardia Nacional?\nGobierno de Mexico"
  },
  {
    "objectID": "posts/python/mexico_gn.html#contact",
    "href": "posts/python/mexico_gn.html#contact",
    "title": "National Guard’s Deployed Personnel in Mexico",
    "section": "Contact",
    "text": "Contact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/sql_duckdb.html",
    "href": "posts/python/sql_duckdb.html",
    "title": "Supercharge Your SQL Analysis with Python and DuckDB",
    "section": "",
    "text": "Overview\nIn this post, we’ll delve into the seamless integration of DuckDB with popular Python libraries, enabling efficient data ingestion, transformation, and analysis. Through practical examples, we demonstrate how to harness the full potential of DuckDB for complex SQL queries, real-time data processing, and exploratory data analysis. By the end of this post, readers will gain the knowledge and skills to supercharge their SQL analysis projects with Python and DuckDB.\n\n\nDatabase Creation\nDatabase: retail_db\nTable: retail_sales\n\n\nShow code\n# Import libraries\nimport polars as pl\nimport duckdb as db\nimport plotly.express as px\n\n\n\n\nShow code\n# Create database\nconn = db.connect('datasets/retail_db.db')\n\n\n\n\nShow code\n# Create table\nconn.sql('''\n    create table if not exists retail_sales (\n        id INT,\n        sale_date DATE,\n        sale_time TIME,\n        customer_id INT,\n        gender VARCHAR(10),\n        age INT,\n        category VARCHAR(35),\n        quantity INT,\n        price_per_unit FLOAT,\n        cogs FLOAT,\n        total_sale FLOAT\n        )\n''')\n\n\n\n\nData Ingestion\n\n\nShow code\n# Insert data into table from csv file\nconn.sql('''\n    INSERT INTO retail_sales\n    SELECT * FROM read_csv('datasets/sales.csv')\n''')\n\n\n\n\nData Exploration and Cleaning\n\nRecord Count: Determine the total number of records in the dataset.\nCustomer Count: Find out how many unique customers are in the dataset.\nCategory Count: Identify all unique product categories in the dataset.\nNull Value Check: Check for any null values in the dataset and delete records with missing data\n\n\n\nShow code\n# Show first 10 records \nconn.sql('select * exclude(cogs) from retail_sales limit 10').pl()\n\n\n\nshape: (10, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n10\n2023-11-22\n12:30:00\n110\n\"Female\"\n37\n\"Clothing\"\n1\n99.989998\n99.989998\n\n\n\n\n\n\nRecord count\n\n\nShow code\nconn.sql('select count(*) as records from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\nrecords\n\n\ni64\n\n\n\n\n448\n\n\n\n\n\n\nCustomer count\n\n\nShow code\nconn.sql('select count(distinct customer_id) customers from retail_sales').pl()\n\n\n\nshape: (1, 1)\n\n\n\ncustomers\n\n\ni64\n\n\n\n\n55\n\n\n\n\n\n\nCategory count\n\n\nShow code\nconn.sql('select distinct category from retail_sales').pl()\n\n\n\nshape: (9, 1)\n\n\n\ncategory\n\n\nstr\n\n\n\n\n\"Toys\"\n\n\n\"Electronics\"\n\n\n\"Books\"\n\n\n\"Home Appliances\"\n\n\n\"Groceries\"\n\n\n\"Sports Equipment\"\n\n\n\"Clothing\"\n\n\n\"Home Goods\"\n\n\n\"Beauty Products\"\n\n\n\n\n\n\nNull value check\n\n\nShow code\nconn.table('retail_sales').pl().null_count()\n\n\n\nshape: (1, 11)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ncogs\ntotal_sale\n\n\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nData Analysis\nWrite a SQL query to retrieve all columns for sales made on ‘2023-11-23’\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where sale_date = '2023-11-23'\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n1\n2023-11-23\n10:15:00\n101\n\"Male\"\n35\n\"Electronics\"\n2\n299.98999\n599.97998\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n3\n2023-11-23\n10:45:00\n103\n\"Male\"\n42\n\"Books\"\n1\n19.99\n19.99\n\n\n4\n2023-11-23\n11:00:00\n104\n\"Female\"\n31\n\"Home Goods\"\n4\n39.990002\n159.960007\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n7\n2023-11-23\n11:45:00\n107\n\"Male\"\n38\n\"Books\"\n3\n14.99\n44.970001\n\n\n8\n2023-11-23\n12:00:00\n108\n\"Female\"\n22\n\"Home Goods\"\n5\n29.99\n149.949997\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n\n\n\n\nWrite a SQL query to retrieve all transactions where the category is ‘Clothing’ and the quantity sold is more than 4 in the month of Nov-2022\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where category = 'Clothing'\n        and extract('month' from sale_date) = '11'\n        and quantity &gt;= 2\n''').pl()\n\n\n\nshape: (72, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n2\n2023-11-23\n10:30:00\n102\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n6\n2023-11-23\n11:30:00\n106\n\"Female\"\n45\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n14\n2023-11-22\n13:30:00\n114\n\"Female\"\n27\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n22\n2023-11-20\n15:30:00\n122\n\"Female\"\n28\n\"Clothing\"\n2\n49.990002\n99.980003\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n26\n2023-11-17\n16:30:00\n126\n\"Female\"\n36\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n30\n2023-11-17\n17:30:00\n130\n\"Female\"\n42\n\"Clothing\"\n2\n69.989998\n139.979996\n\n\n32\n2023-11-16\n14:15:00\n456\n\"Female\"\n28\n\"Clothing\"\n3\n49.990002\n149.970001\n\n\n40\n2023-11-14\n18:30:00\n2859\n\"Female\"\n27\n\"Clothing\"\n4\n39.990002\n159.960007\n\n\n48\n2023-11-06\n19:00:00\n5443\n\"Female\"\n35\n\"Clothing\"\n2\n59.990002\n119.980003\n\n\n\n\n\n\nWrite a SQL query to calculate the total sales for each category\n\n\nShow code\nsales = conn.sql('''\n    select\n        category\n        , round(sum(total_sale),2) as net_sale\n        , count(*) as total_orders\n    from retail_sales\n    group by 1\n    order by total_orders desc\n''').pl()\n\n\n\n\nShow code\nsales\n\n\n\nshape: (9, 3)\n\n\n\ncategory\nnet_sale\ntotal_orders\n\n\nstr\nf64\ni64\n\n\n\n\n\"Electronics\"\n68878.32\n96\n\n\n\"Clothing\"\n12557.76\n96\n\n\n\"Books\"\n4133.04\n80\n\n\n\"Home Goods\"\n8437.84\n56\n\n\n\"Toys\"\n1639.04\n24\n\n\n\"Beauty Products\"\n1359.44\n24\n\n\n\"Home Appliances\"\n9599.76\n24\n\n\n\"Groceries\"\n3069.84\n24\n\n\n\"Sports Equipment\"\n3039.68\n24\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(sales,\n             x=\"net_sale\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','net_sale',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Sales by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to find the average age of customers who purchased items from the ‘Clothing’ category\n\n\nShow code\nconn.sql('''\n    select\n        round(avg(age), 2) as avg_age\n    from retail_sales\n    where category = 'Clothing'\n''').pl()\n\n\n\nshape: (1, 1)\n\n\n\navg_age\n\n\nf64\n\n\n\n\n33.25\n\n\n\n\n\n\nWrite a SQL query to find all transactions where the total_sale is greater than 1,000\n\n\nShow code\nconn.sql('''\n    select *\n        exclude(cogs)\n    from retail_sales\n    where total_sale &gt; 999\n''').pl()\n\n\n\nshape: (24, 10)\n\n\n\nid\nsale_date\nsale_time\ncustomer_id\ngender\nage\ncategory\nquantity\nprice_per_unit\ntotal_sale\n\n\ni32\ndate\ntime\ni32\nstr\ni32\nstr\ni32\nf32\nf32\n\n\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n5\n2023-11-23\n11:15:00\n105\n\"Male\"\n25\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n9\n2023-11-23\n12:15:00\n109\n\"Male\"\n33\n\"Electronics\"\n2\n499.98999\n999.97998\n\n\n29\n2023-11-17\n17:15:00\n129\n\"Male\"\n27\n\"Electronics\"\n1\n999.98999\n999.98999\n\n\n\n\n\n\nWrite a SQL query to find the total number of transactions made by each gender in each category\n\n\nShow code\nconn.sql('''\n    select\n        category\n        , gender\n        , count(*) as total_trans\n    FROM retail_sales\n    group by category\n            , gender\n    order by 2\n''').pl()\n\n\n\nshape: (9, 3)\n\n\n\ncategory\ngender\ntotal_trans\n\n\nstr\nstr\ni64\n\n\n\n\n\"Clothing\"\n\"Female\"\n96\n\n\n\"Home Goods\"\n\"Female\"\n56\n\n\n\"Toys\"\n\"Female\"\n24\n\n\n\"Home Appliances\"\n\"Female\"\n24\n\n\n\"Beauty Products\"\n\"Female\"\n24\n\n\n\"Groceries\"\n\"Male\"\n24\n\n\n\"Electronics\"\n\"Male\"\n96\n\n\n\"Sports Equipment\"\n\"Male\"\n24\n\n\n\"Books\"\n\"Male\"\n80\n\n\n\n\n\n\nWrite a SQL query to calculate the average sale for each month\n\n\nShow code\nconn.sql('''\n    select\n        year\n        , month\n        , avg_sale\n    from\n        (\n        select\n            extract(year from sale_date) as year\n            , EXTRACT(month from sale_date) as month\n            , round(avg(total_sale),2) as avg_sale\n            , rank() over(partition by extract(year from sale_date)\n            order by avg(total_sale) desc) as rank\n        from retail_sales\n        group by 1, 2\n        ) as t1\n''').pl()\n\n\n\nshape: (2, 3)\n\n\n\nyear\nmonth\navg_sale\n\n\ni64\ni64\nf64\n\n\n\n\n2023\n11\n254.61\n\n\n2023\n10\n198.3\n\n\n\n\n\n\nWrite a SQL query to find the top 5 customers based on the highest total sales\n\n\nShow code\nconn.sql('''\n    select customer_id\n            , round(sum(total_sale),2) as total_sales\n    from retail_sales\n    group by 1\n    order by 2 desc\n    limit 5\n''').pl()\n\n\n\nshape: (5, 2)\n\n\n\ncustomer_id\ntotal_sales\n\n\ni32\nf64\n\n\n\n\n129\n7999.92\n\n\n105\n7999.92\n\n\n109\n7999.84\n\n\n113\n6399.92\n\n\n117\n6399.84\n\n\n\n\n\n\nWrite a SQL query to find the number of unique customers who purchased items from each category\n\n\nShow code\ncustomers = conn.sql('''\n    select category\n            , count(distinct customer_id) as count_unique\n    from retail_sales\n    group by category\n    order by 2 desc\n''').pl()\n\n\n\n\nShow code\ncustomers\n\n\n\nshape: (9, 2)\n\n\n\ncategory\ncount_unique\n\n\nstr\ni64\n\n\n\n\n\"Clothing\"\n12\n\n\n\"Electronics\"\n12\n\n\n\"Books\"\n10\n\n\n\"Home Goods\"\n7\n\n\n\"Beauty Products\"\n3\n\n\n\"Toys\"\n3\n\n\n\"Groceries\"\n3\n\n\n\"Home Appliances\"\n3\n\n\n\"Sports Equipment\"\n3\n\n\n\n\n\n\n\n\nShow code\nfig = px.bar(customers,\n             x=\"count_unique\",\n             y=\"category\",\n             orientation='h',\n             hover_data=['category','count_unique',],\n            )\n\nfig.update_traces(marker_color='#0066a1', marker_line_color='black',\n                  marker_line_width=1.5, opacity=0.9)\n\nfig.update_layout(width=850,\n                  height=500,\n                  title_text='&lt;i&gt;Unique Customers Purchases by Category during 2023&lt;/i&gt;',\n                  title_x=0.2,\n                  template=\"ggplot2\",\n                  yaxis={'categoryorder':'total ascending'}\n                 )\n\nfig.show()\n\n\n                                                \n\n\nWrite a SQL query to create each shift and number of orders (Example Morning &lt;12, Afternoon Between 12 & 17, Evening &gt;17)\n\n\nShow code\nconn.sql('''\n    with hourly_sale as\n        (\n        select *\n            , case \n                when extract(hour from sale_time) &lt;12 then 'Morning'\n                when extract(hour from sale_time) between 12 and 17 then 'Afternoon'\n            else 'Evening'\n            end as shift\n        from retail_sales\n        )\n    select\n        shift\n        , count(*) as total_orders\n    from hourly_sale\n    group by shift\n''').pl()\n\n\n\nshape: (3, 2)\n\n\n\nshift\ntotal_orders\n\n\nstr\ni64\n\n\n\n\n\"Evening\"\n32\n\n\n\"Morning\"\n136\n\n\n\"Afternoon\"\n280\n\n\n\n\n\n\n\n\nClose connection\n\n\nShow code\n# Close connection\nconn.close()\n\n\n\n\nConclusions\nThis project demonstrates the power of combining Python and DuckDB for efficient and insightful SQL analysis. By mastering these tools, data analysts can streamline their workflows, uncover valuable insights, and make data-driven decisions that drive business success.\nWe’ve explored the fundamental concepts of SQL and its practical applications in data analysis. By leveraging the capabilities of Python and DuckDB, we’ve been able to efficiently query, clean, and analyze data. This knowledge and skillset can be applied to a wide range of data-driven tasks, from simple data exploration to complex predictive modeling. As we continue to delve deeper into the world of data, the combination of Python and DuckDB promises to be a powerful tool for data analysts.\nThese insights can be used to optimize marketing strategies, improve customer satisfaction, and drive revenue growth. As data continues to proliferate, the ability to harness its power through SQL analysis will become increasingly important for businesses to stay competitive.\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "posts/python/why_bi_tools_fall_short.html",
    "href": "posts/python/why_bi_tools_fall_short.html",
    "title": "Why BI Tools Fall Short, A Failure to Capture the Office Workflow",
    "section": "",
    "text": "Overview\nCompanies use data warehouses or data lakes for centralized data storage, consistency, data quality, scalability and easy access. Business Intelligence (BI) solutions in conjunction with data warehouses are used to make more informed, data-driven decisions by means of dahsboards for stakeholders.\nIn this fashion, a dashboard is created using a BI application, connected to a data warehouse with the aim to be consumed by end users for their business activities.\nIt is an open secret, nonetheless, that staff steadily use spreadsheets to store information and manipulate data sets coming from data warehouses, other information systems and dashboards.\nIn a similar fashion, staff steadily use slide presentations to showcase insights and reports to managers and other stake holders. This means there are countless presentations and data analyses stored in local Excel and PowerPoint files.\n\n\nBeyond the Dashboard: Reporting with Slideshows\nDashboards can be used to gather and analyze data, while slideshows can be used to present the findings in a clear and concise manner. Besides, it enables you to use your existing data insights in the tools you’re most familiar with, without having to switch to more complex ones.\n\nYou can just do your data analysis in Excel and then present it in PowerPoint. This provides you with just the flexibility you need for.\n\nWhile a dashboard is a centralized section that displays your data visually typically by using a license BI tool (Tableau, Power BI), staff prefers to present data insights to potential customers or coworkers in Excel or PowerPoint by copying and pasting charts and tables from dashboards.\n\n\n\n\n\n\nIndeed, BI tools like Tableau or Power BI offer options to download data to Excel or csv files, PowerPoint and images.\n\n\n\nAutomating Spreadsheet Data with Python\nI propose the process of creating an ETL from the data warehouse to a spreadsheet using Python, and synchronizing tables and charts from Excel to PowerPoint to get an automated reporting in a local file with the needs of the end user.\nETL (Extract, Transform, Load) is a data integration process that involves 03 main steps:\n\nExtract Phase. Retrieving data from a source system (in this case, a data warehouse).\nTransform Phase. Manipulating, cleaning, and aggregating the extracted data.\nLoad Phase. Storing the transformed data into a target system (in this case, an Excel file).\n\n\n\n\n\n\n\nFigure 1: Image by El Mehdi Ettaki\n\n\n\n\n\nCase Study\nI’ll show an example using Snowflake as data warehouse, Python for ETL process, and Excel as destination. Finally, PowerPoint will present the data insights.\n\nBy foregoing BI tools, we can substantially reduce project expenses.\n\n\n\n\n\n\n\nFigure 2: Image by author\n\n\n\n\nImport libraries\n\n\nimport pandas as pd\nfrom snowflake.snowpark import Session, Window\nimport snowflake.snowpark.functions as F\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nRead credentials\n\n\n# Credentials\nfile = 'credentials.json'\n# read file\nwith open(file) as f:\n    keys = json.load(f)\n\n\nConnect to Data Warehouse using Snowpark\n\n\n# Snowflake's Snowpark Connection\nconnection = {\n    \"account\": keys['account'],\n    \"user\": keys['user'],\n    \"role\": keys['role'],\n    \"authenticator\": keys['authenticator'],\n    \"warehouse\": keys['warehouse'],\n    \"database\": keys['database'],\n    \"schema\": keys['schema'],\n}\n\ndef snowflake_connection():\n    try:\n        session = Session.builder.configs(connection).create()\n        print(\"Connection successful!\")\n    except:\n        raise ValueError(\"Connection failed!\")\n    return session\n\nsession = snowflake_connection()\n\n\nExtract data using Snowpark\n\n\n# Extract sales data\nsales = conn.sql('''\n  SELECT\n      store_id,\n      SUM(sales_amount) AS total_sales\n  FROM\n      dm_sales\n  WHERE\n      YEAR(dm_sales) = YEAR(CURDATE())\n  GROUP BY\n      store_id\n''')\n# Extract stores data\nstores = (\n  conn.table('dm_stores')\n      .select('location','id','responsible')\n)\n# Label stores region\nstores = stores.with_column(\n        'region',\n        F.when(F.col('region_abv')=='AS', 'ASPAC')\n          .when(F.col('region_abv')=='LA', 'LATAM')\n          .when(F.col('region_abv')=='EU', 'EMEA')\n          .when(F.col('region_abv')=='NA', 'NA')\n          .otherwise('null')\n    )\n# Combine tables\ndata = (sales.join(stores, sales.store_id == stores.id))\n# Save to pandas\ndata = data.to_pandas()\n\n\nTransform data using Pandas\n\n\n# Calculate top 10 products\ntop_10 = data.nlargest(10, 'total_sales')\n\n\nLoad data using Pandas\n\n\n# Write to Excel\ntop_10.to_excel('top_10_products.xlsx', index=False)\n\n\n\nFrom Excel to PowerPoint: Automating Report Creation\nNow, you can customize your tables and charts in Excel with the data saved from Python. \nTo automate your customized tables and charts created in Excel onto PowerPoint, you just need to follow the next steps:\n\n\n\n\n\n\n\n\n\n\nFinally, after customizing your slideshow, you will get a PowerPoint like the following:\n\n\n\n\n\n\nFigure 3: Image by author\n\n\n\n\n\nConclusions\nThe use of spreadsheets and slideshows in businesses is not going to disappear soon. Hence, even if BI tools like Tableau or Power BI are used in businesses, Excel and PowerPoint are going to be used by staff to reporting and presentations to coworkers and managers.\nBy following the above steps and leveraging the power of Python, you can efficiently extract, transform, and load data from your data warehouse into Excel for further analysis and insights that fulfills end user’s requirements.\nMaybe is it time to recalculate the cost-benefit implications for companies to abandon expensive BI licenses in favor of flexible, cost-effective open-source solutions like Python.\n\n\nReferences\n\nBusiness (2023) How to Design a Dashboard Presentation: A Step-by-Step Guide in slidemodel.com\nKarlson, P. (2022) Are Spreadsheets Secretly Running Your Business? in Forbes\nMoore J. (2024) But, Can I Export it to Excel? in Do Mo(o)re with Data\nSchwab, P. (2021) Excel dominates the business world.. and that’s not about to change in Into the Minds\n\n\n\nContact\nJesus L. Monroy  Economist & Data Scientist\nMedium | Linkedin | Twitter"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Economist & Data Scientist\n\nI’ve applied data analysis and visualization to drive strategic value for 10+ years across industries like public security, e-commerce, and healthcare.\nI use Python1, SQL, and Tableau to transform raw data into clear, actionable data products (webpages, slideshows, interactive reports) that support decision-making, occasionally incorporating regression and classification analyses.\nCommitted to fostering data literacy, I share data posts on my Blog and also on Learning Data and T3CH via Medium.\nBackground in Economics (BA), Information Technology (MA) and Data Science and Machine Learning (Certification).\nVegan.\n© 2025"
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "About me",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIncluding libraries such as numpy, pandas, polars, duckdb, matplotlib, seaborn, plotly, folium, sckitlearn.↩︎"
  }
]